# 12

# 提升你的交易策略

在上一章中，我们看到了**随机森林**如何通过将许多树组合成一个集合来改进决策树的预测。减少单株树高方差的关键是使用**套袋**，简称**自举聚合**，它将随机性引入单株树的生长过程。更具体地说，使用替换从数据中提取样本，以便在不同但大小相同的随机子集上训练每棵树，并重复一些观察结果。此外，随机林随机选择特征的子集，以便每个树的训练集的行和列都是原始数据的随机版本。然后，集合通过对单个树的输出进行平均来生成预测。

单个随机森林树木通常生长得很深，以确保低偏差，同时依靠随机训练过程产生不同的、不相关的预测误差，这些误差在聚合时比单个树木预测的方差更低。换句话说，随机训练的目的是去相关（认为*使*多样化）单个树的错误。它这样做是为了使集合不易受到过度拟合的影响，具有较低的方差，从而更好地推广到新数据。

本章探讨**boosting**，这是一种用于决策树的替代集成算法，通常会产生更好的结果。关键的区别在于 boosting 根据迄今为止模型所产生的累积误差修改每个新树的训练数据。与使用训练集样本独立训练许多树的随机森林不同，boosting 使用数据的重新加权版本按顺序进行。最先进的增强实现也采用随机林的随机化策略。

在过去三十年中，boosting 已经成为最成功的**机器学习**（**ML**）算法之一，在结构化表格数据的许多 ML 竞争中占据主导地位（与具有更复杂输入输出关系的高维图像或语音数据相比，深度学习更具优势）。我们将展示 boosting 的工作原理，介绍几种高性能实现，并将 boosting 应用于**高频数据**，并对**日内交易策略**进行回溯测试。

更具体地说，阅读本章后，您将能够：

*   了解增压与装袋的区别，以及梯度增压如何从自适应增压演变而来。
*   使用 scikit learn 设计和调整自适应增压和梯度增压模型。
*   使用最先进的 XGBoost、LightGBM 和 CatBoost 实现在大型数据集上构建、调优和评估梯度增强模型。
*   解释梯度推进模型并从中获得见解。
*   使用高频率数据来设计日内策略。

您可以在 GitHub 存储库的相应目录中找到本章的代码示例以及指向其他资源的链接。笔记本电脑包括图像的彩色版本。

# 入门–自适应增压

与 bagging 一样，boosting 是一种集成学习算法，它将基础学习者（通常为决策树）组合成一个集成。Boosting 最初是为分类问题开发的，但也可以用于回归，被称为过去 20 年中引入的最有效的学习思想之一（Hastine、Tibshirani 和 Friedman，2009）。与 bagging 一样，它是一种通用方法或元方法，可以应用于许多统计学习方法。

boosting 背后的动机是找到一种方法，将**许多弱模型**的输出组合成一个高度**准确的**、**增强的联合预测**（Schapire 和 Freund，2012 年），其中“弱”意味着它们的表现仅略好于随机猜测。

一般来说，boosting 学习一种类似于线性回归的加性假设*H*<sub style="font-style: italic;">M</sub>。然而，求和中的*m*=1、*m*元素都是基础薄弱的学习者，称为*h*<sub style="font-style: italic;">t</sub>，这本身就需要培训。以下公式总结了该方法：

![](img/B15439_12_001.png)

正如前一章所讨论的，打包训练是基于不同的随机数据样本。相比之下，Boosting 则是通过反复修改数据来训练基础学习者，以反映累积学习情况。目标是确保下一个基础学习者弥补当前集合的不足。我们将在本章中看到，boosting 算法在定义缺点方面有所不同。集合使用弱模型预测的加权平均值进行预测。

Robert Schapire 和 Yoav Freund 于 1990 年左右开发了第一个 boosting 算法，该算法通过数学证明可以提高弱学习者的学习成绩。1997 年，以**自适应 boosting**（**AdaBoost**算法）的形式出现了一个实用的分类问题解决方案，该算法在 2003 年获得了 Göedel 奖（Freund and Schapire 1997）。大约又过了 5 年，当 Leo Breiman（发明随机森林的人）将这种方法与梯度下降联系起来，Jerome Friedman 在 1999 年提出了与**梯度提升**（Friedman 2001）时，这种算法被扩展到任意目标函数。

近年来出现了许多优化的实现，如 XGBoost、LightGBM 和 CatBoost，我们将在本章后面介绍这些实现，并牢固地将梯度增强作为结构化数据的解决方案。在接下来的部分中，我们将简要介绍 AdaBoost，然后重点介绍梯度推进模型，以及我们刚才提到的这个非常强大和灵活的算法的三种最先进的实现。

## AdaBoost 算法

当它在 20 世纪 90 年代出现时，AdaBoost 是第一个在拟合额外集成成员时迭代适应累积学习进度的集成算法。特别是，AdaBoost 改变了训练数据上的权重，以反映训练集中当前集合的累积误差，然后再拟合新的弱学习者。AdaBoost 是当时最精确的分类算法，Leo Breiman 在 1996 年 NIPS 会议（Hastine、Tibshirani 和 Friedman，2009 年）上将其称为世界上最好的现成分类器。

在随后的几十年中，该算法对机器学习产生了巨大影响，因为它提供了理论性能保证。这些保证只需要足够的数据和一个比随机猜测更可靠的弱学习者。由于这种分阶段学习的自适应方法，精确的 ML 模型的开发不再需要在整个特征空间上的精确性能。取而代之的是，模型的设计可以专注于寻找那些通过使用一小部分特征而表现优于掷硬币的弱学习者。

与 bagging 不同的是，bagging 构建了一组非常大的树以减少偏差，AdaBoost 以弱学习者的身份种植浅树，通常使用树桩（即通过单次分裂形成的树）产生更高的准确性。该算法从一个等权训练集开始，然后依次改变样本分布。每次迭代后，AdaBoost 都会增加错误分类观察的权重，并降低正确预测样本的权重，以便后续的弱学习者更加关注特别困难的案例。一旦训练完成，新的决策树将被纳入集成中，其权重将反映其对减少训练错误的贡献。

基本学习者集合的 AdaBoost 算法，*h*<sub style="font-style: italic;">m</sub>（*x*）、*m=1*、*m*、预测离散类、*y*![](img/B15439_12_002.png)和*N*训练观察结果可总结如下：

1.  初始化样本重量*w*<sub style="font-style: italic;">i</sub>=*1/N*用于观察*i*=*1*、*N*。
2.  对于每个基础分类器，*h*<sub style="font-style: italic;">m</sub>、*m*=*1*、*m*，执行以下操作：
    1.  将*h*<sub style="font-style: italic;">m</sub>*x*拟合到训练数据中，由*w*<sub style="font-style: italic;">i</sub>加权。
    2.  在训练集上计算基础学习者的加权错误率![](img/B15439_12_003.png)。
    3.  计算基础学习者的整体权重![](img/B15439_12_004.png)作为其错误率的函数，如下公式所示：

        ![](img/B15439_12_005.png)

    4.  根据![](img/B15439_12_006.png)更新错误分类样本的权重
3.  当集合成员的加权和为正时，预测正类，否则为负，如下式所示：

![](img/B15439_12_007.png)

AdaBoost 具有许多实用**优点**，包括易于实现和快速计算，并且可以与任何识别弱学习者的方法相结合。除了集合的大小，没有需要调整的超参数。AdaBoost 对于识别异常值也很有用，因为获得最高权重的样本是那些一贯错误分类且本质上不明确的样本，这也是异常值的典型特征。

还有**缺点**：AdaBoost 在给定数据集上的性能取决于弱学习者充分捕捉特征和结果之间关系的能力。正如理论所表明的那样，当数据不足时，或者当集合成员的复杂性与数据的复杂性不匹配时，boosting 将无法很好地执行。它也容易受到数据中噪声的影响。

参见 Schapire 和 Freund（2012）对 boosting 算法的全面介绍和回顾。

## 使用 AdaBoost 预测每月价格变动

作为集成模块的一部分，scikit learn 提供了支持两个或更多类的`AdaBoostClassifier`实现。本节的代码示例在笔记本`boosting_baseline`中，它将各种算法的性能与总是预测最频繁类的虚拟分类器进行比较。

我们需要首先定义一个`base_estimator`作为所有集合成员的模板，然后配置集合本身。我们将使用默认的`DecisionTreeClassifier`和`max_depth=1`——也就是说，一个树桩有一个单独的分割。备选方案包括从线性或逻辑回归到符合 scikit 学习界面的神经网络的任何其他模型（参见文档）。然而，决策树在实践中是最常见的。

`base_estimator`的复杂性是一个关键的调整参数，因为它取决于数据的性质。如前一章所示，对`max_depth`的更改应与适当的正则化约束相结合，例如对`min_samples_split`进行调整，如下代码所示：

```py
base_estimator = DecisionTreeClassifier(criterion='gini', 
                                        splitter='best',
                                        max_depth=1, 
                                        min_samples_split=2, 
                                        min_samples_leaf=20, 
                                        min_weight_fraction_leaf=0.0,
                                        max_features=None, 
                                        random_state=None, 
                                        max_leaf_nodes=None, 
                                        min_impurity_decrease=0.0, 
                                        min_impurity_split=None) 
```

在第二步中，我们将设计合奏。`n_estimators`参数控制弱学习者的数量，`learning_rate`确定每个弱学习者的贡献，如下代码所示。默认情况下，弱学习者是决策树树桩：

```py
ada_clf = AdaBoostClassifier(base_estimator=base_estimator,
                            n_estimators=100,
                            learning_rate=1.0,
                            algorithm='SAMME.R',
                            random_state=42) 
```

导致良好结果的主要调优参数是`n_estimators`和`base_estimator`复杂度。这是因为树的深度控制特征之间交互的程度。

我们将使用定制的`OneStepTimeSeriesSplit`交叉验证AdaBoost 集成，这是更灵活的`MultipleTimeSeriesCV`的简化版本（参见*第 6 章*、*机器学习过程*。它实现了 12 倍滚动时间序列分割，以使用所有可用的先前数据进行培训，预测样本中最后 12 个月的提前 1 个月，如以下代码所示：

```py
cv = OneStepTimeSeriesSplit(n_splits=12, test_period_length=1, shuffle=True)
def run_cv(clf, X=X_dummies, y=y, metrics=metrics, cv=cv, fit_params=None):
    return cross_validate(estimator=clf,
                          X=X,
                          y=y,
                          scoring=list(metrics.keys()),
                          cv=cv,
                          return_train_score=True,
                          n_jobs=-1,                 # use all cores
                          verbose=1,
                          fit_params=fit_params) 
```

验证结果显示加权准确度为 0.5068，AUC 得分为 0.5348，精密度和召回值分别为 0.547 和 0.576，这意味着 F1 得分为 0.467。这略低于具有默认设置的随机林，其验证 AUC 为 0.5358。*图 12.1*以箱线图的形式显示了 12 个序列和测试折叠的各种度量的分布（请注意，随机林非常适合训练集）：

![](img/B15439_12_01.png)

图 12.1:AdaBoost 交叉验证性能

有关交叉验证和处理结果的代码的更多详细信息，请参见附带的笔记本。

# 渐变增强–适用于大多数任务的集成

AdaBoost 也可以被解释为一种分段向前的方法，用于最小化二元结果的指数损失函数*y*![](img/B15439_12_002.png)，该方法在每次迭代时识别一个新的基础学习者*h*<sub style="font-style: italic;">m</sub>*m*，并具有相应的权重![](img/B15439_12_009.png)，并将其添加到集合中，如以下公式所示：

![](img/B15439_12_010.png)

AdaBoost 的这种解释是一种梯度下降算法，它最小化了一个特定的损失函数，即指数损失，这种解释在最初发表几年后才被发现。

**梯度提升**利用了这一洞察，**将提升方法应用于更广泛的损失函数**。该方法使机器学习算法的设计能够解决任何回归、分类或排序问题，只要它可以使用可微的损失函数来表示，因此具有梯度。不同任务的常见损失函数示例包括：

*   **回归**：均方和绝对损失
*   **分类**：交叉熵
*   **学习排名**：Lambda 排名损失

我们在*第 6 章*、*机器学习过程*中介绍了回归和分类损失函数；学习排名超出了本书的范围，但关于排名损失的详细信息，请参见中本（2011）的介绍和 Chen 等人（2009）。

将这种通用方法定制为许多特定预测任务的灵活性对于提高该方法的受欢迎程度至关重要。梯度提升也不局限于学习能力较弱的学习者，通常在决策树深入数层的情况下获得最佳性能。

产生的**梯度提升机**（**GBMs**算法）背后的主要思想是训练基础学习者学习集合的电流损失函数的负梯度。因此，考虑到以前的集成成员所犯的错误，集成中的每个添加都直接有助于减少总体训练错误。由于每个新成员代表数据的一个新函数，因此梯度提升也可以说是以相加的方式优化函数*h*<sub style="font-style: italic;">m</sub>。

简言之，该算法将弱学习者*h*<sub style="font-style: italic;">m</sub>（如决策树）依次拟合到当前集合评估的损失函数的负梯度，如下式所示：

![](img/B15439_12_011.png)

换句话说，在给定迭代*m*时，该算法计算每个观测值的电流损耗梯度，然后根据这些伪残差拟合回归树。在第二步中，它为每个叶节点确定一个最佳预测，从而最大限度地减少由于将该新学习者添加到集合中而导致的增量损失。

这与独立决策树和随机林不同，在独立决策树和随机林中，预测取决于分配给终端节点的训练样本的结果，即回归情况下的平均值，或二元分类的正类频率。对损失函数梯度的关注还意味着梯度提升使用回归树来学习回归和分类规则，因为梯度始终是一个连续函数。

最终的集成模型基于单个决策树预测的加权和进行预测，每个决策树都经过训练，以最小化集成损失，给定给定特征值集的先验预测，如下图所示：

![](img/B15439_12_02.png)

图 12.2：梯度增强算法

梯度提升树在许多分类、**回归**和**排名基准**上都产生了**最先进的表现。它们可能是最流行的集成学习算法，作为一组不同的 ML 竞赛中的独立预测器，以及在现实世界的生产管道中，例如，用于预测在线广告的点击率。**

梯度提升的成功基于其以增量方式学习复杂函数关系的能力。然而，该算法的灵活性要求通过调整**超参数**仔细管理**过度拟合**的风险，该超参数约束模型在训练数据中学习噪声（而非信号）的趋势。

我们将介绍控制梯度提升树模型复杂性的关键机制，然后说明使用 sklearn 实现的模型调整。

## 如何训练和调整 GBM 模型

提升通常表现出**对过度拟合**的显著恢复力，尽管整体显著增长，因此模型复杂。极低且逐渐减少的训练误差与非增加的验证误差的组合通常与预测置信度的提高有关：随着 boosting 继续增加集合，以改善最具挑战性的情况下的预测，它调整决策边界以最大化距离或裕度，数据点的位置。

然而，过度拟合肯定会发生，梯度提升性能的两个关键驱动因素**是集合的大小及其组成决策树的复杂性。**

控制决策树的**复杂性旨在避免学习高度特定的规则，这些规则通常意味着叶节点中的样本数量非常少。在上一章中，我们讨论了用于限制决策树过度拟合训练数据的能力的最有效约束。它们包括以下方面的最低阈值：**

*   拆分节点或将其接受为终端节点的样本数。
*   节点质量的改善，通过分类的纯度或熵，或回归的均方误差来衡量，以进一步增长树。

除了直接控制集合的大小外，还有各种正则化技术，例如**收缩**，我们在*第 7 章*、*线性模型【从风险因素到回报预测】中的脊线和套索线性回归模型中遇到了这些技术此外，随机森林中使用的随机化技术也普遍应用于梯度提升机。*

### 集合大小和早期停止

每次增强迭代的目的是减少训练损失，增加大型集合过度拟合的风险。交叉验证是找到使泛化误差最小化的最佳集合大小的最佳方法。

由于训练前需要指定集合大小，因此在给定的迭代次数下，当验证误差不再减小时，监控验证集上的性能并中止训练过程非常有用。这种技术被称为**提前停止**，经常用于需要大量迭代且容易过度拟合的模型，包括深度神经网络。

请记住，在大量试验中使用同一验证集的提前停止也会导致过度拟合，但只适用于特定的验证集，而不是训练集。在制定交易策略时，最好避免进行大量实验，因为**错误发现**的风险显著增加。在任何情况下，保留一个**保持测试集**，以获得泛化误差的无偏估计。

### 收缩率和学习率

收缩技术对模型的损失函数施加惩罚，以增加模型的复杂性。对于增强集成，收缩可以通过**将每个新集成成员的贡献**按 0 到 1 之间的系数进行缩放来应用。这个因素被称为助推组合的**学习率**。降低学习速率会增加收缩，因为它会降低每个新决策树对集成的贡献。

学习率与集合大小的效果相反，集合大小随着学习率的降低而增加。研究发现，较低的学习率加上较大的集合可以减少测试误差，特别是在回归和概率估计方面。大量的迭代在计算上更昂贵，但只要单个树保持较浅，使用快速、最先进的实现通常是可行的。

根据实现情况，您还可以使用**自适应学习速率**，该速率可根据迭代次数进行调整，通常会降低过程中后期添加的树的影响。我们将在本章后面看到一些示例。

### 二次抽样与随机梯度增强

如前一章中详细讨论的，引导平均（bagging）提高了其他噪声分类器的性能。

随机梯度提升在每次迭代中对训练数据进行采样，不进行替换，以生长下一棵树（而 bagging 使用替换采样）。这样做的好处是，由于样本较小，计算工作量较低，而且精度通常较高，但子采样应与收缩相结合。

正如您所看到的，超参数的数量不断增加，这会增加潜在组合的数量。因此，当基于有限数量的训练数据从大量试验中选择最佳模型时，假阳性风险增加。最好的方法是按顺序进行，单独选择参数值或使用低基数子集的组合。

## 如何在 sklearn 中使用渐变增强

sklearn 的集成模块包含用于回归和分类的梯度增强树的实现，包括二元和多类。以下`GradientBoostingClassifier`初始化代码说明了关键调整参数。笔记本`sklearn_gbm_tuning`包含本节的代码示例。最近（版本 0.21），scikit learn 引入了一个更快但仍处于实验阶段的`HistGradientBoostingClassifier`，其灵感来源于下一节中的实现。

可用的损失函数包括导致AdaBoost 算法的指数损失和对应于概率输出逻辑回归的偏差。`friedman_mse`节点质量度量是均方误差的一种变化，包括改进分数（参见 GitHub 上链接的 scikit 学习文档），如下代码所示：

```py
# deviance = logistic reg; exponential: AdaBoost
gb_clf = GradientBoostingClassifier(loss='deviance',                
# shrinks the contribution of each tree
                                   learning_rate=0.1,              
# number of boosting stages
                                   n_estimators=100,               
# fraction of samples used t fit base learners
                                   subsample=1.0,                  
# measures the quality of a split
                                   criterion='friedman_mse',       
                                   min_samples_split=2,            
                                   min_samples_leaf=1, 
# min. fraction of sum of weights
                                   min_weight_fraction_leaf=0.0,   
# opt value depends on interaction
                                   max_depth=3,                    
                                   min_impurity_decrease=0.0, 
                                   min_impurity_split=None, 
                                   max_features=None, 
                                   max_leaf_nodes=None, 
                                   warm_start=False, 
                                   presort='auto',
                                   validation_fraction=0.1, 
                                   tol=0.0001) 
```

与`AdaBoostClassifier`类似，此模型无法处理缺失值。我们将再次使用 12 倍交叉验证来获得滚动 1 个月持有期的定向回报分类错误，如下代码所示：

```py
gb_cv_result = run_cv(gb_clf, y=y_clean, X=X_dummies_clean)
gb_result = stack_results(gb_cv_result) 
```

当测试 AUC 增加到 0.537 时，我们分析并绘制结果，使用默认参数值对`AdaBoostClassifier`和随机林进行轻微改进。*图 12.3*显示了我们正在跟踪的各种损失指标的箱线图：

![](img/B15439_12_03.png)

图 12.3:scikit 学习梯度提升分类器的交叉验证性能

### 如何使用 GridSearchCV 调整参数

`model_selection`模块中的`GridSearchCV`类有助于系统评估我们要测试的超参数值的所有组合。在下面的代码中，我们将为七个调优参数演示此功能，这些参数定义后将导致总共![](img/B15439_12_012.png)不同的模型配置：

```py
cv = OneStepTimeSeriesSplit(n_splits=12)
param_grid = dict(
        n_estimators=[100, 300],
        learning_rate=[.01, .1, .2],
        max_depth=list(range(3, 13, 3)),
        subsample=[.8, 1],
        min_samples_split=[10, 50],
        min_impurity_decrease=[0, .01],
        max_features=['sqrt', .8, 1]) 
```

`.fit()`方法使用自定义`OneStepTimeSeriesSplit`和`roc_auc`分数执行交叉验证，以评估 12 倍。Sklearn 允许我们使用`joblib`pickle 实现来持久化结果，就像对任何其他模型一样，如以下代码中的所示：

```py
gs = GridSearchCV(gb_clf,
                  param_grid,
                  cv=cv,
                  scoring='roc_auc',
                  verbose=3,
                  n_jobs=-1,
                  return_train_score=True)
gs.fit(X=X, y=y)
# persist result using joblib for more efficient storage of large numpy arrays
joblib.dump(gs, 'gbm_gridsearch.joblib') 
```

`GridSearchCV`对象有几个附加属性，完成后，我们可以在加载 pickle 结果后访问这些属性。我们可以使用它们来了解哪个超参数组合表现最好，以及它的平均交叉验证 AUC 分数，这会使默认值得到适度的改善。这在以下代码中显示：

```py
pd.Series(gridsearch_result.best_params_)
learning_rate              0.01
max_depth                  9.00
max_features               1.00
min_impurity_decrease      0.01
min_samples_split         50.00
n_estimators             300.00
subsample                  1.00
gridsearch_result.best_score_
0.5569 
```

### 参数对考试成绩的影响

`GridSearchCV`结果存储平均交叉验证分数，以便我们可以分析不同的超参数设置如何影响结果。

*图 12.4*右侧面板中的六个 seaborn swarm 图显示了所有超参数值的 AUC 测试分数分布。在这种情况下，最高的 AUC 测试分数要求较低的`learning_rate`和较大的`max_features`值。一些参数设置，如低`learning_rate`会产生广泛的结果，这取决于其他参数的补充设置：

![](img/B15439_12_04.png)

图 12.4:scikit 学习梯度提升模型的超参数影响

现在，我们将探讨超参数设置如何共同影响交叉验证性能。为了深入了解参数设置是如何相互作用的，我们可以以平均 CV AUC 作为结果来训练`DecisionTreeRegressor`，并以一种热格式或虚拟格式编码参数设置（详情请参见笔记本）。树状结构突出显示，使用上述三个特征（`max_features=1`）、一个低`learning_rate`、一个`max_depth`得到最佳结果，如下图所示：

![](img/B15439_12_05.png)

图 12.5：梯度推进模型超参数设置对试验性能的影响

*图 12.4*的左侧面板中的条形图显示了超参数设置对产生不同结果的影响，通过其对已发展到其最大深度的决策树的特征重要性来衡量。当然，出现在树顶部附近的特征也会累积最高的重要性分数。

### 如何在保持集上进行测试

最后，我们想评估我们排除在`GridSearchCV`练习之外的最佳模型在坚持集上的表现。它包含样本期的最后 7 个月（截至 2018 年 2 月；详情见笔记本）。

我们使用以下代码示例，基于保持期第一个月的 AUC 分数 0.5381，获得了泛化性能估计：

```py
idx = pd.IndexSlice
auc = {}
for i, test_date in enumerate(test_dates):
    test_data = test_feature_data.loc[idx[:, test_date], :]
    preds = best_model.predict(test_data)
    auc[i] = roc_auc_score(y_true=test_target.loc[test_data.index], y_score=preds)
auc = pd.Series(auc) 
```

sklearn gradient boosting 实现的缺点是**有限的训练速度**，这使得很难快速尝试不同的超参数设置。在下一节中，我们将看到在过去几年中出现了一些优化的实现，这些实现大大减少了训练大型模型所需的时间，并且极大地扩大了这种高效算法的应用范围。

# 使用 XGBoost、LightGBM 和 CatBoost

在过去几年中，一些新的梯度增强实现使用了各种创新，这些创新加快了训练，提高了资源效率，并允许算法扩展到非常大的数据集。新的实现及其来源如下：

*   **XGBoost**：2014 年开始工作，由陈先生博士研究。（T.Chen 和 Guestrin 2016）
*   **LightGBM**：微软于 2017 年 1 月发布（Ke 等人，2017）
*   **CatBoost**：Yandex 于 2017 年 4 月发布（Prokhorenkova 等人，2019 年）

这些创新解决了培训梯度提升模型的具体挑战（有关文档链接，请参阅本章 GitHub 上的`README`文件）。XGBoost 实现是第一个受欢迎的新实现：在 Kaggle 于 2015 年发布的 29 个获奖解决方案中，有 17 个解决方案使用了 XGBoost。其中八个完全依赖 XGBoost，而其他的则将 XGBoost 与神经网络相结合。

在说明其实现之前，我们将首先介绍随着时间的推移而出现并随后融合的关键创新（以便大多数功能可用于所有实现）。

## 算法创新如何提高性能

随机森林可以通过在独立的自举样本上生长单株树来并行训练。相比之下，梯度增强的**顺序方法**减慢了训练速度，这反过来又使大量超参数的实验变得复杂，这些超参数需要适应任务和数据集的性质。

为了向集合中添加树，该算法将损失函数负梯度的预测误差降至最低，类似于传统的梯度下降优化器。因此，训练期间的**计算成本与评估每个特征的潜在分割点**所需的时间成正比。

### 二阶损失函数逼近

最重要的算法创新通过使用依赖于二阶导数的近似，降低了评估损失函数的成本，类似于牛顿寻找驻点的方法。因此，得分的潜在分裂变得更快。

如前所述，梯度增强系综*H*<sub style="font-style: italic;">M</sub>被增量训练以最小化预测误差和正则化惩罚之和。将集合在步骤*m*之后对结果*y*<sub style="font-style: italic;">i</sub>的预测表示为![](img/B15439_12_013.png)，表示为测量结果与预测之间差异的可微凸损失函数，![](img/B15439_12_014.png)表示为随着集合*的复杂性而增加的惩罚 H*<sub style="font-style: italic;">M</sub>。增量假设*h*<sub class="Subscript--PACKT-">m</sub>旨在最小化以下目标*L*：

![](img/B15439_12_015.png)

正则化惩罚通过支持使用简单但预测性回归树的模型，有助于避免过度拟合。例如，在 XGBoost 的情况下，回归树*h*的惩罚取决于每棵树*T*的叶数、每个终端节点*w*的回归树得分以及超参数![](img/B15439_12_016.png)和![](img/B15439_12_017.png)。以下公式总结了这一点：

![](img/B15439_12_018.png)

因此，在每一步中，算法贪婪地添加了最能改善正则化目标的假设*h*<sub style="font-style: italic;">m</sub>。基于泰勒展开的损失函数的二阶近似可加速目标的评估，如下式所示：

![](img/B15439_12_019.png)

此处，*g*<sub style="font-style: italic;">i</sub>为给定特征值添加新学习者之前损失函数的一阶梯度，*h*<sub style="font-style: italic;">i</sub>为对应的二阶梯度（或 Hessian）值，如下公式所示：

![](img/B15439_12_020.png)

![](img/B15439_12_021.png)

XGBoost 算法是第一个利用损失函数近似计算给定树结构的最佳叶分数和损失函数对应值的开源算法。分数由终端节点中样本的梯度和 Hessian 之和的比率组成。它使用该值对分割产生的信息增益进行评分，类似于我们在上一章中看到的节点杂质度量，但适用于任意损失函数。详细推导见 Chen 和 Guestrin（2016）。

### 简化的分裂查找算法

sklearn 最初的梯度提升实现找到了最佳分割，该分割列举了连续特征的所有选项。这种**精确贪婪算法**在计算上要求非常高，因为每个特征可能有非常多的分割选项。当数据不适合内存或在多台机器上的分布式环境中进行培训时，这种方法面临额外的挑战。

**近似拆分查找**算法通过将特征值分配给用户确定的一组存储单元来减少拆分点的数量，这也可以大大减少训练期间的内存需求。这是因为每个箱子只需要存储一个值。XGBoost 引入了一种**分位数草图**算法，该算法将加权训练样本划分为百分位箱，以实现均匀分布。XGBoost 还引入了处理由缺失值、频繁的零梯度统计和一个热编码引起的稀疏数据的能力，并且可以学习给定分割的最佳默认方向。因此，该算法只需要计算非缺失值。

相比之下，LightGBM 使用基于**梯度的单侧采样**（**GOSS**）来排除小梯度的大部分样本，并且仅使用余数来估计信息增益并相应地选择分割值。具有更大梯度的样本需要更多的训练，并且往往对信息增益贡献更大。

LightGBM 还使用互斥特性绑定来组合互斥特性，因为它们很少同时采用非零值，以减少特性的数量。因此，LightGBM 在发布时是最快的实现，而且通常性能最好。

### 深度生长与叶片生长

LightGBM 与XGBoost 和 CatBoost 的不同之处在于它如何区分要拆分的节点的优先级。LightGBM 决定按叶分割，也就是说，它分割最大化信息增益的叶节点，即使这会导致不平衡的树。相比之下，XGBoost 和 CatBoost 按深度扩展所有节点，并在添加更多级别之前，首先在给定的深度级别拆分所有节点。这两种方法以不同的顺序展开节点，并将产生不同的结果，除了完整树。下图说明了这两种方法：

![](img/B15439_12_06.png)

图 12.6：深度方向与叶片方向的生长

LightGBM 的叶向分裂倾向于增加模型的复杂性，可能加快收敛速度，但也会增加过度拟合的风险。具有*n*水平的深度方向生长的树最多有 2 个<sup style="font-style: italic;">n</sup>终端节点，而具有 2 个<sup style="font-style: italic;">n</sup>叶片的叶片方向的树可以具有显著更多的水平，并且在某些叶片中包含相应更少的样本。因此，调整 LightGBM 的`num_leaves`设置需要格外小心，库允许我们同时控制`max_depth`，以避免过度的节点不平衡。LightGBM 的最新版本还提供了深度方向的树生长。

### 基于 GPU 的培训

所有新的实现都支持在一个或多个 GPU 上进行训练和预测，以实现显著的加速。它们与当前支持 CUDA 的 GPU 兼容。安装要求各不相同，且发展迅速。XGBoost 和 CatBoost 实现适用于几个当前版本，但 LightGBM 可能需要本地编译（有关文档链接，请参阅 GitHub）。

加速比取决于库和数据类型，从低的一位数倍数到几十倍的因数不等。激活 GPU 只需要更改任务参数，而不需要其他超参数修改。

### DART–加性回归树的退出

Rashmi 和Gilad Bachrach（2015）提出了一种新的模型来训练梯度提升树，以解决他们标记为**过度专业化**的问题：在后期迭代中添加的树往往只会影响少数实例的预测，而对其余实例的影响较小。但是，模型的样本外性能可能会受到影响，并且可能对少量树的贡献过于敏感。

新算法采用了辍学算法，这些辍学算法已成功用于学习更精确的深层神经网络，在训练过程中，它们使神经连接的随机部分静音。因此，更高层中的节点不能依靠几个连接来传递预测所需的信息。这种方法为许多任务的深层神经网络的成功做出了重大贡献，也被用于其他学习技术，如逻辑回归。

**DART**或**加性回归树的退出**在树的层次上运行，并使完整树相对于单个特征静音。目标是让使用 DART 生成的集合中的树对最终预测做出更均匀的贡献。在某些情况下，这已被证明能对排名、回归和分类任务产生更准确的预测。该方法首先在 LightGBM 中实现，也可用于 XGBoost。

### 范畴特征的处理

CatBoost 和 LightGBM实现直接处理分类变量，无需伪编码。

CatBoost 实现（以其对分类特性的处理而命名）除了自动热编码外，还包括几个处理此类特性的选项。它将单个要素的类别或多个要素的类别组合指定给数值。换句话说，CatBoost 可以从现有功能的组合中创建新的分类功能。与单个特征或特征组合的类别级别相关的数值取决于它们与结果值的关系。在分类情况下，这与观察正类的概率有关，正类是在样本上累积计算的，基于先验知识，并使用平滑因子。有关更详细的数值示例，请参阅 CatBoost 文档。

LightGBM 实现对分类特征的级别进行分组，以最大化组内结果值的同质性（或最小化差异）。XGBoost 实现不直接处理分类特性，需要一个热（或虚拟）编码。

### 附加功能和优化

XGBoost 在几个方面优化了计算，以支持多线程。最重要的是，它将数据保存在压缩的列块中的内存中，每个列都按相应的特征值排序。它在培训之前计算一次输入数据布局，并在整个过程中重用它以分摊前期成本。因此，在列上搜索分割统计信息成为分位数的线性扫描，可以并行进行，并支持列子采样。

随后发布的 LightGBM 和 CatBoost 库基于这些创新，LightGBM 通过优化线程和减少内存使用进一步加快了培训。由于其开源性质，图书馆往往会随着时间的推移而趋同。

XGBoost 还支持**单调性约束**。这些约束确保给定特征的值仅在其整个范围内与结果正相关或负相关。它们对于合并已知为真的关于模型的外部假设非常有用。

# 带提振的多空交易策略

在本节中，我们将设计、实施和评估由梯度推进模型生成的每日收益预测驱动的美国股票交易策略。我们将使用 Quandl Wiki 数据设计一些简单的功能（详见笔记本`preparing_the_model_data`，选择一个模型，同时使用 2015/16 作为验证期，并在 2017 年进行样本外测试。

与前面的示例一样，我们将展示一个框架，并构建一个特定的示例，您可以对其进行调整以运行自己的实验。有许多方面可以改变，从资产类别和投资领域到更精细的方面，如特征、持有期或交易规则。例如，参见*附录*中的阿尔法因子库，以了解许多其他特性。

我们将保持交易策略简单，只使用一个 ML 信号；现实生活中的应用程序可能会使用来自不同来源的多个信号，例如在不同数据集上训练的互补 ML 模型，或者具有不同的前瞻或回望周期。它还将使用复杂的风险管理，从简单的止损到风险价值分析。

## 使用 LightGBM 和 CatBoost 生成信号

XGBoost、LightGBM 和 CatBoost提供了多种语言的接口，包括 Python，并且都有一个与其他 scikit 学习功能兼容的 scikit 学习接口，例如`GridSearchCV`以及它们自己训练和预测梯度提升模型的方法。我们在本章前两节中使用的笔记本`boosting_baseline.ipynb`说明了每个库的 scikit 学习界面。笔记本比较了各种库的预测性能和运行时间。它通过训练提振模型来预测 2001-2018 年期间的月度美国股票回报率，我们在*第 4 章*、*金融特征工程——如何研究阿尔法因素*中创建了这些特征。

下图的左面板显示了使用所有实施的默认设置对 1 个月股票价格变动进行预测的准确性，根据 12 倍交叉验证产生的平均 AUC 进行测量：

![](img/B15439_12_07.png)

图 12.7：各种梯度提升模型的预测性能和运行时

**预测性能**在 0.525 到 0.541 之间变化。这看起来可能是一个小范围，但在随机基准 AUC 为 0.5 的情况下，表现最差的模型比基准 AUC 提高了 5%，而表现最好的模型比基准 AUC 提高了 8%，这反过来又相对提高了 60%。带有 GPU 和 LightGBM（使用整数编码的分类变量）的 CatBoost 性能最佳，突出了将分类变量转换为前面概述的数字变量的好处。

实验的**运行时间**比预测性能变化更显著。LightGBM 在这个数据集上比 XGBoost 或 CatBoost（使用 GPU）快 10 倍，同时提供非常相似的预测性能。由于这一巨大的速度优势以及 GPU 并非人人可用，我们将重点介绍 LightGBM，但也将说明如何使用 CatBoost；XGBoost 的工作原理与两者非常相似。

使用 LightGBM 和CatBoost 机型需要：

1.  创建特定于库的二进制数据格式
2.  配置和调整各种超参数
3.  评估结果

我们将在以下章节中描述这些步骤。笔记本`trading_signals_with_lightgbm_and_catboost`包含本小节的代码示例，除非另有说明。

### 从 Python 到 C++——创建二进制数据格式

LeTGBM 和 CatBoost 是用 C++语言编写的，在预计算特征统计之前，将 Python 对象翻译成二进制数据格式，如熊猫数据文件，如前一节所述。结果可以持续，以加速后续训练的开始。

我们将在 2016 年底之前对上一节中提到的数据集进行子集，以交叉验证各种回溯和前瞻窗口的几种模型配置，以及不同的前滚周期和超参数。我们的模型选择方法将与上一章中使用的方法类似，并使用*第 7 章*中引入的自定义`MultipleTimeSeriesCV`、*线性模型–从风险因素到回报预测*。

我们按照 LightGBM 的预期，选择序列和验证集，识别标签和特征，整数编码从零开始的分类变量（只要分类代码的值小于 2<sup class="Superscript--PACKT-">32</sup>，则不需要，但避免警告）：

```py
data = (pd.read_hdf('data.h5', 'model_data')
            .sort_index()
            .loc[idx[:, :'2016'], :])
labels = sorted(data.filter(like='fwd').columns)
features = data.columns.difference(labels).tolist()
categoricals = ['year', 'weekday', 'month']
for feature in categoricals:
    data[feature] = pd.factorize(data[feature], sort=True)[0] 
```

笔记本电脑示例迭代了许多配置，可以选择使用随机样本来加速使用不同子集的模型选择。目标是识别最有影响力的参数，而无需尝试所有可能的组合。

为此，我们创建了二进制`Dataset`对象。对于 LightGBM，如下所示：

```py
import lightgbm as lgb
outcome_data = data.loc[:, features + [label]].dropna()
lgb_data = lgb.Dataset(data=outcome_data.drop(label, axis=1),
                           label=outcome_data[label],
                           categorical_feature=categoricals,
                           free_raw_data=False) 
```

CatBoost 数据结构称为`Pool`，其工作原理类似：

```py
cat_cols_idx = [outcome_data.columns.get_loc(c) for c in categoricals]
catboost_data = Pool(label=outcome_data[label],
                    data=outcome_data.drop(label, axis=1),
                    cat_features=cat_cols_idx) 
```

对于这两个库，我们根据结果信息确定要转换为数值变量的分类变量，如前一节所述。CatBoost 实现需要使用索引而不是标签来标识功能列。

在交叉验证期间，我们可以使用`MultipleTimeSeriesCV`提供的序列和验证集索引简单地对二进制数据集进行切片，如下所示，将两个示例组合成一个片段：

```py
for i, (train_idx, test_idx) in enumerate(cv.split(X=outcome_data)):
   lgb_train = lgb_data.subset(train_idx.tolist()).construct()
   train_set = catboost_data.slice(train_idx.tolist()) 
```

### 如何调整超参数

LightGBM 和 CatBoost 实现附带了许多允许细粒度控制的超参数。每个库都有以下参数设置：

*   指定任务目标和学习算法
*   设计基础学习者
*   应用各种正则化技术
*   训练期间应尽早停车
*   在 CPU 上启用 GPU 或并行化

每个库的文档详细说明了各种参数。因为它们实现了相同算法的变体，所以参数可能引用相同的概念，但在不同的库中具有不同的名称。GitHub 存储库列出了澄清哪些 XGBoost 和 LightGBM 参数具有可比效果的资源。

#### 目标和损失函数

这些库支持几种增强算法，包括树和线性基学习器的梯度增强，以及 LightGBM 和 XGBoost 的 DART。LightGBM 还支持我们前面描述的 GOSS 算法以及随机林。

梯度提升的吸引力在于对任意可微损失函数的有效支持，每个库都为回归、分类和排序任务提供了各种选项。除了选择的损失函数外，还可以使用其他评估指标来监控培训和交叉验证期间的绩效。

#### 学习参数

梯度推进模型通常使用决策树来捕获特征交互，而单个树的大小是最重要的调整参数。XGBoost 和 CatBoost 将`max_depth`默认值设置为 6。相比之下，LightGBM 使用默认的`num_leaves`值 31，这对应于平衡树的五个级别，但不限制级别的数量。为避免过度装配，`num_leaves`应低于 2<sup class="Superscript--PACKT-">最大深度</sup>。例如，对于性能良好的`max_depth`值 7，您可以将`num_leaves`设置为 70–80，而不是 2<sup class="Superscript--PACKT-">7</sup>=128，或者直接约束`max_depth`。

树或提升迭代的数量定义了集合的总体大小。所有库都支持`early_stopping`在给定的迭代次数内，一旦丢失函数没有进一步的改进，就中止训练。因此，设置大量迭代并基于验证集上的预测性能停止训练通常是最有效的。但是，请记住，由于隐含的前瞻性偏差，验证错误将向上偏移。

这些库还允许使用自定义损失度量来跟踪培训和验证性能，并执行`early_stopping`。笔记本说明了如何为 LightGBM 和 CatBoost 编码**信息系数**（**IC**）。然而，我们不会依赖`early_stopping`进行实验，以避免上述偏差。

#### 正规化

所有库都为基础学习者实施正则化策略，例如样本数的最小值或分割和叶节点所需的最小信息增益。

它们还支持在集合级别使用收缩进行正则化，收缩是通过限制新树贡献的学习速率实现的。还可以通过回调函数实现自适应学习速率，随着训练的进行，回调函数会降低学习速率，这一点已成功应用于神经网络中。此外，可以使用 L1 或 L2 正则化来约束梯度增强损失函数，类似于岭回归和套索回归模型，例如，通过增加添加更多树的惩罚。

该库还允许使用袋装或列子抽样对随机森林的树木生长进行随机化，并对预测误差进行去相关处理，以减少总体方差。近似分割查找的特征量化增加了更大的箱子，作为防止过度拟合的附加选项。

#### 随机网格搜索

为了探索 hyperparameter空间，我们指定了要组合测试的关键参数的值。sklearn 库支持`RandomizedSearchCV`交叉验证从指定分布中随机抽样的参数组合子集。我们将实现一个自定义版本，它允许我们监控性能，这样一旦我们对结果感到满意，就可以中止搜索过程，而不是预先指定一组迭代次数。

为此，我们为每个库的相关超参数指定选项，使用 itertools 库提供的笛卡尔积生成器生成所有组合，并洗牌结果。

在 LightGBM 的情况下，我们关注学习速率、树的最大大小、训练期间特征空间的随机化以及分割所需的最小数据点数量。这将产生以下代码，其中我们随机选择一半的配置：

```py
learning_rate_ops = [.01, .1, .3]
max_depths = [2, 3, 5, 7]
num_leaves_opts = [2 ** i for i in max_depths]
feature_fraction_opts = [.3, .6, .95]
min_data_in_leaf_opts = [250, 500, 1000]
cv_params = list(product(learning_rate_ops,
                         num_leaves_opts,
                         feature_fraction_opts,
                         min_data_in_leaf_opts))
n_params = len(cv_params)
# randomly sample 50%
cvp = np.random.choice(list(range(n_params)),
                           size=int(n_params / 2), 
                           replace=False)
cv_params_ = [cv_params[i] for i in cvp] 
```

现在，我们基本上准备就绪：在每次迭代期间，我们基于`lookahead`、`train_period_length`和`test_period_length`参数创建一个`MultipleTimeSeriesCV`实例，并在 2 年的时间内对所选的超参数进行相应的交叉验证。

请注意，我们为一系列集合大小生成验证预测，以便我们可以推断最佳迭代次数：

```py
num_iterations = [10, 25, 50, 75] + list(range(100, 501, 50))
num_boost_round = num_iterations[-1]
for lookahead, train_length, test_length in test_params:
   n_splits = int(2 * YEAR / test_length)
   cv = MultipleTimeSeriesCV(n_splits=n_splits,
                             lookahead=lookahead,
                             test_period_length=test_length,
                             train_period_length=train_length)
   for p, param_vals in enumerate(cv_params_):
       for i, (train_idx, test_idx) in enumerate(cv.split(X=outcome_data)):
           lgb_train = lgb_data.subset(train_idx.tolist()).construct()
           model = lgb.train(params=params,
                             train_set=lgb_train,
                             num_boost_round=num_boost_round,
                             verbose_eval=False)
           test_set = outcome_data.iloc[test_idx, :]
           X_test = test_set.loc[:, model.feature_name()]
           y_test = test_set.loc[:, label]
           y_pred = {str(n): model.predict(X_test, num_iteration=n) for n in num_iterations} 
```

请参阅笔记本`trading_signals_with_lightgbm_and_catboost`了解更多详细信息，包括如何记录结果以及如何计算和捕获评估结果所需的各种指标，我们将在下一步讨论这些指标。

### 如何评估结果

现在，众多配置的交叉验证产生了大量结果，我们需要评估预测性能，以确定为我们的未来交易策略生成最可靠和最有利可图信号的模型。笔记本`evaluate_trading_signals`包含本节的代码示例。

我们制作了大量 LightGBM 模型，因为它比 CatBoost 运行快一个数量级，并将相应地演示一些评估策略。

#### 交叉验证结果–LightGBM 与 CatBoost

首先，我们比较了两个库生成的模型在所有配置中的预测性能，包括其验证 IC，包括整个验证期间和每日预测的平均值。

下图显示，LightGBM 的性能（略）优于 CatBoost，尤其是的视野更长。这不是一个完全公平的比较，因为我们为 LightGBM 运行了更多的配置，这也不足为奇地显示了更广泛的结果分散性：

![](img/B15439_12_08.png)

图 12.8:LightGBM 和 CatBoost 模型在三个预测范围内的总体和每日 IC

无论如何，我们将关注LightGBM 结果；查看笔记本`trading_signals_with_lightgbm_and_catboost`和`evaluate_trading_signals`了解更多关于 CatBoost 的详细信息或运行您自己的实验。

考虑到模型结果的巨大分散性，让我们更仔细地看看性能最好的参数设置。

#### 最佳性能参数设置

表现最好的 LightGBM 模型在三个不同的预测视界中使用以下参数（详见笔记本）：

<colgroup><col> <col> <col> <col> <col> <col> <col> <col> <col></colgroup> 
| 先行 | 学习率 | #叶子 | 特征分数 | 叶中的最小数据 | 日均 | 全面的 |
| 集成电路 | #轮 | 集成电路 | #轮 |
| 1. | 0.3 | 4. | 95% | 1,000 | 1.70 | 75 | 4.41 | 50 |
| 1. | 0.3 | 4. | 95% | 250 | 1.34 | 250 | 4.36 | 25 |
| 1. | 0.3 | 4. | 95% | 1,000 | 1.70 | 75 | 4.30 | 75 |
| 5. | 0.1 | 8. | 95% | 1,000 | 3.95 | 300 | 10.46 | 300 |
| 5. | 0.3 | 4. | 95% | 1,000 | 3.43 | 150 | 10.32 | 50 |
| 5. | 0.3 | 4. | 95% | 1,000 | 3.43 | 150 | 10.24 | 150 |
| 21 | 0.1 | 8. | 60% | 500 | 5.84 | 25 | 13.97 | 10 |
| 21 | 0.1 | 32 | 60% | 250 | 5.89 | 50 | 11.59 | 10 |
| 21 | 0.1 | 4. | 60% | 250 | 7.33 | 75 | 11.40 | 10 |

请注意，在三个预测视界中，浅树产生的整体 IC 最好。4.5 年以上的长期培训也产生了更好的效果。

#### 超参数影响-线性回归

接下来，我们想了解超参数和日常预测结果之间是否存在系统的统计关系。为此，我们将使用各种 LightGBM 超参数设置作为虚拟变量，并使用每日验证 IC 作为结果来运行线性回归。

*图 12.9*中的图表显示了 1 天和 21 天预测期的系数估计及其置信区间。对于较短的视界，较长的回望期、较高的学习率和较深的树（更多的叶节点）具有积极的影响。对于较长的地平线，情况就不那么清楚了：较短的树木效果更好，但回望期并不显著。更高的特征采样率也有帮助。在这两种情况下，较大的合奏效果更好。请注意，这些结果仅适用于此特定示例。

![](img/B15439_12_09.png)

图 12.9：不同预测期的系数估计及其置信区间

#### 用 IC 代替信息系数

我们对前五个模型进行平均，并向 Alphalens 提供相应的价格，以便计算在不同持有期内投资于每日因子五分位数的等权投资组合所获得的平均期内回报：

<colgroup><col> <col> <col> <col> <col></colgroup> 
| 米制的 | 持有期 |
| 1D | 5D | 10 天 | 21D |
| 平均周期利差（bps） | 12.1654 | 6.9514 | 4.9465 | 4.4079 |
| 安。阿尔法 | 0.1759 | 0.0776 | 0.0446 | 0.0374 |
| 贝塔 | 0.0891 | 0.1516 | 0.1919 | 0.1983 |

我们发现顶部和底部五分位数之间的差距为 12 个基点，这意味着年度阿尔法为 0.176，而贝塔值为 0.089（参见*图 12.10*：

![](img/B15439_12_10.png)

图 12.10：按因子分位数划分的平均和累积收益

以下图表显示了最佳性能模型在 2 年验证期内 1 天和 21 天预测的季度滚动 IC：

![](img/B15439_12_11.png)

图 12.11：1 天和 21 天回报预测的滚动 IC

短期模型和长期模型的平均 IC 分别为 2.35 和 8.52，并且在样本中的大部分天数内保持正值。

在我们选择模型、生成预测、定义交易策略和评估其性能之前，我们现在将了解如何进一步了解模型的工作原理。

## 黑匣子内部–解释 GBM 结果

理解模型预测某种结果的原因非常重要，原因包括信任、可操作性、责任和调试。当目标是更多地了解所研究现象的潜在驱动因素时，深入了解特征与模型揭示的结果之间的非线性关系以及特征之间的相互作用也很有价值。

要深入了解树集合方法（如梯度增强或随机森林模型）所做的预测，一种常见的方法是将特征重要性值赋予每个输入变量。这些特征重要性值可以针对单个预测单独计算，也可以针对整个数据集（即所有样本）全局计算，以获得模型如何进行预测的更高层次的视角。

本节的代码示例在笔记本`model_interpretation`中。

### 特征重要性

计算全局特征重要性值的主要方法有三种：

*   **增益**：利奥·布雷曼（Leo Breiman）于 1984 年引入的这一经典方法，使用了对给定特征的所有分裂产生的损失或杂质的总减少量。动机在很大程度上是启发式的，但它是选择特征的常用方法。
*   **分割计数**：这是一种替代方法，根据基于结果信息增益的特征选择，统计特征用于分割决策的频率。
*   **排列**：这种方法随机排列测试集中的特征值，并测量模型的误差变化程度，假设一个重要特征会导致预测误差大幅增加。不同的排列选择会导致这种基本方法的替代实现。

计算单个预测的特征相关性的个性化特征重要性值不太常见。这是因为可用的模型不可知解释方法比特定于树的方法慢得多。

所有梯度增强实现在训练后作为模型属性提供特征重要性分数。LightGBM 库提供两个版本，如下表所示：

*   **增益**：特性对减少损耗的贡献
*   **拆分**：该功能被使用的次数

使用经过训练的模型的`.feature_importance()`方法和相应的`importance_type`参数可以获得这些值。对于性能最佳的 LightGBM 模型，20 个最重要特征的结果如*图 12.12*所示：

![](img/B15439_12_12.png)

图 12.12:LightGBM 功能重要性

时间段指标占主导地位，其次是最新收益、标准化 ATR、部门虚拟和动量指标（有关实施细节，请参阅笔记本）。

### 部分依赖图

除了个别特征对模型预测的总结贡献外，部分依赖图还显示了目标变量和一组特征之间的关系。梯度提升树的非线性特性导致这种关系依赖于所有其他特征的值。因此，我们将这些特征边缘化。通过这样做，我们可以将部分依赖性解释为预期的目标响应。

我们只能对单个特征或特征对进行部分依赖性可视化。后者生成等高线图，显示特征值组合如何产生不同的预测概率，如以下代码所示：

```py
fig, axes = plot_partial_dependence(estimator=best_model,
                                    X=X,
                                    features=['return_12m', 'return_6m', 
                                              'CMA', ('return_12m',
                                                      'return_6m')],
                                    percentiles=(0.01, 0.99),
                                    n_jobs=-1,
                                    n_cols=2,
                                    grid_resolution=250) 
```

经过一些额外的格式化（见配套笔记本），我们得到的结果如*图 12.13*所示：

![](img/B15439_12_13.png)

图 12.13:scikit learn GradientBoostingClassifier 的部分相关性图

右下方的图显示了在[1%，99%]百分位剔除异常值后，考虑到滞后 12 个月和 6 个月收益的值范围，下个月正收益概率的相关性。`month_9`变量是一个虚拟变量，因此类似于图的阶跃函数。我们还可以在 3D 中可视化依赖关系，如以下代码所示：

```py
targets = ['return_12m', 'return_6m']
pdp, axes = partial_dependence(estimator=gb_clf,
                               features=targets,
                               X=X_,
                               grid_resolution=100)
XX, YY = np.meshgrid(axes[0], axes[1])
Z = pdp[0].reshape(list(map(np.size, axes))).T
fig = plt.figure(figsize=(14, 8))
ax = Axes3D(fig)
surf = ax.plot_surface(XX, YY, Z,
                       rstride=1,
                       cstride=1,
                       cmap=plt.cm.BuPu,
                       edgecolor='k')
ax.set_xlabel(' '.join(targets[0].split('_')).capitalize())
ax.set_ylabel(' '.join(targets[1].split('_')).capitalize())
ax.set_zlabel('Partial Dependence')
ax.view_init(elev=22, azim=30) 
```

这产生了 1 个月回报方向对滞后 6 个月和 12 个月回报的部分依赖性的以下3D 图：

![](img/B15439_12_14.png)

图 12.14：作为 3D 图的部分相关性

### 夏普利加法解释

在 2017 届 NIPS 会议上，来自华盛顿大学的 Scott Lundberg 和 Su In Lee 提出了一种新的和更精确的方法来解释个体特征对树集合模型的输出贡献的贡献，称为 OrthT1，Shapley 添加剂解释，T2，或 Po.T3。

这一新算法与观察结果不同，即树集合的特征属性方法（如我们前面看到的）是不一致的，也就是说，模型中增加特征对输出的影响的变化会降低该特征的重要性值（请参阅 GitHub 上的参考资料，以了解这方面的详细说明）。

SHAP 价值观统一了协作博弈论和局部解释的思想，并且已经证明，基于预期，SHAP 价值观在理论上是最优的、一致的，并且在局部是准确的。最重要的是，Lundberg 和 Lee 开发了一种算法，该算法能够降低计算这些模型不可知的、可添加的特征属性方法的复杂性，从*O*（*TLDM*到*O*（*TLD*<sup class="Superscript--PACKT-">2</sup>，其中*T*和*M*分别为树木和地物的数量，*D*和*L*是树上树叶的最大深度和数量。这一重要创新允许在几分之一秒内解释以前难以处理的模型中的预测，这些模型具有数千棵树和功能。一个开源实现于 2017 年底推出，并与 XGBoost、LightGBM、CatBoost 和 sklear 兼容 n 树模型。

Shapley 价值观起源于博弈论，是一种为合作游戏中的每个玩家分配价值的技术，反映了他们对团队成功的贡献。形状值是博弈论概念对基于树的模型的一种改编，并针对每个特征和每个样本进行计算。它们测量特征对给定观测的模型输出的贡献。由于这个原因，SHAP 值提供了关于特征影响如何在样本间变化的不同见解，鉴于这些非线性模型中交互效应的作用，这一点很重要。

#### 如何按特征汇总形状值

为了对多个样本的特征重要性进行高层次的概述，有两种方法来绘制形状值：对所有样本进行简单的平均，类似于先前计算的全局特征重要性度量（如*图 12.15*左侧面板所示），或散点图，以显示每个样本的每个特征的影响（如图右侧面板所示）。使用来自兼容库的经过训练的模型和匹配的输入数据生成它们非常简单，如下代码所示：

```py
# load JS visualization code to notebook
shap.initjs()
# explain the model's predictions using SHAP values
explainer = shap.TreeExplainer(model)
shap_values = explainer.shap_values(X_test)
shap.summary_plot(shap_values, X_test, show=False) 
```

散点图根据所有样本的总形状值对特征进行排序，然后显示每个特征如何影响模型输出（通过形状值测量），作为特征值的函数，由其颜色表示，其中红色表示高值，蓝色表示相对于特征范围的低值：

![](img/B15439_12_15.png)

图 12.15:SHAP 汇总图

与*图 12.12*所示的常规特征重要性相比，存在一些有趣的差异；也就是说，MACD 指标以及相对回报指标更为重要。

#### 如何使用力图解释预测

下图中的力图显示了各种特征及其值对模型输出的**累积影响，在本例中为 0.6，远高于基础值 0.13（所提供数据集上的平均模型输出）。以红色突出显示且箭头指向右侧的功能可增加输出。10 月份是最重要的特征，产量从 0.338 增加到 0.537，而 2017 年则减少了产量。**

因此，我们获得了模型如何得出特定预测的详细细分，如下图所示：

![](img/B15439_12_16.png)

图 12.16：形状力图

我们还可以一次计算多个数据点或预测的**力图，并使用**聚类可视化**深入了解特定影响模式在数据集中的普遍程度。下图显示了前 1000 次观察的力图，旋转 90 度，水平叠加，并按不同特征对给定观察结果的影响排序。**

该实现使用特征形状值上数据点的分层聚集聚类来识别这些模式，并以交互方式显示结果以进行探索性分析（请参见笔记本），如以下代码所示：

```py
shap.force_plot(explainer.expected_value, shap_values[:1000,:],
                X_test.iloc[:1000]) 
```

这将产生以下输出：

![](img/B15439_12_17.png)

图 12.17：形状聚集力图

#### 如何分析特征交互

最后，SHAP 值允许我们通过将这些交互作用与主效应分离，进一步深入了解不同特征之间的交互作用。`shap.dependence_plot`可定义如下：

```py
shap.dependence_plot(ind='r01',
                     shap_values=shap_values,
                     features=X,
                     interaction_index='r05',
                     title='Interaction between 1- and 5-Day Returns') 
```

它显示 1 个月回报的不同值（x 轴上）如何影响结果（y 轴上的形状值），以 3 个月回报区分（见下图）：

![](img/B15439_12_18.png)

图 12.18:SHAP 交互图

SHAP 值在每个单独预测级别提供粒度特征属性，并通过（交互式）可视化更丰富地检查复杂模型。本节前面显示的形状摘要点图（*图 12.15*提供了比全局特征重要性条形图更具差异性的见解。单个聚集预测的力图允许更详细的分析，而形状依赖图捕获交互效应，因此，与部分依赖图相比，提供更准确和详细的结果。

与任何当前特征重要性度量一样，形状值的局限性与高度相关的变量的影响属性有关，因为它们的相似影响可以以任意方式分解。

## 基于 boosting 集合的策略回溯测试

在本节中，我们将使用 Zipline 来评估根据每日收益预测信号进入 25 个多头和 25 个空头头寸的多空策略的绩效。为此，我们将选择表现最好的模型，生成预测，并设计对这些预测起作用的交易规则。

根据我们对交叉验证结果的评估，我们将选择一个或多个模型为新的样本外周期生成信号。在本例中，我们将结合对最佳 10 LightGBM 模型的预测，以减少基于 Alphalens 计算的实心平均分位数扩展的 1 天预测范围的方差。

我们只需要获得最佳性能模型的参数设置，然后进行相应的训练。笔记本`making_out_of_sample_predictions`包含必要的代码。模型训练使用测试期间表现最佳的模型和数据的超参数设置，但在其他方面，它非常接近交叉验证期间使用的逻辑，因此我们将省略此处的细节。

在笔记本`backtesting_with_zipline`中，我们结合了前 10 个模型对验证和测试周期的预测，如下所示：

```py
def load_predictions(bundle):
    predictions = (pd.read_hdf('predictions.h5', 'train/01')
                   .append(pd.read_hdf('predictions.h5', 'test/01')
                   .drop('y_test', axis=1)))
    predictions = (predictions.loc[~predictions.index.duplicated()]
                   .iloc[:, :10]
                   .mean(1)
                   .sort_index()
                   .dropna()
                  .to_frame('prediction')) 
```

我们将使用我们在*第 8 章**ML4T 工作流【从模型到策略回溯测试】*中引入的自定义 ML 因子，导入预测并使其在管道中可访问。

我们将从验证期开始到测试期结束执行`Pipeline`。*图 12.19*显示（毫不奇怪）样本内表现稳定，年回报率为 27.3%，而样本外为 8.0%。图中右侧面板显示了相对于 S&P500 的累积收益：

<colgroup><col> <col> <col> <col></colgroup> 
| 米制的 | 全部的 | 抽样 | 样品外 |
| 年报 | 20.60% | 27.30% | 8.00% |
| 累积回报 | 75.00% | 62.20% | 7.90% |
| 年度波动率 | 19.40% | 21.40% | 14.40% |
| 夏普比率 | 1.06 | 1.24 | 0.61 |
| 最大降深 | -17.60% | -17.60% | -9.80% |
| 索提诺比率 | 1.69 | 2.01 | 0.87 |
| 歪曲 | 0.86 | 0.95 | -0.16 |
| 峰度 | 8.61 | 7.94 | 3.07 |
| 每日风险价值 | -2.40% | -2.60% | -1.80% |
| 日营业额 | 115.10% | 108.60% | 127.30% |
| 阿尔法 | 0.18 | 0.25 | 0.05 |
| 贝塔 | 0.24 | 0.24 | 0.22 |

样品中的夏普比为 1.24，样品外的夏普比为 0.61；右侧面板显示季度滚动值。样品中的α值为 0.25，样品外的α值为 0.05，β值分别为 0.24 和 0.22。最严重的提款导致 2015 年下半年亏损 17.59%：

![](img/B15439_12_19.png)

图 12.19：战略绩效累积回报和滚动夏普比率

多头交易的利润略高于平均亏损的短头交易：

<colgroup><col> <col> <col> <col></colgroup> 
| 统计摘要 | 各行各业 | 空头交易 | 长期交易 |
| 往返总次数 | 22,352 | 11,631 | 10,721 |
| 利润率 | 50.0% | 48.0% | 51.0% |
| 赢得往返旅行 | 11,131 | 5,616 | 5,515 |
| 失去往返行程 | 11,023 | 5,935 | 5,088 |
| 甚至往返 | 198 | 80 | 118 |

## 经验教训和下一步

总的来说，我们可以看到，尽管在高度流动的环境中只使用市场数据，但梯度推进模型能够提供比随机猜测更好的预测。显然，利润是绝对不能保证的，尤其是因为我们对交易成本做出了非常慷慨的假设（注意高营业额）。

然而，有几种方法可以改进这一基本框架，也就是说，通过改变参数，从更一般和战略的方面到更具体和战术的方面，例如：

1.  尝试不同的投资领域（例如，流动性股票或其他资产较少）。
2.  创造性地添加补充数据源。
3.  设计更复杂的功能。
4.  例如，使用更长或更短的等待和回望期来改变实验设置。
5.  想出更有趣的交易规则，使用多个而不是一个 ML 信号。

希望这些建议能激励您在我们制定的模板上进行构建，并提出有效的 ML 驱动交易策略！

# 为盘中策略提振

我们在*第一章*中引入了**高频交易**（**HFT**），将*机器学习用于交易——从想法到执行*，作为加速采用算法策略的关键趋势。HFT 没有客观的定义来确定其包含的活动的属性，包括持有期、订单类型（例如，被动与主动）和策略（动量或逆转、定向或流动性供应等）。然而，大多数对 HFT 的技术性处理似乎都同意，驱动 HFT 活动的数据往往是可用的最细粒度数据。通常，这将是直接来自交易所的微观结构数据，如我们在*第 2 章*、*市场和基础数据–来源和技术*中介绍的纳斯达克瘙痒数据，以演示它如何详细说明每一笔订单、每一次执行和每一次取消，从而允许重建完整的限额订单簿，至少对于股票和某些隐藏订单除外。

ML 对 HFT 的应用包括在官方交易所和暗池中优化交易执行。ML 还可用于生成交易信号，我们将在本节中介绍；另请参见 Kearns 和 Nevmyvaka（2013），了解 ML 如何在 HFT 环境中增值的更多细节和示例。

本节使用证券信息处理器生成的合并提要中的**AlgoSeek NASDAQ 100 数据集**。数据包括**分条频率**下的全国最佳投标报价和交易价格信息。它还包含一些关于价格动态的特征，例如在买入价或卖出价下的交易数量，或在滴答水平上跟随正和负价格变动的交易数量（参见*第 2 章*、*市场和基础数据–来源和技术*，以获取更多背景信息以及 GitHub 存储库中数据目录中的下载和预处理说明）。

我们将首先描述如何为这个数据集设计功能，然后训练一个梯度提升模型来预测下一分钟的成交量加权平均价格，然后评估生成的交易信号的质量。

## 高频数据的工程特性

AlgoSeek 为本书慷慨提供的数据集包含 2013-2017 年间任何给定日期 100 个股票报价器上的 50 多个变量，以分钟为频率。数据还涵盖了交易前和交易后的交易，但我们将此示例限制为正式交易时间为上午 9:30 到下午 4:00 之间的 390分钟，以在一定程度上限制数据的大小，并避免处理非正常交易活动。本节中的代码示例参见笔记本`intraday_features`。

我们将选择 12 个变量和超过 5100 万个观测值作为原始材料，为 ML 模型创建特征。其目的是预测成交量加权平均价格的 1 分钟远期收益：

```py
MultiIndex: 51242505 entries, ('AAL', Timestamp('2014-12-22 09:30:00')) to ('YHOO', Timestamp('2017-06-16 16:00:00'))
Data columns (total 12 columns):
 #   Column  Non-Null Count     Dtype  
---  ------  --------------     -----  
 0   first   51242500 non-null  float64
 1   high    51242500 non-null  float64
 2   low     51242500 non-null  float64
 3   last    51242500 non-null  float64
 4   price   49242369 non-null  float64
 5   volume  51242505 non-null  int64  
 6   up      51242505 non-null  int64  
 7   down    51242505 non-null  int64  
 8   rup     51242505 non-null  int64  
 9   rdown   51242505 non-null  int64  
 10  atask   51242505 non-null  int64  
 11  atbid   51242505 non-null  int64  
dtypes: float64(5), int64(7)
memory usage: 6.1+ GB 
```

由于数据的内存占用较大，我们仅创建 20 个简单功能，即：

*   在过去的 10 分钟中，每一分钟的延迟返回。
*   在一个酒吧里，股票交易的上升和下降数量除以股票总数。
*   交易价格相同（重复）且在酒吧期间上涨或下跌的股票数量除以股票总数。
*   按买入价与卖出价交易的股票数量之差，除以酒吧期间的总交易量。
*   多项技术指标，包括均势、商品渠道指数、随机 RSI（详见*附录*、*α因子库*）。

我们将确保转移数据以避免前瞻性偏差，如使用 TA Lib 实现的货币流指数计算所示：

```py
data['MFI'] = (by_ticker
               .apply(lambda x: talib.MFI(x.high,
                                          x.low,
                                          x['last'],
                                          x.volume,
                                          timeperiod=14)
                      .shift())) 
```

下图显示了单个特征预测内容的独立评估，使用其与 1 分钟远期收益的等级相关性。它表明，最近的滞后回报率可能是信息量最大的变量：

![](img/B15439_12_20.png)

图 12.20：高频特征的信息系数

我们现在可以使用这些特性来训练梯度增强模型。

## 带 LightGBM 的分频信号

为了为我们的 HFT 策略生成预测性的信号，我们将训练一个 LightGBM 推进模型来预测 1 分钟的远期收益。该模型在训练模型期间接收 12 个月的分钟数据，并生成随后 21 个交易日的样本外预测。我们将在 24 次列车测试中重复此步骤，以涵盖我们 5 年样本的最后 2 年。

培训过程严格遵循前面的 LightGBM 示例；具体实施详见笔记本`intraday_model`。

一个关键区别是习惯`MultipleTimeSeriesCV`对分钟频率的适应；我们将参考`MultiIndex`的`date_time`级别（具体实现请参见笔记本）。我们根据每天 390 次观察结果计算列车长度和测试周期，如下所示：

```py
DAY = 390   # minutes; 6.5 hrs (9:30 - 15:59)
MONTH = 21  # trading days
n_splits = 24
cv = MultipleTimeSeriesCV(n_splits=n_splits,
                          lookahead=1,
                          test_period_length=MONTH * DAY,
                          train_period_length=12 * MONTH * DAY,
                          date_idx='date_time') 
```

较大的数据量大大增加了训练时间，因此我们使用默认设置，但将每个集合的树数设置为 250。我们使用以下`ic_lgbm()`自定义度量定义跟踪测试集上的 IC，该定义传递给模型的`.train()`方法。

自定义度量接收模型预测和二进制训练数据集，我们可以使用它们计算任何感兴趣的度量；请注意，我们将`is_higher_better`设置为`True`，因为模型默认情况下会最小化损失函数（有关更多信息，请参阅 LightGBM 文档）：

```py
def ic_lgbm(preds, train_data):
    """Custom IC eval metric for lightgbm"""
    is_higher_better = True
    return 'ic', spearmanr(preds, train_data.get_label())[0], is_higher_better
model = lgb.train(params=params,
                  train_set=lgb_train,
                  valid_sets=[lgb_train, lgb_test],
                  feval=ic_lgbm,
                  num_boost_round=num_boost_round,
                  early_stopping_rounds=50,
                  verbose_eval=50) 
```

在 250 次迭代中，验证 IC 在大多数情况下仍在改进，因此我们的结果不是最优的，但这样的训练已经需要几个小时。现在让我们来看看我们的模型生成的信号的预测内容。

## 评估交易信号质量

现在，我们想知道该模型的样本外预测准确度如何，以及它们是否可以作为有利可图的交易策略的基础。

首先，我们计算所有预测和每日的 IC，如下所示：

```py
ic = spearmanr(cv_preds.y_test, cv_preds.y_pred)[0]
by_day = cv_preds.groupby(cv_preds.index.get_level_values('date_time').date)
ic_by_day = by_day.apply(lambda x: spearmanr(x.y_test, x.y_pred)[0])
daily_ic_mean = ic_by_day.mean()
daily_ic_median = ic_by_day.median() 
```

在 2 年的样本测试中，我们获得了具有统计意义的正 1.90。每日平均 IC 为 1.98，中值 IC 为 1.91。

这些结果清楚地表明，这些预测包含了关于短期价格变动方向和规模的有意义的信息，我们可以将这些信息用于交易策略。

接下来，我们计算每十分之一预测的平均和累积远期收益：

```py
dates = cv_preds.index.get_level_values('date_time').date
cv_preds['decile'] = (cv_preds.groupby(dates, group_keys=False)
min_ret_by_decile = cv_preds.groupby(['date_time', 'decile']).y_test.mean()
                      .apply(lambda x: pd.qcut(x.y_pred, q=10))))
cumulative_ret_by_decile = (min_ret_by_decile
                            .unstack('decile')
                            .add(1)
                            .cumprod()
                            .sub(1)) 
```

*图 12.21*显示结果。左侧面板显示了每十分位数 1 分钟的平均回报率，并显示了每分钟 0.5 个基点的平均价差。右面板显示了投资于每十分位的等权重投资组合的累积回报，表明在交易成本之前——多空策略似乎具有吸引力：

![](img/B15439_12_21.png)

图 12.21：按十分位数划分的 1 分钟平均回报率和累积回报率

使用分钟数据的回溯测试非常耗时，因此我们省略了这一步；不过，请随意试用 Zipline 或 backtrader，在更现实的交易成本假设或使用适当的风险控制下评估此策略。

# 总结

在本章中，我们探讨了梯度推进算法，该算法用于以顺序方式构建集合，添加了一个浅层决策树，该决策树仅使用极少量的特征来改进已做出的预测。我们看到了梯度增强树如何能够非常灵活地应用于广泛的损失函数，以及如何提供许多机会来调整模型以适应给定的数据集和学习任务。

最近的实现极大地促进了梯度提升的使用。他们通过加快培训过程，对功能的重要性和个人预测的驱动因素提供更加一致和详细的见解来实现这一点。

最后，我们开发了一个简单的交易策略，该策略由一组梯度提升模型驱动，至少在显著的交易成本之前，该模型实际上是有利可图的。我们还了解了如何对高频数据使用梯度增强。

在下一章中，我们将转向机器学习的贝叶斯方法。