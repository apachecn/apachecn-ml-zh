# 17

# 交易深度学习

本章从第 4 部分开始，介绍了几种**深度学习**（**DL**建模技术如何对投资和交易有用。DL 在许多领域取得了众多**突破，从图像和语音识别到引起广泛关注的机器人和智能代理，并恢复了**人工智能**（**AI**的大规模研究。人们对快速发展将继续下去抱有很高的期望，对困难的实际问题将出现更多的解决办法。**

在本章中，我们将介绍**前馈神经网络**，以介绍与以下章节中涵盖的各种 DL 体系结构相关的神经网络工作的关键要素。更具体地说，我们将演示如何使用**反向传播算法**高效地训练大型模型，并管理过度拟合的风险。我们还将展示如何使用流行的 TensorFlow 2 和 PyTorch 框架，我们将在第 4 部分中利用这些框架。

最后，我们将根据深度前馈神经网络产生的信号开发、回溯测试和评估交易策略。我们将设计和调整神经网络，并分析关键超参数选择如何影响其性能。

总之，在阅读本章并阅读随附笔记本后，您将了解：

*   DL 如何解决复杂领域中的人工智能挑战
*   推动 DL 目前流行的关键创新
*   前馈网络如何从数据中学习表示
*   用 Python 设计和训练深层**神经网络**（**NNs**）
*   使用 Keras、TensorFlow 和 PyTorch 实现深度 NNs
*   构建和调整深度神经网络以预测资产回报
*   基于深度神经网络信号的交易策略设计与回溯测试

在下面的章节中，我们将在此基础上设计适合于不同投资应用的各种架构，特别关注替代文本和图像数据。

其中包括为时序数据（如时间序列或自然语言）量身定制的**递归神经网络**（**RNNs**），以及**卷积神经网络**（**CNNs**），它们特别适合于图像数据，但也可以与时序数据一起使用。我们还将介绍深度无监督学习，包括自动编码器和**生成性对抗网络**（**GANs**）以及强化学习，以培训从环境中交互学习的代理。

您可以在 GitHub 存储库的相应目录中找到本章的代码示例以及指向其他资源的链接。笔记本电脑包括图像的彩色版本。

# 深度学习——什么是新的，为什么重要

*第 2 部分*中涵盖的**机器学习**（**ML**算法可以很好地解决各种重要问题，包括文本数据，如*第 3 部分*中所示。然而，他们在解决诸如识别语音或对图像中的对象进行分类等核心人工智能问题方面却不太成功。这些局限性推动了 DL 的发展，最近 DL 的突破极大地促进了 AI 兴趣的重新兴起。关于包含并扩展本节许多要点的全面介绍，请参见古德费罗、本吉奥和库尔维尔（2016 年），或者更简短的版本，请参见勒昆、本吉奥和辛顿（2015 年）。

在本节中，我们将概述 DL 如何克服其他 ML 算法的许多限制。这些限制特别限制了高维和非结构化数据的性能，这些数据需要复杂的工作来提取信息性特征。

我们在*第 2 部分*和*第 3 部分*中介绍的 ML 技术最适合处理具有定义良好特征的结构化数据。例如，我们在*第 14 章*、*交易文本数据–情绪分析*中看到了如何使用文档文本矩阵将文本数据转换为表格数据。DL 克服了**设计信息性特征**的挑战，可能是通过手工，通过学习更好地捕捉其结果特征的数据表示。

更具体地说，我们将了解 DL 如何学习数据的**层次表示，以及为什么这种方法适用于高维非结构化数据。我们将描述 NNs 如何使用多层、深层架构来组成一组嵌套函数并发现层次结构。这些函数基于前一层的学习计算每一层中数据的连续且日益抽象的表示。我们还将研究反向传播算法如何调整网络参数，以使这些表示最符合模型的目标。**

我们还将简要介绍 DL 如何适应 AI 的发展，以及旨在实现 AI 当前目标的各种方法。

## 层次特征可以驯服高维数据

如*第 2 部分*所述，监督学习的关键挑战是将训练数据推广到新样本。随着数据维数的增加，泛化变得越来越困难。我们在*第 13 章*、*数据驱动风险因素和无监督学习的资产配置*中遇到了这些困难的根源，如维度诅咒。

这种诅咒的一个方面是体积随维数呈指数增长：对于边长为 10 的超立方体，当其维数从 3 增加到 4 时，体积从 10<sup class="Superscript--PACKT-">3</sup>增加到 10<sup class="Superscript--PACKT-">4</sup>。相反，给定样本量的**数据密度呈指数下降**。换句话说，维持一定密度所需的观测数量呈指数增长。

另一个方面是，当允许特性和输出在越来越多的维度上变化时，它们之间的功能关系可能会变得更加复杂。正如*第 6 章**机器学习过程*中所述，ML 算法在高维空间中努力学习**任意函数，因为候选函数的数量呈指数增长，而用于推断关系的数据密度同时下降。为了缓解这个问题，算法假设目标函数属于某一类，并对在该类中搜索当前问题的最优解施加约束。**

此外，算法通常假设新点处的输出应与附近训练点处的输出相似。如 k-最近邻算法所示（参见*第 6 章*，*机器学习过程*），先前的**平滑性**或局部恒常性假设假设假定学习函数在小区域内不会发生太大变化。然而，随着维数的增加，数据密度呈指数下降，训练样本之间的距离自然会增加。因此，随着目标函数潜在复杂性的增加，邻近训练示例的概念变得不那么有意义。

对于传统的 ML 算法，参数和所需训练样本的数量通常与算法能够识别的输入空间中的区域数量成正比。DL 旨在克服从有限数量的训练点学习指数数量的区域的挑战，假设特征层次结构生成数据。

## DL 作为表征学习

许多人工智能任务，如图像或语音识别，都需要了解世界。其中一个关键的挑战是对这些知识进行编码，以便计算机能够利用它们。几十年来，ML 系统的开发需要相当多的领域专业知识来将原始数据（如图像像素）转换为内部表示，学习算法可用于检测或分类模式。

类似地，ML 算法为交易策略增加了多少价值在很大程度上取决于我们设计表示数据中预测信息的特征的能力，以便算法能够处理它。理想情况下，这些特征能够捕获结果的独立驱动因素，如*第 4 章*、*金融特征工程——如何研究阿尔法因素*，以及*第 2 部分*和*第 3 部分*中在设计和评估捕获交易信号的因素时所述。

表示学习不依赖于手工设计的特征，而是允许 ML 算法自动发现对检测或分类模式最有用的数据表示。DL 将此技术与有关特性性质的特定假设相结合。更多信息请参见 Bengio、Courville 和 Vincent（2013）。

### DL 如何从数据中提取分层特征

DL 背后的核心思想是一个多层次的特征层次结构生成了数据。因此，DL模型对目标函数由一组嵌套的简单函数组成的先验信念进行编码。这种假设允许在给定数量的训练样本中可以区分的区域数量呈指数增长。

换句话说，DL 是一种表示学习方法，它从数据中提取概念的层次结构。它通过**组合简单但非线性的函数**来学习这种层次表示法，该函数将一个级别的表示法（从输入数据开始）依次转换为更高、更抽象级别的新表示法。通过组合足够多的这些转换，DL 能够学习非常复杂的函数。

例如，应用于**分类任务**时，更高级别的表示往往会放大最有助于辨别对象的数据方面，同时抑制不相关的变化源。正如我们将在*第 18 章*、*金融时间序列和卫星图像的 CNN*中更详细地看到的，原始图像数据只是一个二维或三维像素值数组。表示的第一层通常学习聚焦于特定方向和位置的边的存在或不存在的特征。第二层经常学习依赖于特定边缘排列的图案，而不管它们的位置有多小的变化。下一层可能会组合图案来表示相关对象的部分，后续层会将对象检测为这些部分的组合。

DL 的**关键突破在于，通用学习算法能够以一种比人类工程学更具可扩展性的方式，提取适合于高维、非结构化数据建模的层次特征。因此，DL 的兴起与非结构化图像或文本数据的大规模可用性并行也就不足为奇了。由于这些数据源在替代数据中也占有重要地位，DL 已成为算法交易的高度相关数据。**

### 好消息和坏消息——普遍逼近定理

**通用近似定理**将 NNs 捕捉输入和输出数据之间任意关系的能力形式化。George Cybenko（1989）证明了使用 sigmoid 激活函数的单层 NNs可以表示`Rn.`Kurt Hornik（1991）的封闭和有界子集上的任何连续函数，进一步表明这不是激活函数的特定形状，而是**多层结构**这使分层特征表示成为可能，进而使 NNs 能够近似通用函数。

然而，该定理不能帮助我们确定表示特定目标函数所需的网络体系结构。我们将在本章的最后一节中看到，有许多参数需要优化，包括网络的宽度和深度、神经元之间连接的数量以及激活函数的类型。

此外，表示任意函数的能力并不意味着网络可以实际学习给定函数的参数。反向传播（NNs 最流行的学习算法）花了 20 多年时间才大规模生效。不幸的是，考虑到优化问题的高度非线性性质，无法保证它会找到绝对最佳的解决方案，而不仅仅是一个相对较好的解决方案。

## DL 与 ML 和 AI 的关系

人工智能有着悠久的历史，至少可以追溯到 20 世纪 50 年代，作为一个学术领域，作为人类研究的主题，人工智能的历史要长得多，但从那时起，人工智能经历了几次热情的起伏波动（参见 Nilsson，2009 年的深入调查）。ML 是一个重要的分支领域，在相关的学科中有着悠久的历史，如统计学和在 20 世纪 80 年代变得突出。正如我们刚才所讨论的，如*图 17.1*所示，DL 是表示学习的一种形式，本身也是 ML 的一个子字段。

人工智能的最初目标是实现**通用人工智能**，即解决被认为需要人类水平智能的问题的能力，以及对世界进行推理和得出逻辑结论并自动改进自身的能力。不涉及 ML 的 AI 应用程序包括编码世界信息的知识库，以及用于逻辑操作的语言。

从历史上看，许多人工智能致力于开发基于规则的系统，旨在获取专家知识和决策规则，但由于过于复杂，这些规则的硬编码常常失败。相比之下，ML 意味着一种从数据中学习规则的**概率方法**，旨在规避人类设计的基于规则的系统的局限性。它还涉及到向更狭隘、特定于任务的目标的转变。

下图概述了各个 AI 子领域之间的关系，概述了它们的目标，并强调了它们在时间轴上的相关性。

<figure class="mediaobject">![](../Images/B15439_17_01.png)</figure>

图 17.1:AI 时间线和子字段

在下一节中，我们将看到如何实际构建神经网络。

# 神经网络的设计

DL 依赖于**NNs**，后者由几个关键构建块组成，而这些构建块又可以通过多种方式进行配置。在本节中，我们将介绍 NNs 的工作原理，并说明它们用于设计不同体系结构的最重要组件。

（人工）NNs 最初的灵感来源于类似人脑的学习生物学模型，或者是为了模仿它的工作原理并取得类似的成功，或者是为了通过模拟获得更好的理解。当前的神经网络研究较少涉及神经科学，尤其是因为我们对大脑的理解还没有达到足够的粒度水平。另一个制约因素是总体规模：即使神经网络自 20 世纪 50 年代建立以来每年使用的神经元数量继续翻一番，它们也只能在 2050 年左右达到人脑的规模。

我们还将解释**反向传播**（通常简称为**反向传播**）如何使用梯度信息（代价函数相对于参数的偏导数的值）根据训练误差调整所有神经网络参数。各种非线性模块的组合意味着目标函数的优化可能非常具有挑战性。我们还介绍了反向传播的改进，旨在加快学习过程。

## 一种简单的前馈神经网络结构

在本节中，我们将介绍基于**多层感知器**（**MLP**）的前馈 NNs、**，它们由一个或多个连接输入层和输出层的隐藏层组成。在前馈神经网络中，信息只从输入流向输出，因此它们可以表示为有向无环图，如下图所示。相比之下，**递归神经网络**（**RNNs**；参见*第 19 章*、*用于多变量时间序列和情绪分析的 RNN*包括从输出到输入的循环，以跟踪或记忆过去的模式和事件。**

 **我们将首先描述前馈神经网络体系结构以及如何使用 NumPy 实现它。然后，我们将解释反向传播如何学习 NN 权重，并在 Python 中实现它，以训练一个二元分类网络，即使类不是线性可分的，也能产生完美的结果。具体实施见`build_and_train_feedforward_nn`笔记本。

前馈神经网络由几个**层**组成，每个层接收输入数据样本并产生输出。**转换链**从输入层开始，将源数据传递到几个内部或隐藏层中的一个，然后到输出层结束，输出层计算结果以与样本的输出值进行比较。

隐藏层和输出层由节点或神经元组成。**完全连接**或密集层的节点连接到前一层的部分或所有节点。网络架构可以通过其深度、隐藏层的数量或每层的宽度和节点数量来概括。

每个连接都有一个**权重**，用于计算输入值的线性组合。一个层也可能有一个**偏差**节点，该节点总是输出一个 1，并由下一层中的节点使用，就像线性回归中的常数。训练阶段的目标是学习这些权重的值，以优化网络的预测性能。

隐藏层的每个节点计算权重的**点积**和前一层的输出。一个**激活函数**将结果转换为后续层的输入。这种转换通常是非线性的（如用于逻辑回归的 sigmoid 函数；参见*第 7 章*、*线性模型——从风险因素到回报预测，*在线性模型上），因此网络可以学习非线性关系；我们将在下一节讨论常见的激活函数。输出层计算最后一个隐藏层的输出与其权重的线性组合，并使用与 ML 问题类型匹配的激活函数。

因此，来自输入的网络输出的计算通过嵌套函数链，称为**前向传播**。*图 17.2*展示了一个具有二维输入向量的单层前馈神经网络，一个宽度为 3 的隐藏层，以及输出层中的两个节点。该体系结构足够简单，因此我们仍然可以轻松地将其绘制成图形，同时还可以说明关键概念。

<figure class="mediaobject">![](../Images/B15439_17_02.png)</figure>

图 17.2：具有一个隐藏层的前馈架构

**网络图**显示三个隐藏层节点（不计算偏差）各有三个权重，一个用于输入层偏差，两个用于输入变量。类似地，每个输出层节点有四个权重来计算隐藏层偏差和激活的乘积和或点积。总共有 17 个参数需要学习。

图右侧的**正向传播**面板分别列出了隐藏层和输出层*h*和*o*的示例节点的计算。隐藏层中的第一个节点将 sigmoid 函数应用于其权重和输入的线性组合*z*，类似于逻辑回归。因此，隐藏层并行运行三个逻辑回归，而反向传播算法确保它们的参数最有可能不同，以最好地通知后续层。

输出层使用**softmax**激活函数（参见*第 6 章*、*机器学习过程*），将逻辑 S 型函数推广到多个类。它调整隐藏层输出的点积及其权重，以表示类的概率（在这种情况下，仅两个以简化表示）。

前向传播也可以表示为嵌套函数，其中*h*再次表示隐藏层，*o*表示输出层，以产生输出的 NN 估计：![](../Images/B15439_17_001.png)。

## 主要设计选择

一些神经网络设计选择类似于其他监督学习模型。例如，输出取决于 ML 问题的类型，如回归、分类或排序。给定输出，我们需要选择一个成本函数来衡量预测的成功与失败，以及一个优化网络参数以最小化成本的算法。

NN 特定的选择包括每层的层数和节点数、不同层的节点之间的连接以及激活函数的类型。

一个关键问题是**训练效率**：激活的功能形式可以促进或阻碍反向传播算法可用的梯度信息的流动，反向传播算法根据训练错误调整权重。对于较大的输入值范围，具有平坦区域的函数具有非常低的梯度，并且当参数值卡在这样的范围内时，可能会阻碍训练进度。

一些架构添加了**跳过连接**，在相邻层之外建立直接链接，以促进梯度信息的流动。另一方面，故意省略连接可以减少限制网络容量的参数数量，并可能降低泛化误差，同时降低计算成本。

### 隐藏单元与激活函数

除了 sigmoid 函数外，还成功地使用了一些非线性激活函数。它们的设计仍然是一个研究领域，因为它们是允许神经网络学习非线性关系的关键元素。它们对训练过程也有着至关重要的影响，因为它们的导数决定了误差如何转化为体重调整。

一个非常流行的激活函数是**校正线性单元**（**ReLU**）。对于给定的激活*z*，激活计算为*g*（*z*）=max（0，*z*），导致类似于看涨期权的支付的功能形式。当单位激活时，导数为常数。RELU 通常与需要存在偏差节点的仿射输入变换相结合。与 sigmoid 单元相比，它们的发现极大地提高了前馈网络的性能，并且通常被推荐为默认值。有几个 ReLU 扩展，旨在解决 ReLU 在不活动且梯度为零时通过梯度下降学习的局限性（Goodfello、Bengio 和 Courville，2016）。

逻辑函数σ的另一种替代方法是**双曲正切函数 tanh**，它产生[-1,1]范围内的输出值。因为![](../Images/B15439_17_002.png)，它们是密切相关的。这两个函数都会受到饱和的影响，因为对于非常低和非常高的输入值，它们的梯度变得非常小。然而，tanh 通常表现得更好，因为它更像身份函数，因此对于较小的激活值，网络的行为更像一个线性模型，从而有助于训练。

### 产出单位和成本函数

神经网络输出格式和代价函数的选择取决于监督学习问题的类型：

*   **回归问题**使用线性输出单元，计算其权重与最终隐藏层激活的点积，通常与均方误差成本结合
*   **二元分类**使用 sigmoid输出单元来模拟伯努利分布，就像以隐藏激活作为输入的逻辑回归一样
*   **多类问题**依赖于 softmax 单元，该单元概括了逻辑 S 形，并对两类以上的离散分布建模，如前所述

二元和多类问题通常使用交叉熵损失，与均方误差相比，这显著提高了训练效率（有关损失函数的更多信息，请参见*第 6 章*、*机器学习过程*）。

## 如何规范深度 NNs

NNs 逼近任意函数的能力的缺点是大大增加了过度拟合的风险。防止过度拟合的最佳**保护**是在更大的数据集上训练模型。数据扩充，例如创建稍加修改的图像版本，是一种强大的替代方法。为此目的生成综合金融培训数据是一个活跃的研究领域，我们将在*第 20 章*、*条件风险因素和资产定价自动编码器*（例如，见 Fu et al.2019）中阐述。

作为获取更多数据的替代或补充，正则化可以帮助降低过度拟合的风险。对于本书迄今为止讨论的所有模型，都有某种形式的正则化，可以修改学习算法以减少其泛化误差，而不会对其训练误差产生负面影响。示例包括添加到脊线和套索回归目标的惩罚，以及用于决策树和基于树的集成模型的分割或深度约束。

通常，正则化采用对参数值的软约束的形式，用较低的方差交换一些额外的偏差。一个常见的实际发现是，泛化误差最小的模型不是参数大小准确的模型，而是经过良好正则化的更大模型。可组合使用的流行 NN 正则化技术包括参数范数惩罚、提前停止和退出。

### 参数范数惩罚

在*第 7 章**线性模型中，套索回归和岭回归的**参数范数惩罚**分别为**L1 和 L2 正则化**。从风险因素到回报预测*。在 NN上下文中，参数范数惩罚通过添加一个表示参数的 L1 或 L2 范数的项来修改目标函数，该项由需要调整的超参数加权。对于 NN，偏差参数通常不受约束，只受权重约束。

L1 正则化可以通过将权重一直减小到零来产生稀疏参数估计。相比之下，L2 正则化保留了参数显著降低成本函数的方向。惩罚或超参数值可能因层而异，但增加的调优复杂性很快变得令人望而却步。

### 早停

我们在*第 12 章**中遇到了**提前停止**作为一种正则化技术，提升了您的交易策略*。它可能是最常见的 NN 正则化方法，因为它既有效又简单易用：它在验证集上监控模型的性能，并在性能停止改善一定数量的观测值时停止训练，以防止过度拟合。

提前停止可被视为**高效的超参数选择**，可自动确定正确的正则化量，而参数惩罚要求超参数调整以确定理想的权重衰减。只是要小心避免**前瞻性偏差**：当提前停止使用样本数据时，回溯测试结果会非常积极，而在实际实施策略时，这些数据是不可用的。

### 辍学者

**辍学**是指在正向传播或反向传播过程中，以给定概率随机遗漏单个单元。因此，这些省略的单元不会导致训练错误或接收更新。

该技术计算成本低，不限制模型或训练过程的选择。虽然需要更多的迭代来实现相同的学习量，但由于计算成本较低，每次迭代都会更快。辍学通过防止部队补偿其他部队在训练过程中犯下的错误，降低了过度装配的风险。

## 更快的培训–优化深度学习

Backprop 是指计算成本函数相对于我们希望更新的内部参数的梯度，并使用此信息更新参数值。梯度很有用，因为它指示了导致成本函数最大增加的参数变化方向。因此，根据负梯度调整参数产生最佳成本降低，至少对于非常接近观测样本的区域。有关关键梯度下降优化算法的优秀概述，请参见 Ruder（2017）。

由于非凸目标函数和潜在的大量参数，训练深度神经网络可能非常耗时。一些挑战可能会显著延迟收敛，找到较差的最优值，或导致与目标的振荡或偏离：

*   **局部极小值**会阻止收敛到全局最优值，导致性能不佳
*   **非局部最小值的低梯度平坦区域**也会阻止收敛，但很可能远离全局最优值
*   **多个较大权重相乘产生的高梯度**陡峭区域可能会导致过度调整
*   RNN 中的深层架构或长期依赖性需要在反向传播期间乘以许多权重，从而导致**消失梯度**，因此至少部分 NN 接收到很少或没有更新

已经开发了几种算法来解决其中一些挑战，即随机梯度下降的变化和使用自适应学习率的方法。虽然自适应学习率已显示出一些前景，但没有单一的最佳算法。

### 随机梯度下降算法

梯度下降使用梯度信息迭代调整这些参数。对于给定参数![](../Images/B15439_17_003.png)，基本梯度下降规则通过损失函数相对于该参数的负梯度乘以学习率![](../Images/B15439_17_004.png)来调整该值：

<figure class="mediaobject">![](../Images/B15439_17_005.png)</figure>

梯度可以针对所有训练数据、随机批次数据或个人观察（称为在线学习）进行评估。随机样本产生**随机梯度下降**（**SGD**），如果随机样本是整个训练过程中梯度方向的无偏估计，则通常会导致更快的收敛。

然而，有许多挑战：很难定义一个学习率或一个促进有效收敛的速率计划，因为过低的速率会延长过程，过高的速率会导致重复的超调和振荡，甚至偏离最小值。此外，相同的学习速率可能不适合所有参数，即在所有变化方向。

### 推进力

一种流行的基本梯度下降的改进为**增加了动力，加速了收敛到局部最小值**。动量的图解通常使用拉长峡谷中心的局部最优的例子（而在实践中，维度将远远高于三）。这意味着在深而窄的峡谷内有一个最小值，峡谷壁非常陡峭，一侧坡度很大，另一侧向该区域底部的局部最小值有一个更为平缓的坡度。坡度下降自然会跟随陡峭的坡度，并会在峡谷壁上反复调整，朝着最小值移动的速度要慢得多。

动量旨在通过**跟踪最近方向**并通过最近梯度和当前计算值的加权平均值调整参数来解决这种情况。它使用动量项![](../Images/B15439_17_037.png)来衡量最新调整对本次迭代更新*v*<sub class="Subscript--PACKT-">t</sub>的贡献：

<figure class="mediaobject">![](../Images/B15439_17_006.png)</figure>

**内斯特罗夫动量**是对正常动量的简单变化。这里，梯度项不是在当前参数空间位置![](../Images/Image74692.png)处计算的，而是从中间位置计算的。目标是纠正动量项过冲或指向错误方向（Sutskever et al.2013）。

### 自适应学习速率

正如前面关于随机梯度下降的小节所强调的，选择合适的学习速率是非常具有挑战性的。同时，它也是对训练时间和泛化性能有重大影响的最重要的参数之一。

虽然动量解决了学习率的一些问题，但它这样做的代价是引入另一个超参数，**动量率**。有几种算法旨在根据梯度信息在整个训练过程中调整学习速率。

#### 阿达格拉德

AdaGrad 累积所有历史的、特定于参数的梯度信息，并继续将学习速率与给定参数的累积梯度平方成反比。我们的目标是减缓已经改变了很多的参数的变化，并鼓励那些没有改变的参数进行调整。

AdaGrad 被设计为在凸函数上表现良好，在 DL 环境中表现好坏参半，因为它可以基于早期梯度信息过快地降低学习速度。

#### RMSProp

RMSProp将 AdaGrad 修改为使用累积梯度信息的指数加权平均值。目标是更加强调最近的渐变。它还引入了一个新的超参数来控制移动平均线的长度。

RMSProp 是一种流行的算法，其通常表现良好，由各种库提供，我们稍后将介绍这些库并在实践中常规使用。

#### 亚当

Adam 代表**自适应力矩推导**并将 RMSProp 的各个方面与动量相结合。被认为相当稳健，经常被用作默认优化算法（Kingma 和 Ba，2014）。

Adam 有几个超参数，这些超参数具有推荐的默认值，可能会从某些调整中受益：

*   **alpha**：学习速率或步长决定更新权重的多少，以便较大（较小）的值在更新速率之前加速（减慢）学习；许多库使用 0.001 默认值
*   **β**<sub class="Subscript--PACKT-">1</sub>：一阶矩估计的指数衰减率；通常设置为 0.9
*   **β**<sub class="Subscript--PACKT-">2</sub>。二阶矩估计的指数衰减率；通常设置为 0.999
*   **ε**：一个非常小的数字，以防止被零除；通常设置为 1e-8

## 摘要–如何调整关键超参数

超参数优化旨在**调整模型**的容量，使其匹配数据输入之间关系的复杂性。产能过剩可能导致过度拟合，需要更多的数据将额外信息引入学习过程，减少模型的大小，或者更积极地使用刚才描述的各种正则化工具。

**主要诊断工具**是*第 6 章*、*机器学习过程*中描述的训练和验证错误行为：如果在训练错误持续下降的同时验证错误恶化，则模型因容量过大而过度拟合。另一方面，如果性能低于预期，则可能需要增加模型的大小。

参数优化最重要的方面是体系结构本身，因为它在很大程度上决定了参数的数量：在其他条件相同的情况下，更多或更宽的隐藏层会增加容量。如前所述，最佳性能通常与容量过剩但使用辍学或 L1/L2 惩罚等机制很好地规范化的模型相关联。

除了**平衡模型大小和正则化**之外，重要的是调整**学习率**，因为这会破坏优化过程并降低有效模型容量。自适应优化算法为 Adam（最流行的选择）提供了一个良好的起点。

# Python 中从头开始的神经网络

为了更好地理解NNs 的工作原理，我们将使用矩阵代数制定*图 17.2*中显示的单层架构和前向传播计算，并使用 NumPy 实现。您可以在笔记本`build_and_train_feedforward_nn`中找到代码示例。

## 输入层

*图 17.2*所示的架构是针对二维输入数据*X*而设计的，它代表两个不同的类*Y*。在矩阵形式中，*X*和*Y*均为![](../Images/B15439_17_009.png)形状：

<figure class="mediaobject">![](../Images/B15439_17_010.png)</figure>

我们将使用 scikit learn 的`make_circles`函数以两个不同半径的同心圆的形式生成 50000 个随机二进制样本，以便这些类不是线性可分的：

```
N = 50000
factor = 0.1
noise = 0.1
X, y = make_circles(n_samples=N, shuffle=True,
                   factor=factor, noise=noise) 
```

然后，我们将一维输出转换为二维数组：

```
Y = np.zeros((N, 2))
for c in [0, 1]:
   Y[y == c, c] = 1
'Shape of: X: (50000, 2) | Y: (50000, 2) | y: (50000,)' 
```

*图 17.3*显示了数据的散点图，该散点图显然不是线性可分的：

<figure class="mediaobject">![](../Images/B15439_17_03.png)</figure>

图 17.3：二元分类的合成数据

## 隐藏层

隐藏层*h*使用权重 W<sup style="font-style: italic;">h</sup>将二维输入投影到三维空间，并且通过偏置向量 b<sup style="font-style: italic;">h</sup>转换结果。为了执行该仿射变换，隐藏层权重由![](../Images/B15439_17_011.png)矩阵 W<sup style="font-style: italic;">h</sup>表示，而隐藏层偏差向量由三维向量表示：

<figure class="mediaobject">![](../Images/B15439_17_012.png)</figure>

隐藏层激活*H*是在添加偏置向量后，对输入数据和权重的点积应用 sigmoid 函数的结果：

<figure class="mediaobject">![](../Images/B15439_17_013.png)</figure>

为了使用 NumPy 实现隐藏的层，我们首先定义`logistic`sigmoid 函数：

```
def logistic(z):
   """Logistic function."""
   return 1 / (1 + np.exp(-z)) 
```

然后，我们定义一个函数，该函数根据相关输入、权重和偏差值计算隐藏层激活：

```
def hidden_layer(input_data, weights, bias):
   """Compute hidden activations"""
   return logistic(input_data @ weights + bias) 
```

## 输出层

输出层使用![](../Images/B15439_17_014.png)权重矩阵 W<sup style="font-style: italic;">o</sup>和二维偏置向量**b**<sup style="font-style: italic;">o</sup>将三维隐藏层激活*H*压缩回二维：

<figure class="mediaobject">![](../Images/B15439_17_015.png)</figure>

隐层输出的线性组合产生![](../Images/B15439_17_016.png)矩阵**Z**<sup style="font-style: italic;">o</sup>：

<figure class="mediaobject">![](../Images/B15439_17_017.png)</figure>

输出层激活由标准化**Z**<sup style="font-style: italic;">o</sup>的 softmax 函数![](../Images/B15439_17_018.png)计算，以符合离散概率分布的约定：

<figure class="mediaobject">![](../Images/B15439_17_019.png)</figure>

我们在 Python 中创建一个 softmax 函数，如下所示：

```
def softmax(z):
   """Softmax function"""
   return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True) 
```

如本文所定义，输出层激活取决于隐藏层激活和输出层权重和偏差：

```
def output_layer(hidden_activations, weights, bias):
   """Compute the output y_hat"""
   return softmax(hidden_activations @ weights + bias) 
```

现在我们有了所有我们需要的组件来集成层并直接从输入计算 NN 输出。

## 正向传播

`forward_prop`功能结合之前的操作根据输入数据产生输出激活，作为权重和偏差的函数：

```
def forward_prop(data, hidden_weights, hidden_bias, output_weights, output_bias):
   """Neural network as function."""
   hidden_activations = hidden_layer(data, hidden_weights, hidden_bias)
   return output_layer(hidden_activations, output_weights, output_bias) 
```

`predict`函数根据给定的权重、偏差和输入数据生成二进制类预测：

```
def predict(data, hidden_weights, hidden_bias, output_weights, output_bias):
   """Predicts class 0 or 1"""
   y_pred_proba = forward_prop(data,
                               hidden_weights,
                               hidden_bias,
                               output_weights,
                               output_bias)
   return np.around(y_pred_proba) 
```

## 交叉熵代价函数

最后一部分是基于给定标签评估 NN 输出的成本函数。成本函数*J*使用交叉熵损失![](../Images/B15439_17_020.png)，其将每个类别*c*的预测与实际结果的偏差相加：

<figure class="mediaobject">![](../Images/B15439_17_021.png)</figure>

它在 Python 中采用以下形式：

```
def loss(y_hat, y_true):
   """Cross-entropy"""
   return - (y_true * np.log(y_hat)).sum() 
```

## 如何使用 Python 实现 backprop

为了使用 backprop 更新 NN权重和偏差值，我们需要计算成本函数的梯度。梯度表示成本函数相对于目标参数的偏导数。

### 如何计算梯度

NN 由一组嵌套函数组成，如前所述。因此，使用微积分链规则计算损失函数相对于内部隐藏参数的梯度。

对于标量值，给定函数*z*＝*h*（*x*）和*y*＝*o*（*h*（*x*）＝*o*（*z*），我们计算*y*相对于*x 的导数*使用链式法则如下：

<figure class="mediaobject">![](../Images/B15439_17_022.png)</figure>

对于向量，使用![](../Images/B15439_17_023.png)和![](../Images/B15439_17_024.png)使隐藏层*h*从 R<sup xmlns:epub="http://www.idpf.org/2007/ops" style="font-style: italic;">n</sup>映射到 R<sup xmlns:epub="http://www.idpf.org/2007/ops" style="font-style: italic;">m</sup>和*z*=*h*（*x*和*y*=*o*（【T18 z），我们得到：

<figure class="mediaobject">![](../Images/B15439_17_025.png)</figure>

我们可以使用*h*的![](../Images/B15439_17_026.png)雅可比矩阵，用矩阵表示法更简洁地表达这一点：

<figure class="mediaobject">![](../Images/B15439_17_027.png)</figure>

包含*z*的*m*分量相对于*n*输入*x*的偏导数。*y*相对于*x*的梯度![](../Images/B15439_17_028.png)包含所有偏导数，因此可以写成：

<figure class="mediaobject">![](../Images/B15439_17_029.png)</figure>

### 损失函数梯度

交叉熵损失函数*J*对每个输出层激活*i*=1，*N*的导数是一个非常简单的表达式（详见笔记本），标量值如下所示，矩阵表示法如下图所示：

<figure class="mediaobject">![](../Images/B15439_17_030.png)</figure>

我们据此定义了`loss_gradient`函数：

```
def loss_gradient(y_hat, y_true):
   """output layer gradient"""
   return y_hat - y_true 
```

### 输出层梯度

为了将更新传播回输出层权重，我们使用损失函数*J*相对于权重矩阵的梯度：

<figure class="mediaobject">![](../Images/B15439_17_031.png)</figure>

至于偏见：

<figure class="mediaobject">![](../Images/B15439_17_032.png)</figure>

我们现在可以相应地定义`output_weight_gradient`和`output_bias_gradient`，都以损耗梯度![](../Images/B15439_17_033.png)为输入：

```
def output_weight_gradient(H, loss_grad):
   """Gradients for the output layer weights"""
   return  H.T @ loss_grad
def output_bias_gradient(loss_grad):
   """Gradients for the output layer bias"""
   return np.sum(loss_grad, axis=0, keepdims=True) 
```

### 隐藏层渐变

损失函数相对于隐藏层值的梯度计算如下，其中![](../Images/B15439_17_034.png)表示元素矩阵乘积：

<figure class="mediaobject">![](../Images/B15439_17_035.png)</figure>

我们定义了一个`hidden_layer_gradient`函数来编码这个结果：

```
def hidden_layer_gradient(H, out_weights, loss_grad):
   """Error at the hidden layer.
   H * (1-H) * (E . Wo^T)"""
   return H * (1 - H) * (loss_grad @ out_weights.T) 
```

隐藏层权重和偏移的渐变为：

<figure class="mediaobject">![](../Images/B15439_17_036.png)</figure>

相应的功能包括：

```
def hidden_weight_gradient(X, hidden_layer_grad):
   """Gradient for the weight parameters at the hidden layer"""
   return X.T @ hidden_layer_grad

def hidden_bias_gradient(hidden_layer_grad):
   """Gradient for the bias parameters at the output layer"""
   return np.sum(hidden_layer_grad, axis=0, keepdims=True) 
```

### 把它们放在一起

为了准备我们网络的训练，我们创建了一个函数，该函数结合先前的梯度定义，并根据训练数据和标签计算相关的权重和偏差更新，以及当前的权重和偏差值：

```
def compute_gradients(X, y_true, w_h, b_h, w_o, b_o):
   """Evaluate gradients for parameter updates"""
   # Compute hidden and output layer activations
   hidden_activations = hidden_layer(X, w_h, b_h)
   y_hat = output_layer(hidden_activations, w_o, b_o)
   # Compute the output layer gradients
   loss_grad = loss_gradient(y_hat, y_true)
   out_weight_grad = output_weight_gradient(hidden_activations, loss_grad)
   out_bias_grad = output_bias_gradient(loss_grad)
   # Compute the hidden layer gradients
   hidden_layer_grad = hidden_layer_gradient(hidden_activations,
                                             w_o, loss_grad)
   hidden_weight_grad = hidden_weight_gradient(X, hidden_layer_grad)
   hidden_bias_grad = hidden_bias_gradient(hidden_layer_grad)
   return [hidden_weight_grad, hidden_bias_grad, out_weight_grad, out_bias_grad] 
```

#### 测试梯度

笔记本包含一个测试函数，该函数将先前使用多元演算分析得出的梯度与我们通过稍微扰动单个参数得到的数值估计进行比较。测试函数验证输出值的变化与分析梯度估计的变化相似。

#### 使用 Python 实现动量更新

要将动量纳入参数更新，请定义一个`update_momentum`函数，该函数将我们刚才使用的`compute_gradients`函数的结果与每个参数矩阵的最新动量更新相结合：

```
def update_momentum(X, y_true, param_list, Ms, momentum_term, learning_rate):
   """Compute updates with momentum."""
   gradients = compute_gradients(X, y_true, *param_list)
   return [momentum_term * momentum - learning_rate * grads
           for momentum, grads in zip(Ms, gradients)] 
```

`update_params`功能执行实际更新：

```
def update_params(param_list, Ms):
   """Update the parameters."""
   return [P + M for P, M in zip(param_list, Ms)] 
```

### 培训网络

为了训练网络，我们首先使用标准正态分布（见笔记本）随机初始化所有网络参数。对于给定数量的迭代或纪元，我们运行动量更新并计算训练损失，如下所示：

```
def train_network(iterations=1000, lr=.01, mf=.1):
   # Initialize weights and biases
   param_list = list(initialize_weights())
   # Momentum Matrices = [MWh, Mbh, MWo, Mbo]
   Ms = [np.zeros_like(M) for M in param_list]
   train_loss = [loss(forward_prop(X, *param_list), Y)]
   for i in range(iterations):
       # Update the moments and the parameters
       Ms = update_momentum(X, Y, param_list, Ms, mf, lr)
       param_list = update_params(param_list, Ms)
       train_loss.append(loss(forward_prop(X, *param_list), Y))
   return param_list, train_loss 
```

*图 17.4*绘制了 50000 个训练样本在 50000 次迭代中的训练损失，动量项为 0.5，学习率为 1e-4。它表明损失开始下降需要超过 5000 次迭代，但随后下降速度非常快。我们没有使用新加坡元，这可能会大大加快收敛速度。

<figure class="mediaobject">![](../Images/B15439_17_04.png)</figure>

图 17.4：每次迭代的培训损失

*图 17.5*中的曲线图显示了具有三维隐藏层的神经网络从具有两类不可线性分离的二维数据学习的函数。左侧面板显示源数据和决策边界，该边界错误分类了很少的数据点，并将通过继续培训进一步改进。

中心面板显示隐藏层学习的输入数据的表示。该网络学习权重，以便将输入从二维投影到三维，从而实现两类的线性分离。右图显示了输出层如何以输出维度中 0.5 的截止值的形式实现线性分离：

<figure class="mediaobject">![](../Images/B15439_17_05.png)</figure>

图 17.5：可视化神经网络学习的功能

**总结**：我们已经看到了一个非常简单的网络，它有一个隐藏层，有三个节点，共有 17 个参数，能够学习如何使用 backprop 和带动量的梯度下降来解决非线性分类问题。

接下来，我们将回顾如何使用流行的 DL 库，以促进复杂体系结构的设计和快速培训，同时使用复杂的技术来防止过度拟合并评估结果。

# 流行的深度学习图书馆

目前，最流行的 DL 库是 TensorFlow（由 Google 支持）、Keras（由 Francois Chollet 领导，现在在 Google 工作）和 PyTorch（由 Facebook 支持）。截至 2020 年 3 月，PyTorch 版本 1.4 和 TensorFlow 版本 2.2 的开发非常活跃。TensorFlow 2.0 采用 Keras 作为其主界面，有效地将两个库合并为一个库。

所有库都提供了我们在本章前面讨论的设计选择、正则化方法和 backprop 优化。它们也有助于在一个或多个**图形处理单元**（**GPU**上进行快速培训。库的侧重点与 TensorFlow 略有不同，TensorFlow 最初设计用于生产部署，并在行业中流行，而 PyTorch 在学术研究人员中颇受欢迎；然而，这些界面正在逐渐融合。

我们将使用与上一节相同的网络体系结构和数据集来说明 TensorFlow 和 PyTorch 的使用。

## 利用 GPU 加速

DL 是计算密集型的，好的结果通常需要大量的数据集。因此，模型培训和评估可能会变得相当耗时。GPU 针对深度学习模型所需的矩阵运算进行了高度优化，并且往往具有更高的处理能力，因此 10 倍或更高的加速率并不少见。

所有流行的深度学习库都支持使用 GPU，有些库还允许在多个 GPU 上进行并行训练。最常见的 GPU 类型由 NVIDIA 生产，配置需要安装和设置 CUDA 环境。该过程将继续发展，并且可能会有一定的挑战性，具体取决于您的计算环境。

利用 GPU 的更直接的方法是通过 Docker 虚拟化平台。您可以在 Docker 管理的本地容器中运行大量图像，从而避免您可能遇到的许多驱动程序和版本冲突。TensorFlow 在其网站上提供 Docker 图像，该图像也可用于 Keras。

有关 DL 笔记本和安装目录中的参考和相关说明，请参见 GitHub。

## 如何使用 TensorFlow 2

TensorFlow 在 2015 年 9 月发布深度领先的深度学习 lib 库，在 Py 火炬之前一年。TensorFlow 2 通过将 Keras API 作为其主要接口，简化了随着时间的推移日益复杂的 API。

Keras 被设计为一种高级 API，用于加速设计和训练具有计算后端（如 TensorFlow、Theano 或 CNTK）的深层神经网络的迭代工作流程。它已于 2017 年集成到 TensorFlow 中。您还可以组合来自这两个库的代码，以利用 Keras 的高级抽象以及定制的 TensorFlow 图操作。

此外，TensorFlow 采用了**急切执行**。以前，您需要定义一个完整的计算图来编译成优化的操作。运行编译后的图需要配置会话并提供必要的数据。在渴望执行下，您可以像普通 Python 代码一样逐行运行 TensorFlow 操作。

Keras 支持稍微简单的顺序 API 和更灵活的函数 API。在这一点上，我们将介绍前者，并在接下来的章节中在更复杂的示例中使用函数 API。

为了创建一个模型，我们只需要实例化一个`Sequential`对象，并提供一个包含标准层序列及其配置的列表，包括单元数量、激活函数类型或名称。

第一个隐藏层需要关于它通过`input_shape`参数从输入层接收的矩阵中的特征数量的信息。在我们的简单例子中，只有两个。Keras 通过`batch_size`参数推断出训练期间需要处理的行数，我们将在本节后面的`fit`方法中传递该参数。TensorFlow 根据前一层的`units`参数推断其他层接收的输入的大小：

```
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
model = Sequential([
    Dense(units=3, input_shape=(2,), name='hidden'),
    Activation('sigmoid', name='logistic'),
    Dense(2, name='output'),
    Activation('softmax', name='softmax'),
]) 
```

Keras API 提供了许多标准构建块，包括递归层和卷积层、各种正则化选项、一系列损失函数和优化器，以及预处理、可视化和日志记录（参考 GitHub 上的 TensorFlow 文档链接）。它也是可扩展的。

模型的`summary`方法产生了网络架构的简明描述，包括层类型和形状列表以及参数数量：

```
model.summary()
Layer (type)                 Output Shape              Param #   
=================================================================
hidden (Dense)               (None, 3)                 9         
_________________________________________________________________
logistic (Activation)        (None, 3)                 0         
_________________________________________________________________
output (Dense)               (None, 2)                 8         
_________________________________________________________________
softmax (Activation)         (None, 2)                 0         
=================================================================
Total params: 17
Trainable params: 17
Non-trainable params: 0 
```

接下来，我们编译序列模型来配置学习过程。为此，我们定义了优化器、损失函数以及培训期间要监控的一个或多个性能指标：

```
model.compile(optimizer='rmsprop',
             loss='binary_crossentropy',
             metrics=['accuracy']) 
```

Keras 在培训期间使用回调来启用某些功能，例如在 TensorBoard 中记录交互式显示的信息（请参阅下一节）：

```
tb_callback = TensorBoard(log_dir='./tensorboard',
                         histogram_freq=1,
                         write_graph=True,
                         write_images=True) 
```

为了训练模型，我们调用其`fit`方法，除了训练数据外，还传递了几个参数：

```
model.fit(X, Y,
         epochs=25,
         validation_split=.2,
         batch_size=128,
         verbose=1,
         callbacks=[tb_callback]) 
```

有关决策边界的可视化信息，请参见笔记本，该结果类似于我们早期手动网络实现的结果。不过，使用 TensorFlow 的训练速度要快几个数量级。

## 如何使用张力板

TensorBoard 是 TensorFlow 附带的一套很棒的可视化工具。它包括可视化工具，以简化 NNs 的理解、调试和优化。

您可以使用它可视化计算图形，绘制各种执行和性能指标，甚至可视化网络处理的图像数据。它还允许对不同的训练运行进行比较。

运行`how_to_use_tensorflow`笔记本时，安装 TensorFlow 后，可以从命令行启动 TensorBoard：

```
tensorboard --logdir=/full_path_to_your_logs ## e.g. ./tensorboard 
```

或者，您可以在笔记本中使用它，首先加载扩展名，然后通过引用`log`目录启动 TensorBoard：

```
%load_ext tensorboard
%tensorboard --logdir tensorboard/ 
```

首先，可视化包括训练和验证指标（参见*图 17.6*的左面板）。

此外，您可以查看不同时期的权重和偏差直方图（图 17.6 的右面板；时期从后向前发展）。这很有用，因为它允许您监控反向传播是否能够随着学习的进展成功地调整权重，以及它们是否正在收敛。

权重值应在几个时期内改变其初始值，并最终稳定：

<figure class="mediaobject">![](../Images/B15439_17_06.png)</figure>

图 17.6：TensorBoard 学习过程可视化

TensorBoard 还允许您显示和交互浏览网络的计算图，通过单击各个节点，从高层结构向下钻取到底层操作。我们的简单示例体系结构的可视化（参见笔记本）已经包含了许多组件，但在调试时非常有用。有关进一步的参考，请参阅 GitHub 上的链接以获得更详细的教程。

## 如何使用 PyTorch 1.4

Pytork 是由 Yann LeCunn 领导的**Facebook 人工智能研究**（**展会**）团队开发的，第一个 alpha 版本于 2016 年 9 月发布。它提供了与 Python库（如 NumPy）的深度集成，可用于扩展其功能、强大的 GPU 加速以及使用其 autograd 系统进行自动区分。它通过较低级别的 API 提供了比 Keras 更细粒度的控制，主要用作深度学习研究平台，但也可以在启用 GPU 计算的同时取代 NumPy。

与诸如 Theano 或 TensorFlow 所使用的静态计算图相比，它采用了急切执行。它不是最初定义和编译一个用于快速但静态执行的网络，而是依赖其 autograd 包来自动区分张量运算；也就是说，它“动态”计算梯度，这样网络结构可以更容易地进行部分修改。这被称为**按运行定义**，这意味着反向传播是由代码的运行方式定义的，这反过来意味着每一次迭代都可能不同。PyTorch 文档提供了关于此的详细教程。

由此带来的灵活性，再加上直观的 Python first 界面和执行速度，使其迅速普及，并导致开发了许多扩展其功能的支持库。

让我们看看 PyTorch 和 autograd 是如何通过实现我们的简单网络架构来工作的（详细信息请参见`how_to_use_pytorch`笔记本）。

### 如何创建 PyTorch 数据加载器

我们首先将 NumPy 或 pandas 输入数据转换为`torch`张量。从和到 NumPy 的转换非常简单：

```
import torch
X_tensor = torch.from_numpy(X)
y_tensor = torch.from_numpy(y)
X_tensor.shape, y_tensor.shape
(torch.Size([50000, 2]), torch.Size([50000])) 
```

我们可以使用这些 PyTorch 张量来实例化第一个 a`TensorDataset`，在第二步中，a`DataLoader`包含关于`batch_size`的信息：

```
import torch.utils.data as utils
dataset = utils.TensorDataset(X_tensor,y_tensor)
dataloader = utils.DataLoader(dataset,
                              batch_size=batch_size,
                              shuffle=True) 
```

### 如何定义神经网络结构

PyTorch 使用`Net()`类定义 NN 架构。中心元素是`forward`函数。autograd自动定义相应的`backward`函数来计算梯度。

任何合法的张量运算都是`forward`函数的公平博弈，提供了设计灵活性的记录。在我们的简单例子中，我们只是在初始化了张量的属性后，通过函数输入输出关系来链接张量：

```
import torch.nn as nn
class Net(nn.Module):
    def __init__(self, input_size, hidden_size, num_classes):
        super(Net, self).__init__()  # Inherited from nn.Module
        self.fc1 = nn.Linear(input_size, hidden_size)  
        self.logistic = nn.LogSigmoid()                          
        self.fc2 = nn.Linear(hidden_size, num_classes)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        """Forward pass: stacking each layer together"""
        out = self.fc1(x)
        out = self.logistic(out)
        out = self.fc2(out)
        out = self.softmax(out)
        return out 
```

然后我们实例化一个`Net()`对象，并可以按如下方式检查架构：

```
net = Net(input_size, hidden_size, num_classes)
net
Net(
  (fc1): Linear(in_features=2, out_features=3, bias=True)
  (logistic): LogSigmoid()
  (fc2): Linear(in_features=3, out_features=2, bias=True)
  (softmax): Softmax()
) 
```

为了说明急切执行，我们还可以检查第一个张量中的初始化参数：

```
list(net.parameters())[0]
Parameter containing:
tensor([[ 0.3008, -0.2117],
        [-0.5846, -0.1690],
        [-0.6639,  0.1887]], requires_grad=True) 
```

要启用 GPU 处理，您可以使用`net.cuda()`。请参阅 Pytork 文档，了解如何在 CPU 和/或一个或多个 GPU 单元上放置张量。

我们还需要使用一些内置选项定义损失函数和优化器：

```
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate) 
```

### 如何训练模型

模型训练包括每个历元的外循环，即每次传递训练数据，以及`DataLoader`生产批次的内循环。执行学习算法的正向和反向过程。需要注意调整数据类型以适应各种对象和功能的要求；例如，标签需要是整数，特征类型应为`float`：

```
for epoch in range(num_epochs):
    print(epoch)
    for i, (features, label) in enumerate(dataloader):

        features = Variable(features.float())         
        label = Variable(label.long())
        # Initialize the hidden weights
        optimizer.zero_grad()  

        # Forward pass: compute output given features
        outputs = net(features)

        # Compute the loss
        loss = criterion(outputs, label)
        # Backward pass: compute the gradients
        loss.backward()
        # Update the weights
        optimizer.step() 
```

该笔记本还包含一个示例，该示例使用`livelossplot`包在整个培训过程中描绘 Keras 开箱即用提供的损失。

### 如何评估模型预测

为了从我们训练的模型中获得预测，我们将其传递特征数据，并将预测转换为 NumPy 数组。我们得到两类中每一类的 softmax 概率：

```
test_value = Variable(torch.from_numpy(X)).float()
prediction = net(test_value).data.numpy()
Prediction.shape
(50000, 2) 
```

从这里开始，我们可以像以前一样继续计算损失指标或可视化结果，再次重现我们先前发现的决策边界的版本。

## 备选方案

对 DL 的巨大兴趣导致了几个相互竞争的图书馆的开发，这些图书馆促进了 NNs 的设计和培训。最突出的例子包括以下示例（另请参见 GitHub 上的参考资料）。

### ApacheMXnet

MXNet，在 Apache 基金会的 Tyt0At 中孵化，是一个开源的 DL 软件框架，用于训练和部署深度 NNS。它侧重于可伸缩性和快速模型训练。它们包括胶子高级接口，以便于原型制作、训练和部署 DL 模型。亚马逊选择 MXNet 进行 AWS 深度学习。

### Microsoft 认知工具包（CNTK）

微软深度学习 lib 库的贡献是认知工具工具包，以前称为 CNTK。它通过有向图将 NN 描述为一系列计算步骤，类似于 TensorFlow。在这个有向图中，叶节点表示输入值或网络参数，而其他节点表示对其输入的矩阵运算。CNTK 允许用户构建和组合流行的模型架构，包括深度前馈 NNs、卷积网络和递归网络（RNN/LSTM）。

### 法斯泰

fastai 图书馆旨在利用现代最佳实践简化快速准确的 NN 培训。这些实践源于该公司对 DL 的研究，该公司免费提供软件和相关课程。Fastai 支持处理图像、文本、表格和协同过滤数据的模型。

# 为长-短策略优化神经网络

在实践中，我们需要探索 NN 架构的设计选项的变化，以及我们如何从前面概述的内容中训练它，因为我们从一开始就无法确定哪个配置最适合数据。在本节中，我们将探讨使用*第 12 章*中开发的数据集预测每日股票收益率的简单前馈神经网络的各种架构（参见该章 GitHub 目录中的笔记本`preparing_the_model_data`。

为此，我们将定义一个函数，该函数返回基于多个架构输入参数的 TensorFlow 模型，并使用我们在*第 7 章*中介绍的`MultipleTimeSeriesCV`、*线性模型【从风险因素到回报预测】*交叉验证备选设计。为了评估模型预测的信号质量，我们基于样本内交叉验证期间表现最佳的模型集合，构建了一个简单的基于排名的长短策略。为了限制错误发现的风险，我们随后在样本外测试期间评估该策略的性能。

详见`optimizing_a_NN_architecture_for_trading`笔记本。

## 预测每日股票收益的工程特性

为了制定我们的交易策略，我们使用了 995 只美国股票从 2010 年到 2017 年的八年期间的每日股票收益率。我们将使用*第 12 章**中开发的功能，促进您的交易策略*，其中包括波动性和动量因素，以及具有跨部门和部门排名的滞后回报。我们按如下方式加载数据：

```
data = pd.read_hdf('../12_gradient_boosting_machines/data/data.h5', 
                   'model_data').dropna()
outcomes = data.filter(like='fwd').columns.tolist()
lookahead = 1
outcome= f'r{lookahead:02}_fwd'
X = data.loc[idx[:, :'2017'], :].drop(outcomes, axis=1)
y = data.loc[idx[:, :'2017'], outcome] 
```

## 定义神经网络体系结构框架

为了自动化 TensorFlow 模型的生成，我们创建了一个函数，该函数基于参数构造和编译模型，这些参数稍后可以在交叉验证迭代中传递。

以下`make_model`函数说明了如何灵活定义搜索过程中的各种架构元素。`dense_layers`参数将网络的深度和宽度定义为整数列表。我们还使用`dropout`进行正则化，表示为[0,1]范围内的浮点，以定义给定单元将从训练迭代中排除的概率：

```
def make_model(dense_layers, activation, dropout):
    '''Creates a multi-layer perceptron model

    dense_layers: List of layer sizes; one number per layer
    '''
    model = Sequential()
    for i, layer_size in enumerate(dense_layers, 1):
        if i == 1:
            model.add(Dense(layer_size, input_dim=X_cv.shape[1]))
            model.add(Activation(activation))
        else:
            model.add(Dense(layer_size))
            model.add(Activation(activation))
    model.add(Dropout(dropout))
    model.add(Dense(1))
    model.compile(loss='mean_squared_error',
                  optimizer='Adam')
    return model 
```

现在我们可以转向交叉验证过程来评估各种 NN 架构。

## 交叉验证设计选项以调整 NN

我们使用`MultipleTimeSeriesCV`将数据分为滚动培训和验证集，包括 24*12 个月的数据，同时保留最后 12*21 天的数据（从 2016 年 11 月 30 日开始）作为坚持测试。我们对每个模型进行 48 个 21 天周期的培训，并在 3 个 21 天周期内对其结果进行评估，这意味着交叉验证和测试周期共分 12 次：

```
n_splits = 12
train_period_length=21 * 12 * 4
test_period_length=21 * 3
cv = MultipleTimeSeriesCV(n_splits=n_splits,
                          train_period_length=train_period_length,
                          test_period_length=test_period_length,
                          lookahead=lookahead) 
```

接下来，我们定义一组用于交叉验证的配置。其中包括两个隐藏层和退出概率的几个选项；我们将只使用 tanh 激活，因为与 ReLU 相比，试运行未显示出显著差异。（我们也可以尝试不同的优化器。但我建议您不要运行此实验，以限制已经是计算密集型的工作）：

```
dense_layer_opts = [(16, 8), (32, 16), (32, 32), (64, 32)]
dropout_opts = [0, .1, .2]
param_grid = list(product(dense_layer_opts, activation_opts, dropout_opts))
np.random.shuffle(param_grid)
len(param_grid)
12 
```

为了运行交叉验证，我们定义了一个函数，该函数根据`MultipleTimeSeriesCV`生成的整数指数生成列车和验证数据，如下所示：

```
def get_train_valid_data(X, y, train_idx, test_idx):
    x_train, y_train = X.iloc[train_idx, :], y.iloc[train_idx]
    x_val, y_val = X.iloc[test_idx, :], y.iloc[test_idx]
    return x_train, y_train, x_val, y_val 
```

在交叉验证过程中，我们使用先前定义的网格中的一组参数对模型进行 20 个历次的训练。在每个历元之后，我们存储一个`checkpoint`，其中包含学习的权重，我们可以重新加载该权重，以快速生成最佳配置的预测，而无需重新训练。

在每个历元后，我们计算并存储**信息系数**（**IC**），用于按天设置的验证：

```
ic = []
scaler = StandardScaler()
for params in param_grid:
    dense_layers, activation, dropout = params
    for batch_size in [64, 256]:
        checkpoint_path = checkpoint_dir / str(dense_layers) / activation /
                          str(dropout) / str(batch_size)
        for fold, (train_idx, test_idx) in enumerate(cv.split(X_cv)):
            x_train, y_train, x_val, y_val = get_train_valid_data(X_cv, y_cv,
                                             train_idx, test_idx)
            x_train = scaler.fit_transform(x_train)
            x_val = scaler.transform(x_val)
            preds = y_val.to_frame('actual')
            r = pd.DataFrame(index=y_val.groupby(level='date').size().index)
            model = make_model(dense_layers, activation, dropout)
            for epoch in range(20):            
                model.fit(x_train, y_train,
                          batch_size=batch_size,
                          epochs=1, validation_data=(x_val, y_val))
                model.save_weights(
                    (checkpoint_path / f'ckpt_{fold}_{epoch}').as_posix())
                preds[epoch] = model.predict(x_val).squeeze()
                r[epoch] = preds.groupby(level='date').apply(lambda x: spearmanr(x.actual, x[epoch])[0]).to_frame(epoch)
            ic.append(r.assign(dense_layers=str(dense_layers), 
                               activation=activation, 
                               dropout=dropout,
                               batch_size=batch_size,
                               fold=fold)) 
```

使用 NVIDIA GTX 1080 GPU，20 个时代需要一个多小时的时间处理 64 个批次的样本，大约需要 20 分钟处理 256 个样本。

## 评估预测性能

让我们首先看看在交叉验证期间实现最高每日 IC 中值的五种模型。以下代码计算这些值：

```
dates = sorted(ic.index.unique())
cv_period = 24 * 21
cv_dates = dates[:cv_period]
ic_cv = ic.loc[cv_dates]
(ic_cv.drop('fold', axis=1).groupby(params).median().stack()
 .to_frame('ic').reset_index().rename(columns={'level_3': 'epoch'})
 .nlargest(n=5, columns='ic')) 
```

结果表显示，在两个层中分别使用 32 个单元和在第一层/第二层中分别使用 16/8 个单元的架构表现最佳。这些模型也使用了`dropout`，并使用 64 个样本的批量大小对所有褶皱进行训练，这些样本具有给定的时代数。中值 IC 值在 0.0236 和 0.0246 之间变化：

<colgroup><col> <col> <col> <col> <col></colgroup> 
| 致密层 | 辍学者 | 批量大小 | 纪元 | 集成电路 |
| (32, 32) | 0.1 | 64 | 7. | 0.0246 |
| (16, 8) | 0.2 | 64 | 14 | 0.0241 |
| (16, 8) | 0.1 | 64 | 3. | 0.0238 |
| (32, 32) | 0.1 | 64 | 10 | 0.0237 |
| (16, 8) | 0.2 | 256 | 3. | 0.0236 |

接下来，我们将了解参数选择如何影响预测性能。

首先，我们通过历元可视化不同配置的每日信息系数（平均每倍），以了解训练持续时间如何影响预测准确性。然而，*图 17.7*中的曲线图强调了一些结论性模式；IC 在不同的车型之间变化不大，在不同的时代之间变化不大：

<figure class="mediaobject">![](../Images/B15439_17_07.png)</figure>

图 17.7：各种型号配置的信息系数

为了获得更具统计稳健性的见解，我们使用**普通最小二乘法**（**OLS**）（参见*第 7 章*、*线性模型——从风险因素到回报预测*）进行线性回归，使用层、辍学、，和批次大小选择，以及每个历元：

```
data = pd.melt(ic, id_vars=params, var_name='epoch', value_name='ic')
data = pd.get_dummies(data, columns=['epoch'] + params, drop_first=True)
model = sm.OLS(endog=data.ic, exog=sm.add_constant(data.drop('ic', axis=1))) 
```

*图 17.8*中的图表绘制了各回归系数的置信区间；如果不包括零，则该系数在 5%的水平上是显著的。y 轴上的 IC 值反映了常数（0.0027，p 值：0.017）的差值，该常数表示在删除每个虚拟变量的一个类别时排除的配置上的样本平均值。

在所有配置中，批次大小 256 和 0.2 的退出对性能做出了显著（但很小）的积极贡献。类似地，七个时期的训练产生了略好的效果。根据 F 统计量，回归总体上是显著的，但 R2 值非常低，接近于零，这突出了数据中相对于参数选择所传递信号的高度噪声。

<figure class="mediaobject">![](../Images/B15439_17_08.png)</figure>

图 17.8:OLS 系数和置信区间

## 基于集成信号的策略回溯测试

为了将我们的 NN模型转化为交易策略，我们生成预测，评估其信号质量，创建定义如何根据这些预测进行交易的规则，并对实施这些规则的策略的性能进行回溯测试。本节中的代码示例参见笔记本`backtesting_with_zipline`。

### 整合预测以产生可交易信号

为了减少预测的方差并避免样本内过度拟合，我们将前一节表格中列出的三个最佳模型的预测结合起来，并对结果进行平均。

为此，我们定义了以下`generate_predictions()`函数，该函数接收模型参数作为输入，加载所需历元的模型权重，并为交叉验证和样本外周期创建预测（此处仅显示要点以节省一些空间）：

```
def generate_predictions(dense_layers, activation, dropout,
                         batch_size, epoch):
    checkpoint_dir = Path('logs')
    checkpoint_path = checkpoint_dir / dense_layers / activation /
                      str(dropout) / str(batch_size)

    for fold, (train_idx, test_idx) in enumerate(cv.split(X_cv)):
        x_train, y_train, x_val, y_val = get_train_valid_data(X_cv, y_cv, 
                                                              train_idx, 
                                                              test_idx)
        x_val = scaler.fit(x_train).transform(x_val)
        model = make_model(dense_layers, activation, dropout, input_dim)
        status = model.load_weights(
            (checkpoint_path / f'ckpt_{fold}_{epoch}').as_posix())
        status.expect_partial()
        predictions.append(pd.Series(model.predict(x_val).squeeze(), 
                                     index=y_val.index))
    return pd.concat(predictions) 
```

我们使用 Alphalens 和 Zipline 回溯测试存储评估结果。

### 用 Alphalens 评价信号质量

为了深入了解集成模型预测的信号内容，我们使用 Alphalens 计算五个等重投资组合的回报差异，这些投资组合由预测分位数区分为（参见*图 17.9*。在一天的持有期内，顶部和底部五分位之间的利差约为 8 个基点，这意味着阿尔法为 0.094，贝塔为 0.107：

<figure class="mediaobject">![](../Images/B15439_17_09.png)</figure>

图 17.9：信号质量评估

### 使用 Zipline 对策略进行回溯测试

根据 Alphalens 分析，我们的策略将分别为预测回报率最高和最低的 50 只股票进入多头和空头头寸，只要双方至少有 10 个期权。这种策略每天都在交易。

*图 17.10*中的图表显示，该策略在样本内外都表现良好（交易成本之前）：

<figure class="mediaobject">![](../Images/B15439_17_10.png)</figure>

图 17.10：样本内和样本外回测性能

它在 36 个月内产生 22.8%的年化回报率，24 个样本月内产生 16.5%的年化回报率，12 个样本月外产生 35.7%的年化回报率。夏普比在样品中为 0.72，在样品中为 2.15，在样品中的α值为 0.18（0.29），β值为 0.24（0.16）。

## 如何进一步提高成绩

相对简单的体系结构产生了一些有希望的结果。为了进一步提高性能，您可以首先向模型添加新功能和更多数据。

或者，您可以使用更复杂的体系结构，包括 RNN 和 CNN，它们非常适合于顺序数据，而普通前馈 NN 的设计不是为了捕获功能的有序性。

我们将在下一章中讨论这些专门的体系结构。

# 总结

在本章中，我们介绍了 DL 作为一种表示学习形式，它从高维非结构化数据中提取分层特征。我们了解了如何使用 NumPy 设计、训练和正则化前馈神经网络。我们演示了如何使用流行的 DL 库 PyTorch 和 TensorFlow，它们适用于从快速原型到生产部署的用例。

最重要的是，我们使用 TensorFlow 设计和调整了一个神经网络，并能够生成可交易信号，在样本内和样本外期间提供有吸引力的回报。

在下一章中，我们将探讨 CNN，它特别适合于图像数据，但也非常适合于顺序数据。**