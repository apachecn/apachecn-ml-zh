# 18

# 金融时间序列和卫星图像的 CNN

在本章中，我们将介绍*第 4 部分*中介绍的几种专业深度学习体系结构中的第一种。深度**卷积神经网络**（**CNNs**）在各种计算机视觉任务中实现了超人性能，如图像和视频分类以及图像中目标的检测和识别。CNN 还可以从与图像数据具有某些特征的时间序列数据中提取信号，并已成功应用于语音识别（Abdel Hamid et al.2014）。此外，已证明它们在不同领域的时间序列分类方面具有最先进的性能（Ismail Fawaz et al.2019）。

CNN 是以一种称为**卷积**的线性代数运算命名的，该运算在至少一层中取代了前馈网络（在上一章中讨论）的一般矩阵乘法。我们将展示卷积是如何工作的，以及为什么它们特别适合于具有某种规则结构的数据，这种结构通常存在于图像中，但也存在于时间序列中。

对**CNN 架构**的研究进展非常迅速，提高基准性能的新架构不断涌现。我们将描述一组成功应用程序一致使用的构建块。我们还将演示**转移学习**如何通过使用更接近输入的 CNN 层的预训练权重来加速学习，同时根据特定任务微调最终层。我们还将说明如何将 CNN 用于**目标检测**的特定计算机视觉任务。

CNNs 可以通过图像或（多个）时间序列数据生成信号，帮助建立**交易策略**：

*   **卫星数据**可能预示未来的商品趋势，包括通过农业区、矿山或运输网络（如油轮）的航空图像提供某些作物或原材料。例如，来自购物中心的**监控摄像机**镜头可用于跟踪和预测消费者活动。
*   **时间序列数据**包含非常广泛的数据源，CNN 已被证明通过利用其与图像的结构相似性来提供高质量的分类结果。

我们将根据 CNN 的预测创建一个交易策略，该策略使用时间序列数据，刻意将其格式化为图像，并演示如何构建 CNN 来对卫星图像进行分类。

更具体地说，在本章中，您将了解以下内容：

*   CNN 如何使用多个构建块来高效地对类似网格的数据建模
*   使用 TensorFlow 对图像和时间序列数据的 CNN 进行训练、调整和正则化
*   使用迁移学习简化 CNN，即使数据较少
*   利用 CNN 对图像格式的时间序列数据进行训练，利用回报预测设计交易策略
*   如何对卫星图像进行分类

您可以在 GitHub 存储库的相应目录中找到本章的代码示例以及指向其他资源的链接。笔记本电脑包括图像的彩色版本。

# CNN 如何学习对类似网格的数据建模

CNN 在概念上与前馈**神经网络**（**NNs**）类似：它们由具有称为权重和偏差的参数的单元组成，训练过程根据损失函数调整这些参数以优化给定输入的网络输出。它们最常用于分类。每个单元使用其参数对从其他单元接收的输入数据或激活应用线性操作，通常随后是非线性转换。

整个网络建模了一个**可微函数**，该函数使用输出激活函数（如 softmax）将原始数据（如图像像素）映射到类别概率。CNN 使用一个目标函数，如交叉熵损失，用一个度量来衡量输出的质量。它们还依赖于损失相对于网络参数的梯度，通过反向传播进行学习。

具有完全连接层的前馈神经网络不能很好地扩展到具有大量像素值的高维图像数据。甚至我们将在下一节中使用的 CIFAR-10 数据集中包含的低分辨率图像也包含 32×32 像素，每个像素最多有 256 个不同的颜色值，每个值由 8 位表示。例如，对于 RGB 颜色模型的红色、绿色和蓝色通道，使用三个通道时，完全连接的输入层中的单个单元意味着 32×32×3=3072 个权重。640×480 像素的更标准分辨率已经为单个输入单元提供了接近 100 万个权重。具有几层有意义的宽度的深层架构很快会导致参数数量爆炸式增长，这使得训练过程中过度拟合几乎是必然的。

一个完全连接的前馈神经网络对输入数据的局部结构不作任何假设，因此任意地对特征重新排序对训练结果没有影响。相比之下，CNN 做出了**关键假设**，即**数据具有网格状拓扑**，并且**局部结构非常重要**。换句话说，它们编码了输入具有图像数据中典型结构的假设：像素形成二维网格，可能具有多个通道来表示颜色信号的分量。此外，与远处的数据点相比，附近像素的值可能更适合检测边缘和角点等关键特征。自然，最初的 CNN 应用程序（如手写识别）主要关注图像数据。

然而，随着时间的推移，研究人员认识到时间序列数据中的**类似特征，从而扩大了 CNN 的生产性使用范围。时间序列数据由定期测量组成，这些测量沿时间轴创建一维网格，例如给定股票代码的滞后收益。也可以有第二个维度，该维度具有该股票代码的附加功能和相同的时间段。最后，我们可以使用第三维度来表示其他股票。**

除了图像之外，一个常见的 CNN 用例包括音频数据，或者是时域中的一维波形，或者是经过傅立叶变换后的频域中的二维频谱。CNN 在 AlphaGo 中也扮演着关键角色，AlphaGo 是第一个赢得围棋对抗人类游戏的算法，他们评估了网格状棋盘上的不同位置。

对网格状拓扑的**假设进行编码的最重要元素是**卷积**操作，该操作将 CNN 命名为**池**。我们将看到，关于输入和输出数据之间函数关系的特定假设意味着 CNN 需要的参数要少得多，计算效率也更高。**

在本节中，我们将解释卷积层和池层如何学习提取局部特征的过滤器，以及为什么这些操作特别适用于具有刚才描述的结构的数据。最先进的 CNN 结合了许多这些基本构建块，以实现上一章中描述的分层表示学习。最后，我们将描述过去十年中的关键架构创新，这些创新带来了巨大的性能改进。

## 从手工编码到从数据学习过滤器

对于图像数据，这种局部结构传统上推动了手动编码过滤器的开发，该过滤器提取此类模式作为特征用于**机器学习**（**ML**模型）。

*图 18.1*显示了设计用于检测某些边缘的简单过滤器的效果。笔记本`filter_example.ipynb`演示了如何在卷积网络中使用手工编码的过滤器，并将图像的转换结果可视化。过滤器是简单的[-1，1]图案，排列在![](img/B15439_18_001.png)矩阵中，如图右上角所示。每个过滤器下方显示其效果；它们有点微妙，更容易在随附的笔记本中发现。

![](img/B15439_18_01.png)

图 18.1：应用于图像的基本边缘过滤器的结果

相反，卷积层的设计是为了从数据中学习这种局部特征表示。一个关键的洞察是将他们的输入（称为**感受野**）限制在输入的一个小区域，以便它捕获反映边缘或角落等常见模式的基本像素星座。但是，这种模式可能出现在图像中的任何位置，因此 CNN 还需要识别不同位置的类似模式，并且可能存在微小的变化。

随后的层学习合成这些局部特征以检测**高阶特征**。GitHub 上的链接资源包括如何使用我们在下一节参考体系结构中介绍的一些深层体系结构可视化深层 CNN 学习的过滤器的示例。

## 卷积层的元素如何工作

卷积层集成了**三种架构思想**，能够学习对位移、比例变化和失真在一定程度上保持不变的特征表示：

*   稀疏而非密集连接
*   权重分配
*   空间或时间下采样

此外，卷积层允许大小可变的输入。我们将通过一个典型的卷积层，依次描述这些想法。

*图 18.2*概述了通常在三维卷积层中发生的一组操作，假设图像数据是以高度、宽度和深度的三维或通道数输入的。像素值的范围取决于位表示，例如，8 位为[0255]。或者，宽度轴可以表示时间、高度和不同的特征，通道可以捕获对不同对象（如计时器）的观察。

![](img/B15439_18_02.png)

图 18.2：二维卷积层中的典型操作

连续计算通过卷积、检测器和池阶段处理输入，我们将在接下来的三个部分中描述这些阶段。在*图 18.2*所示的示例中，卷积层接收三维输入并产生相同维度的输出。

最先进的 CNN由多个不同尺寸的层组成，这些层要么相互堆叠，要么在不同的分支上并行运行。通过每一层，网络可以检测到更高层次、更抽象的特征。

### 卷积阶段-提取局部特征

第一阶段对输入图像的重叠块应用过滤器，也称为内核。滤波器是一个比输入小得多的矩阵，因此它的感受野仅限于几个连续值，如像素或时间序列值。因此，它关注于局部模式，并相对于完全连接的层大大减少了参数和计算的数量。

一个完整的卷积层有几个**特征映射**组织为深度切片（如*图 18.2*所示），因此每个层可以提取多个特征。

#### 从过滤器到要素地图

扫描输入时，内核与其感受野覆盖的每个输入段进行卷积。卷积运算只是将滤波器权重和匹配输入区域的值重新整形为向量后的点积。因此，每次卷积产生一个数字，整个扫描产生一个特征图。由于相同向量的点积最大化，因此特征图指示每个输入区域的激活程度。

*图 18.3*说明了使用具有给定值的![](img/B15439_18_003.png)过滤器扫描![](img/B15439_18_002.png)输入的结果，以及特征图右上角的激活是如何从平坦输入区域和内核的点积中产生的：

![](img/B15439_18_03.png)

图 18.3：从卷积到特征图

最重要的是，**滤波器值是卷积层的参数**，**在训练期间从数据**中学习，以最小化所选的损失函数。换句话说，CNN 通过查找激活对手头任务最有用的输入模式的内核值来学习有用的特征表示。

#### 如何扫描输入–跨步和填充

**步长**定义用于扫描输入的步长，即水平和垂直移动的像素数。较小的跨步扫描更多（重叠）区域，但计算成本更高。当过滤器与输入不完全匹配且在扫描过程中部分穿过图像边界时，通常使用四个选项：

*   **有效卷积**：丢弃图像和过滤器不完全匹配的扫描
*   **相同卷积**：零填充输入，生成大小相等的特征映射
*   **全卷积**：零填充输入，使每个像素扫描相同次数，包括边界处的像素（以避免对靠近中心的像素进行过采样）
*   **因果**：零点只在左边填充输入，这样输出就不依赖于后期的输入；维护时间序列数据的时间顺序

选择取决于数据的性质以及最有可能定位有用特征的位置。结合深度切片的数量，它们决定卷积阶段的输出大小。Andrew Karpath 的斯坦福课堂讲稿（参见 GitHub）包含使用 NumPy 的有用示例。

#### 鲁棒特征和快速计算的参数共享

显著特征的位置可能因变形或位移而变化。此外，基本特征检测器可能对整个图像有用。CNN 通过在给定深度切片中共享或绑定过滤器的权重来对这些假设进行编码。

因此，每个深度切片专门处理特定的模式，并且进一步减少了参数的数量。然而，当图像在空间上居中且关键模式不太可能均匀分布在整个输入区域时，权重共享的效果较差。

### 探测器级-增加非线性

特征映射通常通过非线性变换传递。我们在上一章中遇到的**整流线性单元**（**ReLU**）是用于此目的的常见函数。ReLUs 将负激活元素替换为零，并降低在其他激活功能（如 tanh）中发现的消失梯度的风险（参见*第 17 章*、*交易深度学习*）。

一个流行的替代方案是**softplus 功能**：

![](img/B15439_18_004.png)

与 ReLU 相反，它到处都有导数，即我们用于逻辑回归的 S 形函数（参见*第 7 章*、*线性模型——从风险因素到回报预测*。

### 池阶段–对特征图进行下采样

卷积层的最后阶段可对特征图的输入表示进行下采样，以执行以下操作：

*   减少其尺寸并防止过度装配
*   降低计算成本
*   启用基本平移不变性

这假设特征的精确位置不仅对识别模式不那么重要，甚至可能有害，因为它可能因目标的不同实例而不同。合并降低了要素地图的空间分辨率，这是一种使位置信息不那么精确的简单方法。然而，这一步是可选的，许多体系结构仅对某些层使用池，或者根本不使用池。

常见的池操作是**最大池**，它仅使用（通常）非重叠子区域的最大激活值。例如，对于一个小的![](img/B15439_18_005.png)特征图，![](img/B15439_18_001.png)最大池输出四个非重叠![](img/B15439_18_001.png)区域中每个区域的最大值。较不常见的池操作员使用平均值或中位数。池不会添加或学习新参数，但输入窗口的大小和步幅可能是额外的超参数。

## CNN 架构的演变——关键创新

在过去的二十年中，几个 CNN架构通过引入重要的创新，突破了性能界限。随着以 ImageNet（Fei-Fei 2015）形式出现的大数据的到来，预测性能的增长急剧加快，人类通过亚马逊的 Mechanical Turk 将 1400 万张图像分配到 20000 个类别。**ImageNet 大规模视觉识别挑战**（**ILSVRC**）成为 CNN 进步的焦点，围绕 1000 个等级的 120 万张稍小的图像。

出于实际原因，熟悉主导这些竞争的**参考体系结构**是很有用的。正如我们将在下一节中看到的，使用 CNN 处理图像数据，它们为标准任务提供了一个良好的起点。此外，**转移学习**允许我们通过构建一个具有预训练权重的成功架构来处理许多计算机视觉任务。迁移学习不仅加快了体系结构选择和培训，而且能够在更小的数据集上成功应用。

此外，许多出版物都提到了这些体系结构，它们通常是为细分或本地化任务定制的网络的基础。在图像分类和迁移学习部分，我们将进一步描述一些里程碑式的体系结构。

### 性能突破和网络规模

*图 18.4*左侧根据各种网络架构的计算成本绘制了 top-1 精度。这表明参数数量与性能之间存在正相关关系，但也表明更多参数的边际效益下降，建筑设计和创新也很重要。

右侧绘制了所有网络每个参数的最高精度。一些新的架构针对功能较弱的设备（如手机）上的用例。虽然他们没有达到最先进的性能，但他们发现了更高效的实现。请参阅 GitHub 上的参考资料，了解这些架构的更多详细信息以及这些图表背后的分析。

![](img/B15439_18_04.png)

图 18.4：预测性能和计算复杂性

### 经验教训

从 20 年的 CNN 架构发展（特别是自 2012 年以来）中汲取的一些经验教训包括：

*   **较小的卷积**滤波器性能更好（可能除了第一层），因为几个较小的滤波器可以以较低的计算成本替代较大的滤波器。
*   **1×1 卷积**降低特征映射的维数，使网络能够整体学习更多的数字。
*   **跳过连接**能够通过网络创建多条路径，并支持训练更大容量的 CNN。

# 用于卫星图像和目标检测的 CNN

在本节中，我们将演示如何解决关键的计算机视觉任务，如图像分类和目标检测。正如引言和*第 3 章**金融替代数据–类别和用例*中所述，图像数据可以通过提供与目标资产类别或投资领域相关的未来趋势、变化的基本面或特定事件的线索来通知交易策略。流行的例子包括利用卫星图像寻找有关农产品供应、消费者和经济活动或制造业或原材料供应链状况的线索。具体任务可能包括以下内容，例如：

*   **图像分类**：识别某些作物的耕地是否在扩大，或者预测收获的质量和数量
*   **目标检测**：统计某条运输路线上的油轮数量或停车场内的汽车数量，或识别购物商场内购物者的位置

在本节中，我们将演示如何设计 CNN 以自动提取此类信息，既可以使用流行的体系结构从头开始，也可以通过将预先训练的权重微调到给定任务的转移学习。我们还将演示如何在给定场景中检测对象。

我们将介绍这些任务的关键 CNN 体系结构，解释它们工作良好的原因，并展示如何使用 TensorFlow 2 对它们进行培训。我们还将演示如何获取预训练权重和微调时间。不幸的是，含有与交易策略直接相关信息的卫星图像获取成本非常高，而且不容易获得。然而，我们将演示如何使用 EuroSat 数据集来构建识别不同土地用途的分类器。本《CNNs for computer vision》简介旨在展示如何处理您在设计基于所选投资领域相关图像的交易策略时可能需要处理的常见任务。

我们在上一章介绍的所有库都支持卷积层；我们将关注 TensorFlow 2 的 Keras 接口。我们首先将使用 MNIST 手写数字数据集来说明 LeNet5 体系结构。接下来，我们将演示在 CIFAR-10（原始 ImageNet 的简化版本）上使用 AlexNet 进行数据增强。然后，在将所学知识应用于实际卫星图像之前，我们将继续基于最先进的体系结构的迁移学习。最后，我们给出了一个现实场景中的目标检测示例。

## LeNet5–第一个具有工业应用的 CNN

Yann LeCun 现在是 Facebook 的人工智能研究总监，是 CNN 发展的先驱。1998 年，在 20 世纪 80 年代开始的几次迭代之后，LeNet5 成为第一个在现实世界应用中使用的现代 CNN，它引入了一些至今仍然相关的架构元素。

LeNet5 发表在一篇非常有启发性的论文中，*基于梯度的学习应用于文档识别*（LeCun 等人，1989 年），阐述了许多核心概念。最重要的是，它促进了这样一种认识，即使用可学习滤波器的卷积可以有效地在多个位置以较少的参数提取相关特征。考虑到当时有限的计算资源，效率至关重要。

LeNet5 被设计用来识别支票上的笔迹，并被多家银行使用。它建立了一个新的分类准确率基准，在 MNIST 手写数字数据集上的结果为 99.2%。它由三个卷积层组成，每个卷积层包含一个非线性 tanh 变换、一个池操作和一个完全连接的输出层。在整个卷积层中，特征贴图的数量增加，而其尺寸减小。它总共有 60850 个可训练参数（Lecun 等人，1998 年）。

### CNN 的“Hello World”-手写数字分类

在本节中，我们将实现一个略为简化的 LeNet5 版本，以演示如何使用 TensorFlow 实现构建 CNN。原始 MNIST 数据集包含 60000 个分辨率为![](img/B15439_18_008.png)像素的灰度图像，每个图像包含一个从 0 到 9 的手写数字。一个好的选择是我们在*第 13 章*、*数据驱动风险因素和无监督学习的资产配置*中遇到的更具挑战性但结构类似的时尚 MNIST 数据集。具体实施详见`digit_classification_with_lenet5`笔记本。

我们可以将其从箱子中装入 Keras：

```py
from tensorflow.keras.datasets import mnist
(X_train, y_train), (X_test, y_test) = mnist.load_data()
X_train.shape, X_test.shape
((60000, 28, 28), (10000, 28, 28)) 
```

*图 18.5*显示了数据集中的前十幅图像，并突出显示了同一数字实例之间的显著差异。在右侧，它显示了单个图像的像素值如何在 0 到 255 之间变化：

![](img/B15439_18_05.png)

图 18.5:MNIST 示例图像

我们将像素值重新缩放到[0,1]范围，以规范化训练数据，促进反向传播过程，并将数据转换为 32 位浮点，从而减少内存需求和计算成本，同时为我们的用例提供足够的精度：

```py
X_train = X_train.astype('float32')/255
X_test = X_test.astype('float32')/255 
```

### 定义 LeNet5 体系结构

我们可以定义一个简化版的LeNet5，它省略了包含径向基函数的原始最终层，如下所示，使用默认的“有效”填充和单步跨步，除非另有定义：

```py
lenet5 = Sequential([
    Conv2D(filters=6, kernel_size=5, activation='relu', 
           input_shape=(28, 28, 1), name='CONV1'),
    AveragePooling2D(pool_size=(2, 2), strides=(1, 1), 
                     padding='valid', name='POOL1'),
    Conv2D(filters=16, kernel_size=(5, 5), activation='tanh', name='CONV2'),
    AveragePooling2D(pool_size=(2, 2), strides=(2, 2), name='POOL2'),
    Conv2D(filters=120, kernel_size=(5, 5), activation='tanh', name='CONV3'),
    Flatten(name='FLAT'),
    Dense(units=84, activation='tanh', name='FC6'),
    Dense(units=10, activation='softmax', name='FC7')
]) 
```

总结表明，这样定义的模型有超过 300000 个参数：

```py
Layer (type)                 Output Shape              Param #   
CONV1 (Conv2D)               (None, 24, 24, 6)         156       
POOL1 (AveragePooling2D)     (None, 23, 23, 6)         0         
CONV2 (Conv2D)               (None, 19, 19, 16)        2416      
_________________________________________________________________
POOL2 (AveragePooling2D)     (None, 9, 9, 16)          0         
_________________________________________________________________
CONV3 (Conv2D)               (None, 5, 5, 120)         48120     
_________________________________________________________________
FLAT (Flatten)               (None, 3000)              0         
_________________________________________________________________
FC6 (Dense)                  (None, 84)                252084    
________________________________________________________________
FC7 (Dense)                  (None, 10)                850       =================================================================
Total params: 303,626
Trainable params: 303,626 
```

我们使用`sparse_crossentropy_loss`编译，它接受整数而不是一个热编码标签和原始随机梯度优化器：

```py
lenet5.compile(loss='sparse_categorical_crossentropy',
               optimizer='SGD',
               metrics=['accuracy']) 
```

### 培训和评估模型

现在我们准备训练模型。模型需要四维输入，因此我们相应地重塑。我们使用 32 的标准批量和 80:20 的序列验证分割。此外，如果验证错误有所改善，我们将利用检查点存储模型权重，并确保数据集被随机洗牌。我们还定义了一个`early_stopping`回调，一旦验证精度在 20 次迭代中不再提高，就中断培训：

```py
lenet_history = lenet5.fit(X_train.reshape(-1, 28, 28, 1),
                          y_train,
                          batch_size=32,
                          epochs=100,
                          validation_split=0.2, # use 0 to train on all data
                          callbacks=[checkpointer, early_stopping],
                          verbose=1,
                          shuffle=True) 
```

训练历史记录了在一个 GPU 上进行了 81 次耗时约 4 分钟的训练后的最后一次改进。此样本运行的测试准确率为 99.09%，与原始 LeNet5 的结果几乎完全相同：

```py
accuracy = lenet5.evaluate(X_test.reshape(-1, 28, 28, 1), y_test, verbose=0)[1]
print('Test accuracy: {:.2%}'.format(accuracy))
Test accuracy: 99.09% 
```

相比之下，一个简单的两层前馈网络“仅”达到 97.04%的测试精度（见笔记本）。事实上，MNIST 的 LeNet5 改进是适度的。非神经方法也取得了大于或等于 99%的分类准确率，包括 K 近邻和支持向量机。CNN 在更具挑战性的数据集上大放异彩，我们将在下面看到。

## AlexNet–重新启动深度学习研究

AlxNET，由多伦多大学的 Alex Krizhevsky、Ilya Sutskever 和 Geoff Hinton 开发的 AutoT0T，显著地降低了错误率，并且显著地超过了 2012 ILVRC 的亚军，获得了 16%比 26%的前 5 误差（KrijaveSky，StSkWyver 和 Huton 2012）。这一突破触发了 ML 研究的复兴，并将计算机视觉的深度学习牢牢地放在全球技术地图上。

AlexNet 架构类似于 LeNet，但更深入、更广泛。人们通常认为它发现了深度的重要性，大约有 6000 万个参数，超过 LeNet5 1000 倍，这证明了计算能力的提高，特别是 GPU 的使用，以及更大的数据集。

它包括堆叠在彼此之上的卷积，而不是将每个卷积与池阶段相结合，并成功地使用 dropout 进行正则化，使用 ReLU 进行有效的非线性变换。它还使用数据扩充来增加训练样本的数量，增加权重衰减，并使用更有效的卷积实现。它还通过在两个 GPU 上分配网络来加速训练。

笔记本`image_classification_with_alexnet.ipynb`有一个稍微简化的 AlexNet 版本，该版本根据 CIFAR-10 数据集定制，其中包含原始 1000 个类中 10 个类的 60000 个图像。它已经从原来的![](img/B15439_18_010.png)压缩到像素分辨率，但仍然有三个颜色通道。

具体实施见`image_classification_with_alexnet`笔记本；这里我们将跳过一些重复的步骤。

### 基于图像增强的 CIFAR-10 数据预处理

CIFAR-10 也可以使用 TensorFlow 的 Keras 接口下载，我们重新缩放像素值，并对十个类别标签进行热编码，正如我们在上一节中对 MNIST 所做的。

我们首先在 50000 个训练样本上训练一个两层前馈网络，训练时间为 45 个历元，以达到 45.78%的测试准确率。我们还使用一个三层卷积网络进行了实验，该网络具有超过 528000 个参数，达到 74.51%的测试精度（见笔记本）。

提高性能的一个常见技巧是通过创建合成数据人为地增加训练集的大小。这包括随机移动或水平翻转图像，或在图像中引入噪声。TensorFlow 为此包含一个`ImageDataGenerator`类。我们可以按如下方式配置它并拟合培训数据：

```py
from tensorflow.keras.preprocessing.image import ImageDataGenerator
datagen = ImageDataGenerator(
    width_shift_range=0.1,   # randomly horizontal shift
    height_shift_range=0.1,  # randomly vertical shift
    horizontal_flip=True)    # randomly horizontal flip
datagen.fit(X_train) 
```

结果显示了增强图像（低 32×32 分辨率）是如何按照预期以各种方式改变的：

![](img/B15439_18_06.png)

图 18.6：原始和扩充样本

三层 CNN 的测试准确率在对更大的增强数据进行训练后略微提高到 76.71%。

### 定义模型架构

我们需要使 AlexNet 体系结构适应 CIFAR-10 图像相对于竞赛中使用的 ImageNet 样本的低维性。为此，我们使用了原始数量的过滤器，但使其更小（有关实现细节，请参阅笔记本）。

摘要（见笔记本）显示了五个卷积层，然后是两个完全连接的层，经常使用批标准化，总共有 2150 万个参数。

### 比较 AlexNet 性能

除了 AlexNet 之外，我们还训练了一个 2 层前馈神经网络和一个 3 层 CNN，后者有图像增强和没有图像增强。经过 100 个历次（如果 20 轮验证精度没有提高，则提前停止），我们获得了四个模型的交叉验证轨迹和测试精度，如*图 18.7*所示：

![](img/B15439_18_07.png)

图 18.7:CIFAR-10 的验证性能和测试精度

亚历克斯内特在经历了大约 35 个时代后，以 79.33%的准确率达到了最高的测试，紧随其后的是较浅的 CNN，其增强图像的准确率为 78.29%，由于数据集较大，训练时间更长。在这个更复杂的数据集上，前馈神经网络的性能比 MNIST 差得多，测试准确率为 43.05%。

## 转移学习–使用更少的数据进行更快的培训

在实践中，有时我们没有足够的数据来用随机初始化从头开始训练 CNN。**转移学习**是一种ML 技术，它将根据一组数据训练的模型重新用于另一项任务。当然，如果从第一个任务中学习到感兴趣的任务，它就会起作用。如果成功，它可以带来更好的性能和更快的训练，与在目标任务上从头开始训练神经网络相比，需要更少的标记数据。

### 迁移学习的替代方法

CNN 的迁移学习方法依赖于对像 ImageNet 这样的大型数据集进行预训练。卷积滤波器的目标是提取一种可以推广到新图像的特征表示。在第二步中，它利用结果初始化和重新训练新的 CNN，或者将其用作处理感兴趣任务的新网络的输入。

如前所述，CNN 架构通常使用卷积层序列来检测分层模式，添加一个或多个完全连接的层以将卷积激活映射到结果类或值。最后一个卷积层的输出反馈到完全连接的部分，称为瓶颈特征。我们可以使用预训练网络的**瓶颈特征**作为新的完全连接网络的输入，通常在应用 ReLU 激活功能之后。

换言之，我们冻结卷积层并**替换网络**的密集部分。另一个好处是，我们可以使用不同大小的输入，因为正是密集层限制了输入大小。

或者，我们可以使用瓶颈特征作为**输入到不同的机器学习算法**。例如，在 AlexNet 架构中，瓶颈层为每个![](img/B15439_18_010.png)输入图像计算一个包含 4096 个条目的向量。然后我们使用这个向量作为新模型的特征。

我们还可以更进一步，不仅使用新数据替换和重新训练最终层，还可以**微调预训练 CNN**的权重。为了实现这一点，我们继续训练，或者只为后面的层进行训练，同时冻结一些早期层的重量，或者为所有层进行训练。其动机可能是保留较低层（如边缘或色块检测器）学习到的更通用的模式，同时允许 CNN 的后续层适应新任务的细节。例如，ImageNet 包含各种各样的狗品种，这可能导致特征表示特别有助于区分这些类别。

### 以最先进的建筑为基础

迁移学习使我们能够利用性能最佳的体系结构，而无需进行潜在的 GPU 和数据密集型培训。我们简要地概述了作为流行起点的一些其他流行体系结构的关键特性。

#### VGGNet–深度更大，过滤器更小

2014 年 ILSVRC 的亚军由牛津大学的视觉几何组（VGG，Simonyan 2015）开发。它证明了更小的**![](img/B15439_18_012.png)**卷积滤波器**按顺序组合的有效性，并强调了深度对于高性能的重要性。VGG16 包含 16 个仅执行![](img/B15439_18_012.png)卷积和![](img/B15439_18_014.png)池的卷积和完全连接层（参见*图 18.5*。**

 **VGG16 有**1.4 亿个**参数，增加了训练和推理的计算成本以及内存需求。但是，大多数参数都位于完全连接的层中，这些层后来被发现不是必需的，因此删除它们可以大大减少参数数量，而不会对性能产生负面影响。

#### GoogLeNet–通过初始阶段减少参数

Google的 Christian Szegedy 使用更高效的 CNN 实现来简化大规模的实际应用，从而降低了计算成本。由此产生的谷歌网（Szegedy et al.2015）凭借**初始模块**仅以 400 万个参数赢得了 2014 年 ILSVRC，相比之下，AlexNet 的 6000 万个参数和 VGG16 的 1.4 亿个参数。

初始模块建立在网络概念中的**网络上，该网络使用![](img/B15439_18_015.png)卷积来压缩卷积滤波器的深层堆栈，从而降低计算成本。模块使用并行的![](img/B15439_18_015.png)、![](img/B15439_18_017.png)和![](img/B15439_18_018.png)滤波器，将后两者与![](img/B15439_18_015.png)卷积相结合，以降低前一层传入的滤波器的维数。**

此外，它使用平均池而不是卷积层之上的完全连接层来消除许多影响较小的参数。有几个增强版本，最近的是 Inception-v4。

#### ResNet–超越人因绩效的快捷连接

**剩余网络****（ResNet）**架构由微软开发，并获得 2015 年 ILSVRC 大奖。它将前五名错误率推至 3.7%，低于该任务中约 5%的人因水平（He 等人，2015 年）。

它引入了跨越几层的身份快捷连接，克服了训练深层网络的一些挑战，实现了数百层甚至上千层的使用。它还大量使用批量标准化，这被证明允许更高的学习率和更宽容的重量初始化。该体系结构还省略了完全连接的最终层。

如上一章所述，深度网络的训练面临着臭名昭著的消失梯度挑战：当梯度传播到早期层时，小权重的重复乘法有可能将梯度缩小到零。因此，增加深度可能会限制学习。

跳过两层或更多层的快捷连接已成为 CNN 架构中最流行的发展之一，并引发了无数研究工作，以改进和解释其性能。有关更多信息，请参阅 GitHub 上的参考资料。

### VGG16 在迁移学习中的应用

现代 CNN 可能需要周时间在 ImageNet 上的多个 GPU 上进行训练，但幸运的是，许多研究人员分享了他们的最终体重。例如，TensorFlow 2 包含前面讨论的几种参考体系结构的预训练模型，即 VGG16 及其更大版本 VGG19、ResNet50、InceptionV3 和 InceptionResNetV2，以及 MobileNet、DenseNet、NASNet 和 MobileNet V2。

#### 如何提取瓶颈特征

笔记本`bottleneck_features.ipynb`说明了如何下载预训练的 VGG16 模型，可以下载最终层生成预测，也可以不下载最终层，如*图 18.8*所示，以提取瓶颈特征产生的输出：

![](img/B15439_18_08.png)

图 18.8:VGG16 体系结构

TensorFlow 2 使下载和使用预训练模型变得非常简单：

```py
from tensorflow.keras.applications.vgg16 import VGG16
vgg16 = VGG16()
vgg16.summary()
Layer (type)                 Output Shape              Param #   
input_1 (InputLayer)         (None, 224, 224, 3)       0            
… several layers omitted... 
block5_conv4 (Conv2D)        (None, 14, 14, 512)       2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         
_________________________________________________________________
flatten (Flatten)            (None, 25088)             0         
fc1 (Dense)                  (None, 4096)              102764544 
fc2 (Dense)                  (None, 4096)              16781312  
predictions (Dense)          (None, 1000)              4097000   
Total params: 138,357,544
Trainable params: 138,357,544 
```

您可以像任何其他 Keras 模型一样使用此模型进行预测：我们传入七张样本图像，并获得 1000 个 ImageNet 类别中每一个类别的类别概率：

```py
y_pred = vgg16.predict(img_input)
Y_pred.shape
(7, 1000) 
```

要排除完全连接的层，只需添加关键字`include_top=False`。预测现在由最终卷积层`block5_pool`输出，并与该层的形状相匹配：

```py
vgg16 = VGG16(include_top=False)
vgg16.predict(img_input).shape
(7, 7, 7, 512) 
```

通过省略完全连接的层并仅保留卷积模块，我们不再被迫为模型使用固定的输入大小，如原始的![](img/B15439_18_010.png)ImageNet 格式。相反，我们可以使模型适应任意的输入大小。

#### 如何微调预训练模型

我们将演示如何冻结预训练模型的部分或所有层，并使用一组新的完全连接的层和不同格式的数据继续训练（参见笔记本`transfer_learning.ipynb`中的代码示例，改编自 TensorFlow 2 教程）。

我们使用 ImageNet 上预训练的 VGG16 权重和 TensorFlow 内置的猫对狗图像（参见如何获取数据集的笔记本）。

预处理将所有图像的大小调整为![](img/B15439_18_021.png)像素。在实例化预训练的 VGG16 实例，然后冻结所有权重时，我们指示新的输入大小：

```py
vgg16 = VGG16(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')
vgg16.trainable = False
vgg16.summary()
Layer (type)                 Output Shape              Param #   
... omitted layers...
block5_conv3 (Conv2D)        (None, 10, 10, 512)         2359808   
block5_pool (MaxPooling2D)   (None, 5, 5, 512)         0         
Total params: 14,714,688
Trainable params: 0
Non-trainable params: 14,714,688 
```

现在，32 个样本图像的模型输出形状与无头模型中最后一个卷积层的形状匹配：

```py
feature_batch = vgg16(image_batch)
Feature_batch.shape
TensorShape([32, 5, 5, 512]) 
```

我们可以使用顺序 API 或函数 API 将新层附加到 headless 模型中。对于顺序 API，添加`GlobalAveragePooling2D`、`Dense`和`Dropout`层的工作如下：

```py
global_average_layer = GlobalAveragePooling2D()
dense_layer = Dense(64, activation='relu')
dropout = Dropout(0.5)
prediction_layer = Dense(1, activation='sigmoid')
seq_model = tf.keras.Sequential([vgg16, 
                                 global_average_layer, 
                                 dense_layer, 
                                 dropout, 
                                 prediction_layer])
seq_model.compile(loss = tf.keras.losses.BinaryCrossentropy(from logits=True),
                       optimizer = 'Adam', 
                       metrics=["accuracy"]) 
```

我们为`BinaryCrossentropy`损耗设置`from_logits=True`，因为模型提供了线性输出。摘要显示了新模型如何结合预训练 VGG16 卷积层和新的最终层：

```py
seq_model.summary()
Layer (type)                 Output Shape              Param #   
vgg16 (Model)                (None, 5, 5, 512)         14714688  
global_average_pooling2d (Gl (None, 512)               0         
dense_7 (Dense)              (None, 64)                32832     
dropout_3 (Dropout)          (None, 64)                0         
dense_8 (Dense)              (None, 1)                 65        
Total params: 14,747,585
Trainable params: 11,831,937
Non-trainable params: 2,915,648 
```

功能 API 版本见笔记本。

在训练新的最后一层之前，经过预训练的 VGG16 提供了 48.75%的验证准确率。现在，我们继续对模型进行 10 个阶段的训练，如下所示，仅调整最终层权重：

```py
history = transfer_model.fit(train_batches,
                            epochs=initial_epochs,
                            validation_data=validation_batches) 
```

10 个时代将验证准确率提高到 94%以上。要微调模型，我们可以解冻 VGG16 模型并继续训练。请注意，您应该仅在训练新的最终层后执行此操作：随机初始化的分类层可能会生成大的渐变更新，从而消除预训练结果。

要解冻模型的部分，我们选择一个层，然后将权重设置为`trainable`；在这种情况下，VGG16 体系结构中总共 19 层中的第 12 层：

```py
vgg16.trainable = True
len(vgg16.layers)
19
# Fine-tune from this layer onward
start_fine_tuning_at = 12
# Freeze all the layers before the 'fine_tune_at' layer
for layer in vgg16.layers[:start_fine_tuning_at]:
    layer.trainable =  False 
```

现在，只需重新编译模型，并使用提前停止，从第 10 个历元开始，继续训练 50 个历元，如下所示：

```py
fine_tune_epochs = 50
total_epochs = initial_epochs + fine_tune_epochs
history_fine_tune = transfer_model.fit(train_batches,
                                     epochs=total_epochs,
                                     initial_epoch=history.epoch[-1],
                                     validation_data=validation_batches,
                                     callbacks=[early_stopping]) 
```

*图 18.9*显示了验证准确率是如何大幅提高的，在另外 22 个时期后达到 97.89%：

![](img/B15439_18_09.png)

图 18.9：交叉验证性能：准确性和交叉熵损失

在训练数据有限的情况下，迁移学习是一项重要的技术，在实践中经常如此。虽然猫和狗不太可能产生可交易的信号，但转移学习肯定有助于提高相关替代数据集的预测准确性，比如我们接下来要处理的卫星图像。

### 基于迁移学习的卫星图像分类

卫星图像图在备选数据中占据显著位置（参见*第 3 章*、*财务备选数据-类别和用例*。例如，商品贸易商可能依靠卫星图像，通过监测农场、采矿场的活动或油轮交通来预测某些作物或资源的供应。

#### 欧洲卫星数据集

为了说明处理此类数据，我们加载了 TensorFlow 2 数据集中包含的 EuroSat 数据集（Helber et al.2019）。EuroSat 数据集包括约 27000 张![](img/B15439_18_022.png)格式的图像，代表 10 种不同的土地利用类型。*图 18.10*显示了每个标签的示例：

![](img/B15439_18_10.png)

图 18.10：数据集中包含的十种土地利用类型

类似数据的时间序列可用于跟踪种植区、工业区和居民区的相对大小或特定作物的状态，以预测收获数量或质量，例如葡萄酒。

#### 微调非常深入的 CNN–DenseNet201

Huang et al.（2018）开发了一种称为**密集连接**的新架构，基于这样一种洞察：如果 CNN 在靠近输入层和靠近输出层之间包含较短的连接，则 CNN 可以更深入、更准确、更有效地进行训练。

一个名为**DenseNet201**的架构以前馈方式将每一层连接到其他每一层。它使用所有前面图层的特征贴图作为输入，而每个图层自己的特征贴图成为所有后续图层的输入。

我们从`tensorflow.keras.applications`下载 DenseNet201 体系结构，并用以下密集层替换其最终层，这些密集层穿插着批量标准化，以缓解这一拥有 700 多层的非常深的网络中的爆炸或消失梯度：

```py
Layer (type)                 Output Shape              Param #   
densenet201 (Model)          (None, 1920)              18321984  
batch_normalization (BatchNo (None, 1920)              7680      
dense (Dense)                (None, 2048)              3934208   
batch_normalization_1 (Batch (None, 2048)              8192      
dense_1 (Dense)              (None, 2048)              4196352   
batch_normalization_2 (Batch (None, 2048)              8192      
dense_2 (Dense)              (None, 2048)              4196352   
batch_normalization_3 (Batch (None, 2048)              8192      
dense_3 (Dense)              (None, 2048)              4196352   
batch_normalization_4 (Batch (None, 2048)              8192      
dense_4 (Dense)              (None, 10)                20490     
Total params: 34,906,186
Trainable params: 34,656,906
Non-trainable params: 249,280 
```

#### 模式培训和成果评价

我们使用 10%的训练图像进行验证，在 10 个时间段后实现了 97.96%的最佳样本外分类准确率。这超过了原论文中提到的性能最佳的 ResNet-50 体系结构（90-10 分割）。

![](img/B15439_18_11.png)

图 18.11：交叉验证性能

增加相对较小的训练集可能会带来额外的性能提升。

## 目标检测与分割

图像分类是一项基本的计算机视觉任务，需要根据图像包含的特定对象对图像进行标记。许多实际应用，包括投资和交易策略，都需要额外的信息：

*   **目标检测**任务不仅需要识别所有感兴趣的目标，还需要空间位置，通常使用边界框。已经开发了几种算法来克服强力滑动窗口方法的低效性，包括区域建议方法（R-CNN；参见 Ren 等人 2015 的例子）和**只看一次**（**YOLO**）实时对象检测算法（Redmon 2016）。
*   **对象分割**任务更进一步，需要输入图像中每个对象的类别标签和轮廓。这可能有助于计算图像中的对象，如油轮、个人或汽车，并评估活动水平。
*   **语义分割**也称场景解析，通过密集预测为图像中的每个像素指定一个类别标签。结果，图像被划分为语义区域，每个像素被指定给其包围的对象或区域。

目标检测要求能够区分几类对象，并确定图像中存在多少以及哪些对象。

## 实际中的目标检测

一个突出的例子是伊恩·古德费罗从谷歌的**街景门牌号**（**SVHN**）数据集（古德费罗 2014）中识别门牌号。需要模型识别以下内容：

*   最多五位数字中有多少位构成门牌号
*   每个组件的正确数字
*   组成数字的正确顺序

我们将展示如何预处理不规则形状的源图像，调整 VGG16 架构以产生多个输出，并在微调预训练权重以完成任务之前训练最后一层。

### 对源图像进行预处理

笔记本`svhn_preprocessing.ipynb`包含代码，用于生成简化的裁剪数据集，该数据集使用边界框信息创建包含数字的规则形状![](img/B15439_18_023.png)图像；原始图像具有任意形状（Netzer 2011）。

![](img/B15439_18_12.png)

图 18.12:SVHN 数据集的裁剪样本图像

SVHN 数据集包含最多五位的门牌号，如果没有数字，则使用类别 10。但是，由于只有很少的五位数的示例，我们将图像限制为仅包含四位数的图像。

### 具有自定义最终层的迁移学习

笔记本`svhn_object_detection.ipynb`说明了如何将迁移学习应用到基于 VGG16 架构的深度 CNN，如前一节所述。我们将描述如何创建新的最终层，该层生成多个输出，以满足三个 SVHN 任务目标，包括一个对数字数量的预测，以及一个对每个数字按出现顺序的值的预测。

原始数据集上性能最好的架构有八个卷积层和两个最终完全连接的层。我们将使用**迁移学习**，与 VGG16 架构不同。如前所述，我们导入在 ImageNet 权重上预训练的 VGG16 网络，移除卷积块后的层，冻结权重，并使用函数 API 创建新的密集预测层，如下所示：

```py
vgg16 = VGG16(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')
vgg16.trainable = False
x = vgg16.output
x = Flatten()(x)
x = BatchNormalization()(x)
x = Dense(256)(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = Dense(128)(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
n_digits = Dense(SEQ_LENGTH, activation='softmax', name='n_digits')(x)
digit1 = Dense(N_CLASSES-1, activation='softmax', name='d1')(x)
digit2 = Dense(N_CLASSES, activation='softmax', name='d2')(x)
digit3 = Dense(N_CLASSES, activation='softmax', name='d3')(x)
digit4 = Dense(N_CLASSES, activation='softmax', name='d4')(x)
predictions = Concatenate()([n_digits, digit1, digit2, digit3, digit4]) 
```

预测层将数字数量`n_digits`的四类输出与预测该位置存在哪个数字的四个输出相结合。

### 创建自定义损失函数和评估指标

自定义输出要求我们定义一个损失函数，以捕获模型满足其目标的程度。我们还希望测量准确度，以反映针对特定标签定制的预测准确度。

对于自定义损失，我们平均五个分类输出的交叉熵，即位数及其各自的值：

```py
def weighted_entropy(y_true, y_pred):
    cce = tf.keras.losses.SparseCategoricalCrossentropy()
    n_digits = y_pred[:, :SEQ_LENGTH]
    digits = {}
    for digit, (start, end) in digit_pos.items():
        digits[digit] = y_pred[:, start:end]
    return (cce(y_true[:, 0], n_digits) +
            cce(y_true[:, 1], digits[1]) +
            cce(y_true[:, 2], digits[2]) +
            cce(y_true[:, 3], digits[3]) +
            cce(y_true[:, 4], digits[4])) / 5 
```

为了测量预测准确度，我们将五个预测值与相应的标签值进行比较，并平均该批次样本中正确匹配的份额：

```py
def weighted_accuracy(y_true, y_pred):
    n_digits_pred = K.argmax(y_pred[:, :SEQ_LENGTH], axis=1)
    digit_preds = {}
    for digit, (start, end) in digit_pos.items():
        digit_preds[digit] = K.argmax(y_pred[:, start:end], axis=1)
    preds = tf.dtypes.cast(tf.stack((n_digits_pred,
                                     digit_preds[1],
                                     digit_preds[2],
                                     digit_preds[3],
                                     digit_preds[4]), axis=1), tf.float32)
    return K.mean(K.sum(tf.dtypes.cast(K.equal(y_true, preds), tf.int64), axis=1) / 5) 
```

最后，我们整合基础层和最终层，并使用自定义损失和精度度量编译模型，如下所示：

```py
model = Model(inputs=vgg16.input, outputs=predictions)
model.compile(optimizer='adam',
              loss=weighted_entropy,
              metrics=[weighted_accuracy]) 
```

### 微调 VGG16 权重和最终层

我们对新的最终层进行 14 个周期的训练，并继续微调所有 VGG16 权重，如前一节所述，再进行 23 个周期（在这两种情况下都使用提前停止）。

下图显示了整个培训期间的培训和验证准确性以及损失。当我们在初始训练期后解冻 VGG16 重量时，准确度会下降，然后提高，从而实现 94.52%的验证性能：

![](img/B15439_18_13.png)

图 18.13：交叉验证性能

有关其他实施细节和结果评估，请参见笔记本。

### 经验教训

我们只需使用较小的训练集就可以达到相当高的精确度。然而，最先进的性能仅实现 1.02%的错误率（[https://benchmarks.ai/svhn](https://benchmarks.ai/svhn) ）。为了更进一步，最重要的一步是增加训练数据量。

有两种简单的方法可以实现这一点：我们可以在**额外的**数据集中包含更多的样本，我们可以使用图像增强（参见*AlexNet:重新启动深度学习研究*部分）。目前表现最好的方法在很大程度上依赖于从数据中获得的增强（Cubuk 2019）。

# 时间序列数据的 CNN–预测收益

CNN 最初是为处理图像数据而开发的，并在各种计算机视觉任务中取得了超人的性能。如第一节所述，时间序列数据具有类似于图像的网格结构，CNN 已成功应用于时间数据的一维、二维和三维表示。

如果数据符合模型的关键假设，即局部模式或关系有助于预测结果，那么 CNN 在时间序列中的应用最有可能取得成果。在时间序列上下文中，局部模式可以是相关间隔处的自相关或类似非线性关系。沿着第二和第三维度，局部模式意味着多变量序列的不同组成部分之间的系统关系，或者不同股票的这些序列之间的系统关系。由于局部性很重要，因此，与前馈网络相比，数据的组织很重要，在前馈网络中，对任何维度的元素进行洗牌都不会对学习过程产生负面影响。

在本节中，我们提供了一个相对简单的示例，使用一维卷积对自回归过程建模（参见*第 9 章*、*波动预测时间序列模型和统计套利*），该过程基于滞后回报预测未来回报。然后，我们复制了最近的一篇研究论文，该论文通过格式化多变量时间序列数据（如图像）来预测收益，取得了良好的结果。我们还将根据预测中包含的信号制定和测试交易策略。

## 一维卷积自回归神经网络

我们将介绍 CNN 使用单变量自回归资产回报模型的时间序列用例。更具体地说，该模型接收最近 12 个月的回报，并使用单层一维卷积预测下一个月的回报。

所需步骤如下：

1.  创造 12 个月的滚动滞后回报和相应结果
2.  定义模型架构
3.  培训模型并评估结果

在下面的章节中，我们将依次描述每个步骤；笔记本`time_series_prediction`包含本节的代码示例。

### 数据预处理

首先，我们将选择自 2000 年以来所有 Quandl Wiki 股票的调整后收盘价，如下所示：

```py
prices = (pd.read_hdf('../data/assets.h5', 'quandl/wiki/prices')
          .adj_close
          .unstack().loc['2000':])
prices.info()
DatetimeIndex: 2896 entries, 2007-01-01 to 2018-03-27
Columns: 3199 entries, A to ZUMZ 
```

接下来，我们将价格数据重新采样到月末频率，计算回报率，并将超过 100%的月回报率设置为缺失，因为它们可能表示数据错误。然后，我们放弃了缺少观察值的股票，保留了 1511 只股票，每个股票有 215 个观察值：

```py
returns = (prices
           .resample('M')
           .last()
           .pct_change()
           .dropna(how='all')
           .loc['2000': '2017']
           .dropna(axis=1)
           .sort_index(ascending=False))
# remove outliers likely representing data errors
returns = returns.where(returns<1).dropna(axis=1)
returns.info()
DatetimeIndex: 215 entries, 2017-12-31 to 2000-02-29
Columns: 1511 entries, A to ZQK 
```

为了创建 12 个滞后月收益率及其相应结果的滚动序列，我们对滚动 13 个月的切片进行迭代，并在将结果日期分配给指数后，将每个切片的转置附加到列表中。完成循环后，我们按照如下方式连接列表中的数据帧：

```py
n = len(returns)
nlags = 12
lags = list(range(1, nlags + 1))
cnn_data = []
for i in range(n-nlags-1):
    df = returns.iloc[i:i+nlags+1]        # select outcome and lags
    date = df.index.max()                 # use outcome date
    cnn_data.append(df.reset_index(drop=True)  # append transposed series
                    .transpose()
                    .assign(date=date)
                    .set_index('date', append=True)
                    .sort_index(1, ascending=True))
cnn_data = (pd.concat(cnn_data)
            .rename(columns={0: 'label'})
            .sort_index()) 
```

在 2001-2017 年期间，我们最终获得了超过 305000 对结果和滞后回报：

```py
cnn_data.info(null_counts=True)
MultiIndex: 305222 entries, ('A', Timestamp('2001-03-31 00:00:00')) to 
                            ('ZQK', Timestamp('2017-12-31 00:00:00'))
Data columns (total 13 columns):
... 
```

当我们计算每个滞后回报和结果的信息系数时，我们发现只有滞后 5 在统计上不显著：

![](img/B15439_18_14.png)

图 18.14：关于滞后远期收益的信息系数

### 定义模型架构

现在我们将使用 TensorFlow 的 Keras 接口定义模型架构。我们将一维卷积层与最大池和批处理规范化相结合，以产生实值标量输出：

```py
model = Sequential([Conv1D(filters=32,
                           kernel_size=4,
                           activation='relu',
                           padding='causal',
                           input_shape=(12, 1),
                           use_bias=True,
                           kernel_regularizer=regularizers.l1_l2(l1=1e-5,
                                                                 l2=1e-5)),
                    MaxPooling1D(pool_size=4),
                    Flatten(),
                    BatchNormalization(),
                    Dense(1, activation='linear')]) 
```

一维卷积计算长度为 4 的（正则化）向量与长度为 12 的每个输入序列的滑动点积，使用因果填充来维持时间顺序（参见*如何扫描输入：步幅和填充*部分）。生成的 32 个特征映射的长度 12 与大小为 4 的组中的 max pooling 减少为长度为 3 的 32 个向量的输入长度相同。

该模型输出加权平均值加上长度为 96 的平坦和归一化单向量的偏差，并具有 449 个可训练参数：

```py
Layer (type)                 Output Shape              Param #   
conv1d (Conv1D)              (None, 12, 32)            160       
max_pooling1d (MaxPooling1D) (None, 3, 32)             0         
flatten (Flatten)            (None, 96)                0         
batch_normalization (BatchNo (None, 96)                384       
dense (Dense)                (None, 1)                 97        
Total params: 641
Trainable params: 449
Non-trainable params: 192 
```

笔记本将模型生成和后续编译封装到`get_model()`函数中，该函数将模型配置参数化，以便于实验。

### 模型培训和绩效评估

我们对模型进行五年期数据训练，以预测该时段后的第一个月，并使用*第 7 章*中开发的`MultipleTimeSeriesCV`和*线性模型【从风险因素到回报预测】*重复该过程 36 次。关于遵循上一章中演示的模式的培训循环，请参见笔记本。

我们使用五个时期后的提前停止来简化阐述，导致正面偏差，因此结果仅具有说明性。训练长度从 1 到 27 个时期不等，中位数为 5 个时期，这表明该模型通常只能从过去的回报中学习非常有限的系统信息。因此，结果的累积平均信息系数约为 4，如*图 18.15*所示：

![](img/B15439_18_15.png)

图 18.15：（有偏）最佳时期的样本外信息系数

现在，我们将继续讨论一个更复杂的例子，即对多个时间序列数据使用 CNN。

## CNN-TA–2D 格式的时间序列聚类

为了开发时间序列数据的网格状结构，我们可以对单变量和多变量时间序列使用 CNN 体系结构。在后一种情况下，我们考虑不同的时间序列作为通道，类似于不同的颜色信号。

另一种方法是将α因子的时间序列转换为二维格式，以利用 CNN 的能力检测局部模式。Sezer 和 Ozbayoglu（2018）提出了**CNN-TA**，针对不同区间计算 15 项技术指标，并采用层次聚类法（见*第 13 章*、*数据驱动风险因素和无监督学习的资产配置*）在二维网格中查找彼此行为相似的指示器。

作者训练了一个 CNN，类似于我们之前使用的 CIFAR-10 示例，用于预测在给定日期是否购买、持有或出售资产。他们将 CNN 的表现与“买入并持有”和其他模型进行了比较，发现其表现优于 2007-2017 年期间道琼斯 30 种股票和 9 种交易量最大的 ETF 的每日价格系列的所有替代方案。

在本节中，我们使用每日美国股票价格数据对这种方法进行了实验，并演示了如何计算一组类似的指标并将其转换为图像格式。然后，我们训练 CNN 预测每日收益，并根据结果信号评估简单的长短策略。

### 以不同的时间间隔创建技术指标

我们首先从 Quandl Wiki 数据集中选择2007-2017 年连续五年交易量最大的 500 只美国股票。有关本节中的代码示例和一些附加的实现细节，请参见笔记本`engineer_cnn_features.ipynb`。

我们的功能包括 15 个技术指标和风险因素，我们计算了 15 个不同的区间，然后将它们排列在![](img/B15439_18_024.png)网格中。下表列出了一些技术指标；此外，我们跟随作者使用以下指标（更多信息请参见*附录*：

*   收盘价的**加权指数移动平均线**（**WMA**和**EMA**）
*   收盘价**变动率**（**ROC**）
*   **昌德动量振荡器**（**CMO**）
*   **柴金 A/D 振荡器****ADOSC**
*   **平均定向运动指数**（**ADX**）

![](img/B15439_18_16.png)

图 8.16：技术指标

对于每个指标，我们将时间段从 6 变为 20，以获得 15 个不同的测量值。例如，下面的代码示例计算**相对强度指数**（**RSI**）：

```py
T = list(range(6, 21))
for t in T:
    universe[f'{t:02}_RSI'] = universe.groupby(level='symbol').close.apply(RSI, timeperiod=t) 
```

对于需要多个输入的**归一化平均真实范围**（**NATR**，计算如下：

```py
for t in T:
    universe[f'{t:02}_NATR'] = universe.groupby(
                        level='symbol', group_keys=False).apply(
                        lambda x: NATR(x.high, x.low, x.close, timeperiod=t)) 
```

有关更多详细信息，请参阅 TA Lib 文档。

### 计算不同层位的滚动系数β

我们还使用了**五个法玛-法国风险因素**（**法玛**和法兰西，2015；参见*第 4 章*，*金融特征工程——如何研究阿尔法因素*。它们反映了股票回报对影响股票回报的因素的敏感性。我们通过计算股票每日收益与投资组合收益的滚动 OLS 回归系数来获取这些因素，该回归旨在反映基本驱动因素：

*   **股权风险溢价**：美国股票价值加权收益减去 1 个月美国国债利率
*   **规模****中小银行**：属于**小**（按市值计算）**的股票收益率减去**大股**的股票收益率**
*   **价值****HML**：账面市值**高于**的股票收益率减去**账面市值**低于**的股票收益率**
*   **投资**（**CMA**）**保守**投资支出**减去**积极支出的公司的回报差异
*   **盈利能力**（**RMW**）：类似地，**稳健**盈利能力**减去**的股票回报差异，与**弱**指标的股票回报差异。

我们使用`pandas_datareader`（参见*第 4 章*、*金融特征工程——如何研究阿尔法因子*）从肯尼思·弗伦奇的数据库中获取数据：

```py
import pandas_datareader.data as web
factor_data = (web.DataReader('F-F_Research_Data_5_Factors_2x3_daily', 
                              'famafrench', start=START)[0]) 
```

接下来，我们应用 statsmodels 的`RollingOLS()`在不同长度的窗口期（从 15 到 90 天）上运行回归。我们在`.fit()`方法上设置`params_only`参数，以加快计算速度，并使用拟合的`factor_model`的`.params`属性捕获系数：

```py
factors = [Mkt-RF, 'SMB', 'HML', 'RMW', 'CMA']
windows = list(range(15, 90, 5))
for window in windows:
    betas = []
    for symbol, data in universe.groupby(level='symbol'):
        model_data = data[[ret]].merge(factor_data, on='date').dropna()
        model_data[ret] -= model_data.RF
        rolling_ols = RollingOLS(endog=model_data[ret], 
                                 exog=sm.add_constant(model_data[factors]), 
                                                      window=window)
        factor_model = rolling_ols.fit(params_only=True).params.drop('const',  
                                                                     axis=1)
        result = factor_model.assign(symbol=symbol).set_index('symbol', 
                                                              append=True)
        betas.append(result)
    betas = pd.concat(betas).rename(columns=lambda x: f'{window:02}_{x}')
    universe = universe.join(betas) 
```

### 基于互信息的特征选择

下一步是从 20 个候选特征中选择 15 个最相关的特征，填充 15×15 输入网格。以下步骤的代码示例在笔记本`convert_cnn_features_to_image_format`中。

为此，我们估计了每个指标的互信息以及与我们的目标（一天远期收益）相关的 15 个区间。正如*第 4 章*、*金融特征工程——如何研究阿尔法因子*中所述，scikit learn 提供了`mutual_info_regression()`功能，使这一过程变得简单明了，尽管耗时且占用大量内存。为了加快这一过程，我们随机抽取了 100000 个观察结果：

```py
df = features.join(targets[target]).dropna().sample(n=100000)
X = df.drop(target, axis=1)
y = df[target]
mi[t] = pd.Series(mutual_info_regression(X=X, y=y), index=X.columns) 
```

*图 18.16*中的左面板显示了每个指示器 15 个间隔的平均互信息。从这个指标的角度来看，NATR、PPO 和 Bollinger 谱带是最重要的：

![](img/B15439_18_17.png)

图 18.17：时间序列的互信息和二维网格布局

### 层次特征聚类

*图 18.16*中的右侧面板绘制了 15 X 15 二维特征网格，我们将把它输入 CNN。如本章第一节所述，CNN 依赖于相关模式的位置，这些模式通常存在于附近像素密切相关的图像中，并且从一个像素到下一个像素的变化通常是渐进的。

为了以类似的方式组织我们的指标，我们将遵循 Sezer 和 Ozbayoglu 应用分层聚类的方法。目标是识别行为类似的特征，并相应地对网格的列和行进行排序。

我们可以在*第 13 章*介绍的 SciPy`pairwise_distance()`、`linkage()`和`dendrogram()`功能的基础上，*数据驱动的风险因素和资产配置，以及无监督学习*和其他形式的聚类。我们创建了一个辅助函数，该函数将输入列标准化，以避免由于尺度差异而扭曲特征之间的距离，并使用合并簇以最小化方差的 Ward 准则。该函数返回树状图中叶节点的顺序，树状图依次显示较大簇的连续形成：

```py
def cluster_features(data, labels, ax, title):
    data = StandardScaler().fit_transform(data)
    pairwise_distance = pdist(data)
    Z = linkage(data, 'ward')
    dend = dendrogram(Z,
                      labels=labels,
                      orientation='top',
                      leaf_rotation=0.,
                      leaf_font_size=8.,
                      ax=ax)
    return dend['ivl'] 
```

为了获得技术指标在列中的优化顺序和行中的不同间隔，我们使用 NumPy 的`.reshape()`方法确保我们想要聚类的维度出现在传递给`cluster_features()`的二维数组的列中：

```py
labels = sorted(best_features)
col_order = cluster_features(features.dropna().values.reshape(-1, 15).T, 
                             labels)
labels = list(range(1, 16))
row_order = cluster_features(
    features.dropna().values.reshape(-1, 15, 15).transpose((0, 2, 1)).reshape(-1, 15).T, labels) 
```

*图 18.18*显示了行和列特征的树状图：

![](img/B15439_18_18.png)

图 18.18：行和列特征的树状图

我们相应地重新排列特征，并将结果存储为 CNN 的输入，我们将在下一步创建 CNN。

### 卷积神经网络的建立与训练

现在我们准备按照上一节中概述的步骤设计、训练和评估 CNN。笔记本`cnn_for_trading.ipynb`包含相关的代码示例。

我们再次密切关注作者创建了一个 CNN，它有两个卷积层，内核大小分别为 3、16 和 32 个过滤器，然后是一个最大池层大小为 2。我们将最后一组过滤器的输出展平，并将生成的 1568 个输出连接到一个大小为 32 的密集层，对输入和输出连接应用 25%和 50%的退出概率，以减轻过拟合。下表总结了包含 55041 个可培训参数的 CNN 结构：

```py
Layer (type)                 Output Shape              Param #   
CONV1 (Conv2D)               (None, 15, 15, 16)        160       
CONV2 (Conv2D)               (None, 15, 15, 32)        4640      
POOL1 (MaxPooling2D)         (None, 7, 7, 32)          0         
DROP1 (Dropout)              (None, 7, 7, 32)          0         
FLAT1 (Flatten)              (None, 1568)              0         
FC1 (Dense)                  (None, 32)                50208     
DROP2 (Dropout)              (None, 32)                0         
FC2 (Dense)                  (None, 1)                 33        
Total params: 55,041
Trainable params: 55,041
Non-trainable params: 0 
```

我们使用*第 7 章**线性模型【从风险因素到回报预测】*中介绍的`MutipleTimeSeriesCV`序列和验证集指标生成器对模型进行交叉验证。我们在培训期间分批提供 64 个随机样本，为期 5 年的交易日，并使用随后的 3 个月（涵盖 2014-2017 年）进行验证。

我们将特征缩放到范围[-1,1]，再次使用 NumPy 的`.reshape()`方法创建必要的![](img/Image78145.png)格式：

```py
def get_train_valid_data(X, y, train_idx, test_idx):
    x_train, y_train = X.iloc[train_idx, :], y.iloc[train_idx]
    x_val, y_val = X.iloc[test_idx, :], y.iloc[test_idx]
    scaler = MinMaxScaler(feature_range=(-1, 1))
    x_train = scaler.fit_transform(x_train)
    x_val = scaler.transform(x_val)
    return (x_train.reshape(-1, size, size, 1), y_train,
            x_val.reshape(-1, size, size, 1), y_val) 
```

培训和验证遵循*第 17 章*、*交易深度学习*中规定的流程，依靠检查点在每个历元后存储权重，并生成最佳执行迭代的预测，而无需昂贵的再培训。

为了评估模型的预测精度，我们计算验证集的每日**信息系数**（**IC**），如下所示：

```py
checkpoint_path = Path('models', 'cnn_ts')
for fold, (train_idx, test_idx) in enumerate(cv.split(features)):
    X_train, y_train, X_val, y_val = get_train_valid_data(features, target, train_idx, test_idx)
    preds = y_val.to_frame('actual')
    r = pd.DataFrame(index=y_val.index.unique(level='date')).sort_index()
    model = make_model(filter1=16, act1='relu', filter2=32, 
                       act2='relu', do1=.25, do2=.5, dense=32)
    for epoch in range(n_epochs):            
        model.fit(X_train, y_train,
                  batch_size=batch_size,
                  validation_data=(X_val, y_val),
                  epochs=1, verbose=0, shuffle=True)
        model.save_weights(
            (checkpoint_path / f'ckpt_{fold}_{epoch}').as_posix())
        preds[epoch] = model.predict(X_val).squeeze()
        r[epoch] = preds.groupby(level='date').apply(
            lambda x: spearmanr(x.actual, x[epoch])[0]).to_frame(epoch) 
```

我们使用带有**Nesterov**动量的**随机梯度下降**（见*第 17 章*、*交易深度学习*对模型进行了多达 10 个时期的训练，发现表现最好的时期 8 和 9 的（低）日平均 IC 约为 0.009。

### 组合最佳模型以生成可交易信号

为了减少测试期预测的方差，我们为交叉验证期间表现最佳的 3 个模型生成并平均预测，这 3 个模型对应于 4、8 和 9 个时期的训练。与前面的时间序列示例一样，相对较短的训练周期强调，与例如图像数据中包含的系统信息相比，金融时间序列中的信号量较低。

`generate_predictions()`函数重新加载模型权重并返回目标时段的预测：

```py
def generate_predictions(epoch):
    predictions = []
    for fold, (train_idx, test_idx) in enumerate(cv.split(features)):
        X_train, y_train, X_val, y_val = get_train_valid_data(
            features, target, train_idx, test_idx)
        preds = y_val.to_frame('actual')
        model = make_model(filter1=16, act1='relu', filter2=32, 
                       act2='relu', do1=.25, do2=.5, dense=32)
        status = model.load_weights(
            (checkpoint_path / f'ckpt_{fold}_{epoch}').as_posix())
        status.expect_partial()
        predictions.append(pd.Series(model.predict(X_val).squeeze(), 
                                     index=y_val.index))
    return pd.concat(predictions)   
preds = {}
for i, epoch in enumerate(ic.drop('fold', axis=1).mean().nlargest(3).index):
    preds[i] = generate_predictions(epoch) 
```

我们存储预测，并根据这些每日回报预测对交易策略进行回溯测试。

### 回溯测试多空交易策略

为了了解信号质量，我们使用 Alphalens 计算投资于根据信号五分位数选择的股票的等权投资组合之间的利差（参见*第 4 章*、*金融特征工程——如何研究阿尔法因子*。

*图 18.19*显示，在一天的投资期限内，这种幼稚的策略在 2013-2017 年期间每天将获得略高于四个基点的收益：

![](img/B15439_18_19.png)

图 18.19：Alphalens 信号质量评估

我们将这一略微令人鼓舞的结果转化为一个简单的策略，即对 25 只回报预测最高（最低）的股票进行多头（空头）头寸，每天进行交易。*图 18.20*显示，在回溯测试期间的大部分时间内，该策略与 S&P500 基准具有竞争力（左面板），导致 35.6%的累积回报率和 0.53 的夏普比率（交易成本前；右面板）

![](img/B15439_18_20.png)

图 18.20：样本内外的回测性能

### 总结和经验教训

CNN 似乎能够从转换成二维网格的阿尔法因子时间序列中提取有意义的信息。对不同体系结构和训练参数的实验表明，结果不是很稳健，轻微的修改可能会导致性能显著下降。

调谐尝试也暴露了成功训练深度神经网络的臭名昭著的困难，特别是当信噪比较低时：太复杂的网络或错误的优化器会导致 CNN 陷入局部最优，它总是预测一个恒定值。

改进结果并获得更接近作者所实现的性能（使用不同的结果）的最重要步骤是重新审视这些特性。对于一组有限的技术指标，有许多不同的时间间隔可供选择。任何适当数量的时间序列特征都可以以矩形*n*×*m*格式排列，并受益于 CNN 学习本地模式的能力。*n*指标和*m*区间的选择使得二维网格的行和列的组织更加容易。试试看！

此外，作者对算法标记的买入、持有和卖出结果采用了分类方法（计算概要见论文），而我们的实验将回归应用于每日收益。*图 18.18*中的 Alphalens 图表表明，更长的持有期（特别是 10 天）可能会更好，因此也有相应调整策略或转向分类方法的余地。

# 总结

在本章中，我们介绍了 CNNs，这是一种专门的神经网络体系结构，它从我们对人类视觉的（有限的）理解中获得了线索，并且在类似网格的数据上表现得特别好。我们讨论了卷积或互相关的核心运算，这些运算推动了滤波器的发现，而滤波器又反过来检测对解决手头任务有用的特征。

我们回顾了几种最先进的体系结构，它们是很好的起点，特别是因为转移学习使我们能够重用预先训练的权重，并减少计算和数据密集型训练工作。我们还看到，Keras 使实现和培训一组不同的深层 CNN 体系结构变得相对简单。

在下一章中，我们将注意力转向专门为序列数据（如时间序列数据）设计的递归神经网络，它是投资和交易的核心。**